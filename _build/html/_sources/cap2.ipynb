{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "377e461a",
   "metadata": {},
   "source": [
    "# 3.2 Chapter 2: Classical methods\n",
    "\n",
    "In order to show that the exponential methods improve in dealing with Stiff problems, that is necessary to know how the previows methods deal with them, so a review on the theory of the classical methods is made in this chapter. In particular there will be focus on the one step methods. All the information is from [3]."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b5b5e785",
   "metadata": {},
   "source": [
    "## One step methods for ODE "
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "505bd6e7",
   "metadata": {},
   "source": [
    "In order to find a approximation for the solution of the problem\n",
    "\\begin{cases}\n",
    "y'(t) = f(t, y(t)), t \\in [t_0,T] \\\\\n",
    "y(t_0)=y_0 \\text{,}\n",
    "\\end{cases}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "51d0fcc2",
   "metadata": {},
   "source": [
    "they are of the form:\n",
    "$$\n",
    "y_{k+1} = y_{k} + h \\phi (t_{k},y_{k},t_{k+1},y_{k+1},h) \\text{,}\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "27f5598c",
   "metadata": {},
   "source": [
    "with $$k = 0, 1, ..., n-1;$$\n",
    "$$\n",
    "N \\in \\mathbb{N}; h = \\frac{T-t_0}{N}; \\\\\n",
    "\\{t_i = t_0 + ih : i = 0, 1, ..., N\\}; \\\\ \n",
    "y_n \\thickapprox y(t_n) .\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bd7c9d73",
   "metadata": {},
   "source": [
    "To analyse the method, there is a model problem\n",
    "\\begin{cases}\n",
    "    y'(t) = - \\lambda y(t) \\text{ ; } t \\in[t_0,T]\\\\ \n",
    "    y(t_0)=y_0,\\\\\n",
    "\\end{cases}\n",
    "\n",
    "whose solution is $y(t) = y_0 e^{-\\lambda (t-t_0)}$\n",
    "with $\\lambda > 0.$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "536e4884",
   "metadata": {},
   "source": [
    "If that is possible to manipulate the method so that, for this problem, can be written as\n",
    "$$\n",
    "y_{k+1} = \\zeta(\\lambda,h) y_k,\n",
    "$$\n",
    "then $$\\zeta(\\lambda,h)$$ is called $\\textbf{amplification factor}$ of the method."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8c22c2ef",
   "metadata": {},
   "source": [
    "By induction, it gives\n",
    "$$\n",
    "y_{k+1} = \\zeta(\\lambda, h)^{k+1} y_0.\n",
    "$$"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c650681c",
   "metadata": {},
   "source": [
    "It is well known that this expression only converges as k goes to infinity if \n",
    "$$\n",
    "|\\zeta(\\lambda, h)| < 1\n",
    "$$\n",
    "and then converges to zero."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "d3c126cc",
   "metadata": {},
   "source": [
    "When it occurs, i.e., $k \\rightarrow \\infty \\Rightarrow y_k \\rightarrow 0$ such as the exact solution $y(t) = y_0 e^{-\\lambda (t-t_0)}$, it is said that there is $\\textbf{stability}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "b98a1399",
   "metadata": {},
   "source": [
    "The inequation gives a interval for which values of $\\lambda h$, $|\\zeta(\\lambda, h)|<1$, called $\\textbf{interval of stability}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "20926578",
   "metadata": {},
   "source": [
    "And if the interval of stability contains all the points $z$ such that $Re(z) < 0$, the method is said $\\textbf{A-stable}$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37592453",
   "metadata": {},
   "source": [
    "The reason for taking this specific problem is that it models the behaviour of the difference between the approximation and the solution on a small neighbourhood of any Cauchy problem:"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "9d1fae9b",
   "metadata": {},
   "source": [
    "Taking\n",
    "\\begin{cases}\n",
    "    y'(t) = f(y(t), t), t \\in (t_0, T) \\\\\n",
    "    y(t_0) = y_0 \\in \\mathbb{K}\n",
    "\\end{cases}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "1fe24ff9",
   "metadata": {},
   "source": [
    "and a approximation $z$ of the solution $y$, doing\n",
    "\\begin{align*}\n",
    "\\sigma(t) = z(t) - y(t) \\Rightarrow\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "ca40d1b4",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\dot{\\sigma}(t) = \\dot{z}(t) - \\dot{y}(t) = f(z(t), t) - f(y(t), t) \\Rightarrow\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "8242104a",
   "metadata": {},
   "source": [
    "\\begin{align*}\n",
    "\\dot{\\sigma}(t) + \\dot{y}(t) = \\dot{z}(t) = f(z(t), t) = f(y(t)+\\sigma(t), t) = f(y(t), t) + \\sigma(t)\\frac{\\partial f}{\\partial y} + O(\\sigma^2(t)),\n",
    "\\end{align*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "dce75e4f",
   "metadata": {},
   "source": [
    "so\n",
    "\\begin{cases}\n",
    "    \\dot{\\sigma}(t) \\approx \\sigma(t) \\frac{\\partial f}{\\partial y} (y(t), t) \\\\\n",
    "    \\sigma(t_k) = \\sigma_k.\n",
    "\\end{cases}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4d67a950",
   "metadata": {},
   "source": [
    "Other important definitions are:\n",
    "\n",
    "$\\textbf{Local truncation error:}$ Is the difference between the exact expression and its numerical approximation in a certain point and with a certain domain discretization. If the domain is equally spaced by $h$ is often denoted by $\\tau(h,t_0)$ being $t_0$ the point."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "c0b64e16",
   "metadata": {},
   "source": [
    "$\\textbf{Order of the local truncation error:}$ the local truncation error (which depends on the $h$ spacing of the discretized domain) $\\tau(h)$ has order $n \\in \\mathbb{N}$ if $\\tau(h) = O(h^n) $, i.e., if there is constant $M \\in \\mathbb{R}$ and $h_0 \\in \\mathbb{R}$ such that $\\tau(h) \\leq M h^n$, $\\forall h \\leq h_0$."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "160a9007",
   "metadata": {},
   "source": [
    "$\\textbf{Global error:}$ Is the difference between the approximation given by the method for the solution of the problem on a certain point and the exact one (unlike the local truncation error, here we take the solution we got, not the expression used to find the approximation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "48de4228",
   "metadata": {},
   "source": [
    "$\\textbf{Consistency:}$ The method is said consistent if $\\lim _{h \\to 0} \\frac{1}{h}\\tau(h,x_0) = 0$.\n",
    "\n",
    "$\\textbf{Obs.:}$ For consistency, we usually only analyse for the linear part of the Cauchy problem, since this is the part that most influences in the consistency.\n",
    "\n",
    "$\\textbf{Order of consistency:}$ is the smallest order (varying the points at which the local error is calculated) of the local truncation error."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "6c3fbce8",
   "metadata": {},
   "source": [
    "$\\textbf{Convergence:}$ A numerical method is convergent if, and only if, for any well-posed Cauchy problem and for every $t \\in (t_0, T)$,\n",
    "$$\\lim_{h \\to 0} e_k = 0$$\n",
    "with $t - t_0 = kh$ fixed and $e_k$ denoting the global error on $t_k$ (following the past notation)."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "37190d00",
   "metadata": {},
   "source": [
    "$\\textbf{Theorem:}$ A one-step explicit method given by\n",
    "$$\n",
    "y_0 = y(t_0) \\\\\n",
    "y_{k+1} = y_{k} + h \\phi (t_{k},y_{k},h)\n",
    "$$\n",
    "such that $\\phi$ is Lipschitzian in y, continuous in their arguments, and consistent for any well-posed Cauchy problem is convergent. Besides that, the convergence order is greater or equal to the consistency order.\n",
    "\n",
    "$\\textit{Prove:}$ [3] pÃ¡g 29-31."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "7c3f10a6",
   "metadata": {},
   "source": [
    "## Examples"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "66ab633b",
   "metadata": {},
   "source": [
    "Euler method: \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\phi (t_{k},y_{k},h) = f(t_{k},y_{k})\n",
    "\\end{equation*}\n",
    "\n",
    "Modified Euler method: \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\phi (t_{k},y_{k},h) = \\frac{1}{2} \\left[ f(t_{k},y_{k}) + f(t_{k+1},y_{k} + h f(t_{k},y_{k})) \\right]\n",
    "\\end{equation*}\n",
    "\n",
    "Midpoint method: \n",
    "\n",
    "\\begin{equation*}\n",
    "    \\phi (t_{k},y_{k},h) = f(t_{k} + \\frac{h}{2},y_{k} + \\frac{h}{2} f(t_{k},y_{k}))\n",
    "\\end{equation*}\n",
    "\n",
    "Classic Runge-Kutta (RK 4-4): \n",
    "\n",
    "\\begin{gather*}\n",
    "    \\phi (t_{k},y_{k},h) = \\frac{1}{6} \\left( \\kappa_1 + 2 \\kappa_2 + 2 \\kappa_3 + \\kappa_4 \\right), \\text{with }\\\\\n",
    "    \\kappa_1 = f(t_{k},y_{k})\\\\\n",
    "    \\kappa_2 = f(t_{k} + \\frac{h}{2},y_{k} + \\frac{h}{2} \\kappa_1)\\\\\n",
    "    \\kappa_3 = f(t_{k} + \\frac{h}{2},y_{k} + \\frac{h}{2} \\kappa_2)\\\\\n",
    "    \\kappa_4 = f(t_{k} + h, y_{k} + h \\kappa_3)\n",
    "\\end{gather*}"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "bb394a4b",
   "metadata": {},
   "source": [
    "## Euler method"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "163c5374",
   "metadata": {},
   "source": [
    "Further detailing this explicit one-step method of\n",
    "\n",
    "\\begin{equation*}\n",
    "    \\phi (t_{k},y_{k},h) = f(t_{k},y_{k}),\n",
    "\\end{equation*}\n",
    "\n",
    "an analysis on stability, convergence and order of convergence is done."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "5ad7e125",
   "metadata": {},
   "source": [
    "### Stability\n",
    "\n",
    "For the problem\n",
    "\\begin{cases}\n",
    "    y'(t) = - \\lambda y(t) \\text{ ; } t \\in [t_0 , T] \\\\\n",
    "    y(t_0)=y_0,\n",
    "\\end{cases}\n",
    "\n",
    "with known solution\n",
    "\n",
    "$$ y(t) = y_0e^{-\\lambda (t-t_0)},$$\n",
    "\n",
    "the method turn into:\n",
    "\n",
    "$$\n",
    "y_0 = y(t_0)\\\\\n",
    "\\textbf{for } k = 0, 1, 2, ..., N-1 :\\\\\n",
    "    y_{k+1} = y_k + h \\lambda y_k \\\\\n",
    "    t_{k+1} = t_k + h.\n",
    "$$\n",
    "\n",
    "Then the amplification factor is:\n",
    "$$\n",
    "(1 - h \\lambda).\n",
    "$$\n",
    "If $|1 - h \\lambda| > 1$, for fixed $N$, it will be a divergent series ($k \\rightarrow \\infty \\Rightarrow y_k \\rightarrow \\infty $), so, since the computer has a limitant number that can represent, even if the number of steps is such that $h$ is not small enought, it might have sufficient steps to reach the maximum number represented by the machine.\n",
    "\n",
    "However, if $|1 - h \\lambda| < 1$ and $N$ is fixed, it converges to zero ($k \\rightarrow \\infty \\Rightarrow y_k \\rightarrow 0 )$.\n",
    "\n",
    "\n",
    "Besides that, $|1 - h \\lambda| < 1$ is the same as $0 < h \\lambda < 2$.\n",
    "So the interval of stability is (0,2).\n",
    "That's why the method suddenly converged, it was when $h$ got small enought to $h \\lambda$ be in the interval of stability, i.e., $h < 2/\\lambda$.\n",
    "\n",
    "It is worth mentioning here that if $-1 < 1 - h \\lambda < 0$, the error will converge oscillating since it takes positive values with even exponents and negative with odd ones."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "4c4a4606",
   "metadata": {},
   "source": [
    "### Convergence\n",
    "Since\n",
    "$$\n",
    "\\lim_{m \\to +\\infty} \\left(1 + \\frac{p}{m} \\right)^m = e^p,\n",
    "$$\n",
    "and h = $\\frac{T-t_0}{N}$, for $y_N$ we have\n",
    "$$\n",
    "\\lim_{N \\to +\\infty} y_N = \\lim_{N \\to +\\infty} \\left(1 - h \\lambda \\right)^N y_0 = \\lim_{N \\to +\\infty} \\left(1 - \\frac{(T-t_0) \\lambda}{N} \\right)^N y_0.\n",
    "$$\n",
    "It is reasonable to take $p = -(T-t_0) \\lambda$ and conclude that the last point estimated by the method will converge to\n",
    "$$\n",
    "y_0e^{-\\lambda (T-t_0)}.\n",
    "$$\n",
    "Which is precisely $y(T)$ and proves the convergence."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "id": "cb7db596",
   "metadata": {},
   "source": [
    "### Order of convergence\n",
    "\n",
    "Being $\\tau(h, t_k)$ the local truncation error.\n",
    "\n",
    "From\n",
    "\\begin{equation*}\n",
    "    y(t_{k+1}) = y(t_k) + h f(y(t_k),t_k) + O(h^2),\n",
    "\\end{equation*}\n",
    "\n",
    "we have\n",
    "\\begin{equation*}\n",
    "    h \\tau(h, t_k) \\doteq \\frac{y(t_{k+1}) - y(t_k)}{h} - f(t_k, y(t_k)) = O(h^2),\n",
    "\\end{equation*}\n",
    "\n",
    "so\n",
    "\\begin{equation*}\n",
    "    \\tau(h, t_k) = O(h).\n",
    "\\end{equation*}\n",
    "\n",
    "Since for one step methods the order of convergence is the order of the local truncation error, the order is of $O(h)$, order 1."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
