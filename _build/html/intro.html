
<!DOCTYPE html>

<html>
  <head>
    <meta charset="utf-8" />
    <meta name="viewport" content="width=device-width, initial-scale=1.0" /><meta name="generator" content="Docutils 0.17.1: http://docutils.sourceforge.net/" />

    <title>Numerical exponential integrators for dynamical systems</title>
    
  <!-- Loaded before other Sphinx assets -->
  <link href="_static/styles/theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">
<link href="_static/styles/pydata-sphinx-theme.css?digest=1999514e3f237ded88cf" rel="stylesheet">

    
  <link rel="stylesheet"
    href="_static/vendor/fontawesome/5.13.0/css/all.min.css">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-solid-900.woff2">
  <link rel="preload" as="font" type="font/woff2" crossorigin
    href="_static/vendor/fontawesome/5.13.0/webfonts/fa-brands-400.woff2">

    <link rel="stylesheet" type="text/css" href="_static/pygments.css" />
    <link rel="stylesheet" href="_static/styles/sphinx-book-theme.css?digest=5115cc725059bd94278eecd172e13a965bf8f5a9" type="text/css" />
    <link rel="stylesheet" type="text/css" href="_static/togglebutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/copybutton.css" />
    <link rel="stylesheet" type="text/css" href="_static/mystnb.css" />
    <link rel="stylesheet" type="text/css" href="_static/sphinx-thebe.css" />
    <link rel="stylesheet" type="text/css" href="_static/design-style.b7bb847fb20b106c3d81b95245e65545.min.css" />
    
  <!-- Pre-loaded scripts that we'll load fully later -->
  <link rel="preload" as="script" href="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf">

    <script data-url_root="./" id="documentation_options" src="_static/documentation_options.js"></script>
    <script src="_static/jquery.js"></script>
    <script src="_static/underscore.js"></script>
    <script src="_static/doctools.js"></script>
    <script src="_static/clipboard.min.js"></script>
    <script src="_static/copybutton.js"></script>
    <script src="_static/scripts/sphinx-book-theme.js?digest=9c920249402e914e316237a7dbc6769907cce411"></script>
    <script>let toggleHintShow = 'Click to show';</script>
    <script>let toggleHintHide = 'Click to hide';</script>
    <script>let toggleOpenOnPrint = 'true';</script>
    <script src="_static/togglebutton.js"></script>
    <script>var togglebuttonSelector = '.toggle, .admonition.dropdown, .tag_hide_input div.cell_input, .tag_hide-input div.cell_input, .tag_hide_output div.cell_output, .tag_hide-output div.cell_output, .tag_hide_cell.cell, .tag_hide-cell.cell';</script>
    <script src="_static/design-tabs.js"></script>
    <script>const THEBE_JS_URL = "https://unpkg.com/thebe@0.8.2/lib/index.js"
const thebe_selector = ".thebe,.cell"
const thebe_selector_input = "pre"
const thebe_selector_output = ".output, .cell_output"
</script>
    <script async="async" src="_static/sphinx-thebe.js"></script>
    <script>window.MathJax = {"options": {"processHtmlClass": "tex2jax_process|mathjax_process|math|output_area"}}</script>
    <script defer="defer" src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-mml-chtml.js"></script>
    <link rel="index" title="Index" href="genindex.html" />
    <link rel="search" title="Search" href="search.html" />
    <meta name="viewport" content="width=device-width, initial-scale=1" />
    <meta name="docsearch:language" content="None">
    

    <!-- Google Analytics -->
    
  </head>
  <body data-spy="scroll" data-target="#bd-toc-nav" data-offset="60">
<!-- Checkboxes to toggle the left sidebar -->
<input type="checkbox" class="sidebar-toggle" name="__navigation" id="__navigation" aria-label="Toggle navigation sidebar">
<label class="overlay overlay-navbar" for="__navigation">
    <div class="visually-hidden">Toggle navigation sidebar</div>
</label>
<!-- Checkboxes to toggle the in-page toc -->
<input type="checkbox" class="sidebar-toggle" name="__page-toc" id="__page-toc" aria-label="Toggle in-page Table of Contents">
<label class="overlay overlay-pagetoc" for="__page-toc">
    <div class="visually-hidden">Toggle in-page Table of Contents</div>
</label>
<!-- Headers at the top -->
<div class="announcement header-item noprint"></div>
<div class="header header-item noprint"></div>

    
    <div class="container-fluid" id="banner"></div>

    

    <div class="container-xl">
      <div class="row">
          
<!-- Sidebar -->
<div class="bd-sidebar noprint single-page" id="site-navigation">
</div>


          


          
<!-- A tiny helper pixel to detect if we've scrolled -->
<div class="sbt-scroll-pixel-helper"></div>
<!-- Main content -->
<div class="col py-0 content-container">
    
    <div class="header-article row sticky-top noprint">
        



<div class="col py-1 d-flex header-article-main">
    <div class="header-article__left">
        
    </div>
    <div class="header-article__right">
<button onclick="toggleFullScreen()"
  class="headerbtn"
  data-toggle="tooltip"
data-placement="bottom"
title="Fullscreen mode"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-expand"></i>
  </span>

</button>

<div class="menu-dropdown menu-dropdown-repository-buttons">
  <button class="headerbtn menu-dropdown__trigger"
      aria-label="Source repositories">
      <i class="fab fa-github"></i>
  </button>
  <div class="menu-dropdown__content">
    <ul>
      <li>
        <a href="https://github.com/Isabela-Miki-Suzuki/Numerical-exponential-integrators-for-dynamical-systems"
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Source repository"
>
  

<span class="headerbtn__icon-container">
  <i class="fab fa-github"></i>
  </span>
<span class="headerbtn__text-container">repository</span>
</a>

      </li>
      
      <li>
        <a href="https://github.com/Isabela-Miki-Suzuki/Numerical-exponential-integrators-for-dynamical-systems/issues/new?title=Issue%20on%20page%20%2Fintro.html&body=Your%20issue%20content%20here."
   class="headerbtn"
   data-toggle="tooltip"
data-placement="left"
title="Open an issue"
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-lightbulb"></i>
  </span>
<span class="headerbtn__text-container">open issue</span>
</a>

      </li>
      
    </ul>
  </div>
</div>
<label for="__page-toc"
  class="headerbtn headerbtn-page-toc"
  
>
  

<span class="headerbtn__icon-container">
  <i class="fas fa-list"></i>
  </span>

</label>

    </div>
</div>

<!-- Table of contents -->
<div class="col-md-3 bd-toc show noprint">
    <div class="tocsection onthispage pt-5 pb-3">
        <i class="fas fa-list"></i> Contents
    </div>
    <nav id="bd-toc-nav" aria-label="Page">
        <ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-Summary_of_the_proposed_project">
   Summary of the proposed project
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-Project_execution">
   Project execution
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap1">
     Motivation - Stiffness
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap2">
     Classical methods
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap3">
     Important concepts for the study of exponential methods
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap4">
     Exponential methods
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap5">
     Swing spring application
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-References">
   References
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-Participation_in_scientific_event,_list_of_publications_and_list_of_papers_prepared_or_submitted">
   Participation in scientific event, list of publications and list of papers prepared or submitted
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-appendix">
   Appendix
  </a>
 </li>
</ul>

    </nav>
</div>
    </div>
    <div class="article row">
        <div class="col pl-md-3 pl-lg-5 content-container">
            <!-- Table of contents that is only displayed when printing the page -->
            <div id="jb-print-docs-body" class="onlyprint">
                <h1>General Information about the Project</h1>
                <!-- Table of contents -->
                <div id="print-main-content">
                    <div id="jb-print-toc">
                        
                        <div>
                            <h2> Contents </h2>
                        </div>
                        <nav aria-label="Page">
                            <ul class="visible nav section-nav flex-column">
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-Summary_of_the_proposed_project">
   Summary of the proposed project
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-Project_execution">
   Project execution
  </a>
  <ul class="visible nav section-nav flex-column">
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap1">
     Motivation - Stiffness
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap2">
     Classical methods
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap3">
     Important concepts for the study of exponential methods
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap4">
     Exponential methods
    </a>
   </li>
   <li class="toctree-l2 toc-h2 nav-item toc-entry">
    <a class="reference internal nav-link" href="intro.html#document-cap5">
     Swing spring application
    </a>
   </li>
  </ul>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-References">
   References
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-Participation_in_scientific_event,_list_of_publications_and_list_of_papers_prepared_or_submitted">
   Participation in scientific event, list of publications and list of papers prepared or submitted
  </a>
 </li>
 <li class="toctree-l1 toc-h1 nav-item toc-entry">
  <a class="reference internal nav-link" href="intro.html#document-appendix">
   Appendix
  </a>
 </li>
</ul>

                        </nav>
                    </div>
                </div>
            </div>
            <main id="main-content" role="main">
                
              <div>
                
  <section class="tex2jax_ignore mathjax_ignore" id="general-information-about-the-project">
<h1>General Information about the Project<a class="headerlink" href="#general-information-about-the-project" title="Permalink to this headline">#</a></h1>
<ul class="simple">
<li><p>Title:</p></li>
</ul>
<p><strong>Numerical exponential integrators for dynamical systems</strong></p>
<ul class="simple">
<li><p>Researcher in charge:</p></li>
</ul>
<p><strong>André Salles Carvalho, Prof. Dr.</strong></p>
<ul class="simple">
<li><p>Beneficiary:</p></li>
</ul>
<p><strong>Isabela Miki Suzuki</strong></p>
<ul class="simple">
<li><p>Host institution:</p></li>
</ul>
<p><strong>Instituto de Matemática e Estatística at the Universidade de São Paulo</strong></p>
<ul class="simple">
<li><p>Research team:</p></li>
</ul>
<p><strong>Isabela Miki Suzuki</strong></p>
<p><strong>Pedro S. Peixoto, Prof. Dr.</strong></p>
<ul class="simple">
<li><p>Number of the research project:</p></li>
</ul>
<p><strong>2021/06678-5</strong></p>
<ul class="simple">
<li><p>Duration:</p></li>
</ul>
<p><strong>1 August 2021 to 31 July 2023</strong></p>
<ul class="simple">
<li><p>Period covered by this research report:</p></li>
</ul>
<p><strong>1 August 2021 to 31 July 2023</strong></p>
<div class="toctree-wrapper compound">
<span id="document-Summary_of_the_proposed_project"></span><section class="tex2jax_ignore mathjax_ignore" id="summary-of-the-proposed-project">
<h2>Summary of the proposed project<a class="headerlink" href="#summary-of-the-proposed-project" title="Permalink to this headline">#</a></h2>
<p>This is a scientific initiation project that proposes the deep study of some of the main methods
of exponential integration for problems in dynamic systems, with emphasis on the paper [1].
Here, the undergraduate will study the construction, analysis, implementation and application
of them and at the end, it is expected that she is familiar with modern techniques of numerical
methods.</p>
<p><strong>Keywords:</strong> exponential integrator, numerical methods, dynamical systems.</p>
</section>
<span id="document-Project_execution"></span><section class="tex2jax_ignore mathjax_ignore" id="project-execution">
<h2>Project execution<a class="headerlink" href="#project-execution" title="Permalink to this headline">#</a></h2>
<div class="toctree-wrapper compound">
<span id="document-cap1"></span><section class="tex2jax_ignore mathjax_ignore" id="motivation-stiffness">
<h3>Motivation - Stiffness<a class="headerlink" href="#motivation-stiffness" title="Permalink to this headline">#</a></h3>
<p>The reason for studying exponential methods is that those are good with <span class="math notranslate nohighlight">\(\textbf{stiff differential equations}\)</span> in terms of precision and how small the time step is required to be to achieve good accuracy.</p>
<section id="cauchy-problem">
<h4>Cauchy problem<a class="headerlink" href="#cauchy-problem" title="Permalink to this headline">#</a></h4>
<p>A <span class="math notranslate nohighlight">\(\textbf{Cauchy problem}\)</span> is a ordinary differential equation (ODE) with initial conditions. Being its standard scalar form:</p>
<p><span class="math notranslate nohighlight">\(\begin{cases}
    y'(t) = f(y(t), t), t \in (t_0, T) \\
    y(t_0) = y_0 \in \mathbb{K} \text{,}
\end{cases}\)</span></p>
<p>with <span class="math notranslate nohighlight">\(\mathbb{K}\)</span> a field, <span class="math notranslate nohighlight">\(f\)</span> function with image in <span class="math notranslate nohighlight">\(\mathbb{K}\)</span> and <span class="math notranslate nohighlight">\(t_0, T \in \mathbb{R}\)</span>.</p>
<p>Sometimes, it is convenient to separate the linear part of <span class="math notranslate nohighlight">\(f\)</span> as indicated below:</p>
<div class="math notranslate nohighlight">
\[
    f(y(t), t) = g(y(t), t) - \lambda y(t) \text{,}
\]</div>
<p>with <span class="math notranslate nohighlight">\(\lambda \in \mathbb{K}\)</span> or <span class="math notranslate nohighlight">\(\mathscr{M}_{N \times N}(\mathbb{K})\)</span>.</p>
<p>So the system is:</p>
<p><span class="math notranslate nohighlight">\(\begin{cases}
    y'(t) + \lambda y(t) = g(y(t), t), t \in (t_0, T) \\
    y(0) = y_0 
    \text{.}
\end{cases}\)</span></p>
<p>In this project, the stiff ones were those addressed.</p>
<p>Notation as in [1].</p>
</section>
<section id="stiffness">
<h4>Stiffness<a class="headerlink" href="#stiffness" title="Permalink to this headline">#</a></h4>
<p>The error of the approximation given by a method trying to estimate the solution of a Cauchy problem is always given by a term multiplied by a higher derivative of the exact solution, because of the Taylor expansion with Lagrange form of the remainder. In that way, if that is enough information about this derivative, the error can be estimated.</p>
<p>If the norm of the derivative increases with the time, but the exact solution doesn’t, that is possible that the error dominates the approximation and the precision is lost. Those problems are called <span class="math notranslate nohighlight">\(\textbf{stiff equations}\)</span>.</p>
<p>Between them, there are the <span class="math notranslate nohighlight">\(\textbf{stiff differential equations}\)</span>, that have exact solution given by the sum of a <span class="math notranslate nohighlight">\(\textit{transient solution}\)</span> with a <span class="math notranslate nohighlight">\(\textit{steady state solution}\)</span>.</p>
<p>The <span class="math notranslate nohighlight">\(\textbf{transient solution}\)</span> is of the form:</p>
<div class="math notranslate nohighlight">
\[
    e^{-ct} \text{, with c &gt;&gt;1, }
\]</div>
<p>which is known to go to zero really fast as t increases. But its <span class="math notranslate nohighlight">\(n\)</span>th derivative</p>
<div class="math notranslate nohighlight">
\[
    \mp c^{n}e^{-ct}
\]</div>
<p>doesn’t go as quickly and may increase in magnitude.</p>
<p>The <span class="math notranslate nohighlight">\(\textbf{steady state solution}\)</span>, however, as its name implies, have small changes as time passes, with higher derivative being almost constant zero.</p>
<p>In a system of ODE’s, these characteristics are most common in problems in which the solution of the initial value problem is of the form</p>
<div class="math notranslate nohighlight">
\[
    e^{A}
\]</div>
<p>being <span class="math notranslate nohighlight">\(A\)</span> a matrix such that <span class="math notranslate nohighlight">\(\lambda_{min}\)</span> and <span class="math notranslate nohighlight">\(\lambda_{max}\)</span> are the eigenvalue with minimum and maximum value in modulus and <span class="math notranslate nohighlight">\(\lambda_{min} &lt;&lt; \lambda_{max}\)</span>. On the bigger magnitude eigenvalue direction, the behaviour is very similar to the transient solution, having drastic changes over time and on the smaller one, comparing to that, changes almost nothing as times passes, like the steady state solution.</p>
<p>Work around these problems and being able to accurately approximate these so contrasting parts of the solutions requires more robust methods than the more classic and common one-step methods addressed at the beginning of the study of numerical methods for Cauchy problems. For the systems, it is also required that that is a precise way to calculate the exponential of a matrix.</p>
<p>In this project, we studied the <span class="math notranslate nohighlight">\(\textbf{exponential methods}\)</span>, their capabilities to deal with these problems and the comparision with other simpler methods.</p>
<p>Definition from [2].</p>
</section>
</section>
<span id="document-cap2"></span><section class="tex2jax_ignore mathjax_ignore" id="classical-methods">
<h3>Classical methods<a class="headerlink" href="#classical-methods" title="Permalink to this headline">#</a></h3>
<p>In order to show that the exponential methods improve in dealing with Stiff problems, that is necessary to know how the previows methods deal with them, so a review on the theory of the classical methods is made in this chapter. In particular there will be focus on the one step methods. All the information is from [3].</p>
<section id="one-step-methods-for-ode">
<h4>One step methods for ODE<a class="headerlink" href="#one-step-methods-for-ode" title="Permalink to this headline">#</a></h4>
<p>In order to find a approximation for the solution of the problem
<span class="math notranslate nohighlight">\(\begin{cases}
y'(t) = f(t, y(t)), t \in [t_0,T] \\
y(t_0)=y_0 \text{,}
\end{cases}\)</span></p>
<p>they are of the form:</p>
<div class="math notranslate nohighlight">
\[
y_{k+1} = y_{k} + h \phi (t_{k},y_{k},t_{k+1},y_{k+1},h) \text{,}
\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[k = 0, 1, ..., n-1;\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
N \in \mathbb{N}; h = \frac{T-t_0}{N}; \\
\{t_i = t_0 + ih : i = 0, 1, ..., N\}; \\ 
y_n \thickapprox y(t_n) .
\end{split}\]</div>
<p>To analyse the method, there is a model problem</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    y'(t) = - \lambda y(t) \text{ ; } t \in[t_0,T]\\ 
    y(t_0)=y_0,\\
\end{cases}
\end{split}\]</div>
<p>whose solution is <span class="math notranslate nohighlight">\(y(t) = y_0 e^{-\lambda (t-t_0)}\)</span>
with <span class="math notranslate nohighlight">\(\lambda &gt; 0.\)</span></p>
<p>If that is possible to manipulate the method so that, for this problem, can be written as <span class="math notranslate nohighlight">\(y_{k+1} = \zeta(\lambda,h) y_k,\)</span></p>
<p>then <span class="math notranslate nohighlight">\(\zeta(\lambda,h)\)</span> is called <span class="math notranslate nohighlight">\(\textbf{amplification factor}\)</span> of the method.</p>
<p>By induction, it gives</p>
<div class="math notranslate nohighlight">
\[
y_{k+1} = \zeta(\lambda, h)^{k+1} y_0.
\]</div>
<p>It is well known that this expression only converges as k goes to infinity if <span class="math notranslate nohighlight">\( |\zeta(\lambda, h)| &lt; 1\)</span></p>
<p>and then converges to zero.</p>
<p>When it occurs, i.e.,</p>
<div class="math notranslate nohighlight">
\[
    k \rightarrow \infty \Rightarrow y_k \rightarrow 0
\]</div>
<p>such as the exact solution</p>
<div class="math notranslate nohighlight">
\[
    y(t) = y_0 e^{-\lambda (t-t_0)},
\]</div>
<p>it is said that there is <span class="math notranslate nohighlight">\(\textbf{stability}\)</span>.</p>
<p>The interval with the values of <span class="math notranslate nohighlight">\(\lambda h\)</span> such as</p>
<div class="math notranslate nohighlight">
\[
|\zeta(\lambda, h)|&lt;1,
\]</div>
<p>is called <span class="math notranslate nohighlight">\(\textbf{interval of stability}\)</span>.</p>
<p>And if the interval of stability contains all the points <span class="math notranslate nohighlight">\(z\)</span> such that</p>
<div class="math notranslate nohighlight">
\[
    Re(z) &lt; 0,
\]</div>
<p>the method is said <span class="math notranslate nohighlight">\(\textbf{A-stable}\)</span>.</p>
<p>The reason for taking this specific problem is that it models the behaviour of the difference between the approximation and the solution on a small neighbourhood of any Cauchy problem:</p>
<p>Taking</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    y'(t) = f(y(t), t), t \in (t_0, T) \\
    y(t_0) = y_0 \in \mathbb{K}
\end{cases}
\end{split}\]</div>
<p>and a approximation <span class="math notranslate nohighlight">\(z\)</span> of the solution <span class="math notranslate nohighlight">\(y\)</span>, doing
$<span class="math notranslate nohighlight">\(
\sigma(t) = z(t) - y(t) \Rightarrow
\)</span>$</p>
<div class="math notranslate nohighlight">
\[
\dot{\sigma}(t) = \dot{z}(t) - \dot{y}(t) = f(z(t), t) - f(y(t), t) \Rightarrow
\]</div>
<div class="math notranslate nohighlight">
\[
\dot{\sigma}(t) + \dot{y}(t) = \dot{z}(t) = f(z(t), t) = f(y(t)+\sigma(t), t)
\]</div>
<div class="math notranslate nohighlight">
\[
 = f(y(t), t) + \sigma(t)\frac{\partial f}{\partial y} + O(\sigma^2(t)),
\]</div>
<p>so</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    \dot{\sigma}(t) \approx \sigma(t) \frac{\partial f}{\partial y} (y(t), t) \\
    \sigma(t_k) = \sigma_k.
\end{cases}
\end{split}\]</div>
<p>Other important definitions are:</p>
<p><span class="math notranslate nohighlight">\(\textbf{Local truncation error:}\)</span> Is the difference between the exact expression and its numerical approximation in a certain point and with a certain domain discretization. If the domain is equally spaced by <span class="math notranslate nohighlight">\(h\)</span> is often denoted by <span class="math notranslate nohighlight">\(\tau(h,t_0)\)</span> being <span class="math notranslate nohighlight">\(t_0\)</span> the point.</p>
<p><span class="math notranslate nohighlight">\(\textbf{Order of the local truncation error:}\)</span> the local truncation error (which depends on the <span class="math notranslate nohighlight">\(h\)</span> spacing of the discretized domain) <span class="math notranslate nohighlight">\(\tau(h)\)</span> has order <span class="math notranslate nohighlight">\(n \in \mathbb{N}\)</span> if <span class="math notranslate nohighlight">\(\tau(h) = O(h^n) \)</span>, i.e., if there is constant <span class="math notranslate nohighlight">\(M \in \mathbb{R}\)</span> and <span class="math notranslate nohighlight">\(h_0 \in \mathbb{R}\)</span> such that <span class="math notranslate nohighlight">\(\tau(h) \leq M h^n\)</span>, <span class="math notranslate nohighlight">\(\forall h \leq h_0\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\textbf{Global error:}\)</span> Is the difference between the approximation given by the method for the solution of the problem on a certain point and the exact one (unlike the local truncation error, here we take the solution we got, not the expression used to find the approximation).</p>
<p><span class="math notranslate nohighlight">\(\textbf{Consistency:}\)</span> The method is said consistent if <span class="math notranslate nohighlight">\(\lim _{h \to 0} \frac{1}{h}\tau(h,x_0) = 0\)</span>.</p>
<p><span class="math notranslate nohighlight">\(\textbf{Obs.:}\)</span> For consistency, we usually only analyse for the linear part of the Cauchy problem, since this is the part that most influences in the consistency.</p>
<p><span class="math notranslate nohighlight">\(\textbf{Order of consistency:}\)</span> is the smallest order (varying the points at which the local error is calculated) of the local truncation error.</p>
<p><span class="math notranslate nohighlight">\(\textbf{Convergence:}\)</span> A numerical method is convergent if, and only if, for any well-posed Cauchy problem and for every <span class="math notranslate nohighlight">\(t \in (t_0, T)\)</span>,</p>
<div class="math notranslate nohighlight">
\[\lim_{h \to 0} e_k = 0\]</div>
<p>with <span class="math notranslate nohighlight">\(t - t_0 = kh\)</span> fixed and <span class="math notranslate nohighlight">\(e_k\)</span> denoting the global error on <span class="math notranslate nohighlight">\(t_k\)</span> (following the past notation).</p>
<p><span class="math notranslate nohighlight">\(\textbf{Theorem:}\)</span> A one-step explicit method given by</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y_0 = y(t_0) \\
y_{k+1} = y_{k} + h \phi (t_{k},y_{k},h)
\end{split}\]</div>
<p>such that <span class="math notranslate nohighlight">\(\phi\)</span> is Lipschitzian in y, continuous in their arguments, and consistent for any well-posed Cauchy problem is convergent. Besides that, the convergence order is greater or equal to the consistency order.</p>
<p><span class="math notranslate nohighlight">\(\textit{Prove:}\)</span> [3] pág 29-31.</p>
</section>
<section id="examples">
<h4>Examples<a class="headerlink" href="#examples" title="Permalink to this headline">#</a></h4>
<p>Euler method:</p>
<div class="math notranslate nohighlight">
\[
    \phi (t_{k},y_{k},h) = f(t_{k},y_{k})
\]</div>
<p>Modified Euler method:</p>
<div class="math notranslate nohighlight">
\[
    \phi (t_{k},y_{k},h) = \frac{1}{2} \left[ f(t_{k},y_{k}) + f(t_{k+1},y_{k} + h f(t_{k},y_{k})) \right]
\]</div>
<p>Midpoint method:</p>
<div class="math notranslate nohighlight">
\[
    \phi (t_{k},y_{k},h) = f(t_{k} + \frac{h}{2},y_{k} + \frac{h}{2} f(t_{k},y_{k}))
\]</div>
<p>Classic Runge-Kutta (RK 4-4):</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    \phi (t_{k},y_{k},h) = \frac{1}{6} \left( \kappa_1 + 2 \kappa_2 + 2 \kappa_3 + \kappa_4 \right), \text{with }\\
    \kappa_1 = f(t_{k},y_{k})\\
    \kappa_2 = f(t_{k} + \frac{h}{2},y_{k} + \frac{h}{2} \kappa_1)\\
    \kappa_3 = f(t_{k} + \frac{h}{2},y_{k} + \frac{h}{2} \kappa_2)\\
    \kappa_4 = f(t_{k} + h, y_{k} + h \kappa_3)
\end{split}\]</div>
<p>Further detailing the Euler mathod, explicit one-step method of</p>
<div class="math notranslate nohighlight">
\[
    \phi (t_{k},y_{k},h) = f(t_{k},y_{k}),
\]</div>
<p>an analysis on stability, convergence and order of convergence is done on the appendix.</p>
</section>
</section>
<span id="document-cap3"></span><section class="tex2jax_ignore mathjax_ignore" id="important-concepts-for-the-study-of-exponential-methods">
<h3>Important concepts for the study of exponential methods<a class="headerlink" href="#important-concepts-for-the-study-of-exponential-methods" title="Permalink to this headline">#</a></h3>
<p>In this chapter, a review on <span class="math notranslate nohighlight">\(\phi\)</span> functions is done, because of its need when applying exponential methods in systems of ODE with initial value. Besides that, the format of the treated problem is shown.</p>
<p>It is worth remembering the importance of the matrix exponential for the linear problems treated here, which is in the
appendix.</p>
<section id="linear-problem">
<h4>Linear problem<a class="headerlink" href="#linear-problem" title="Permalink to this headline">#</a></h4>
<p>The linear problem is, following with the used notation:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    y'(t) + \lambda y(t) = g(y(t), t), t \in (t_0, T) \\
    y(t_0) = y_0 
    \text{,}
\end{cases}
\end{split}\]</div>
<p>the one with <span class="math notranslate nohighlight">\(g \equiv 0.\)</span></p>
<p>So, generaly, it is of the form:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{cases}
    y'(t) = A y(t), t \in (t_0, T) \\
    y(t_0) = y_0 
    \text{,}
\end{cases}
\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(A \in \mathscr{M}_{N \times N}(\mathbb{C}), N \in \mathbb{N}\)</span>  (remembering that a matrix <span class="math notranslate nohighlight">\(1 \times 1\)</span> is simply a number).</p>
<p>Because <span class="math notranslate nohighlight">\(A y(t)\)</span> is a <span class="math notranslate nohighlight">\(C^1\)</span> function in <span class="math notranslate nohighlight">\(y\)</span>, continuous in <span class="math notranslate nohighlight">\(t\)</span> and <span class="math notranslate nohighlight">\(t \in (t_0, T)\)</span>, a limited interval, by the existence and uniqueness theorem, there is a single solution of the problem.</p>
<p>Since</p>
<div class="math notranslate nohighlight">
\[
    \frac{d}{dt}y_0e^{A(t-t_0)} \doteq \lim_{h\to0} \frac{y_0e^{A(t-t_0+h)}-y_0e^{A(t-t_0)}}{h}
\]</div>
<div class="math notranslate nohighlight">
\[
    = y_0e^{(t-t_0)A}\lim_{h\to0} \frac{e^{Ah}-I}{h} 
\]</div>
<div class="math notranslate nohighlight">
\[
    = y_0e^{(t-t_0)A}\lim_{h\to0} \frac{Ae^{Ah}}{1} 
\]</div>
<div class="math notranslate nohighlight">
\[
    = y_0e^{(t-t_0)A} \frac{Ae^{A0}}{1}
\]</div>
<div class="math notranslate nohighlight">
\[
    = y_0e^{(t-t_0)A} A I = A y_0e^{(t-t_0)A} 
\]</div>
<p>using L’Hôpital’s rule on the second equality and noting that <span class="math notranslate nohighlight">\(A(t-t_0+h) = A(t-t_0)+Ah\)</span> and <span class="math notranslate nohighlight">\(A(t-t_0) \cdot Ah = (t-t_0)hAA = Ah \cdot A(t-t_0)\)</span>, so it was possible to apply the last proposition and make <span class="math notranslate nohighlight">\(e^{A(t-t_0+h)} = e^{A(t-t_0)} \cdot e^{Ah}\)</span>,</p>
<p>taking</p>
<div class="math notranslate nohighlight">
\[
    y(t) = y_0e^{A(t-t_0)},
\]</div>
<div class="math notranslate nohighlight">
\[
    y'(t) = A y_0 e^{(t-t_0)A} = A y(t) \text{ and } y(t_0) = y_0 e^{(t_0-t_0)A} = y_0 I = y_0.
\]</div>
<p>So, the solution for the general linear problem is <span class="math notranslate nohighlight">\(y(t)=y_0 e^{A(t-t_0)}\)</span>.</p>
<p>All information about matrix exponential is from [4].</p>
</section>
<section id="general-problem">
<h4>General problem<a class="headerlink" href="#general-problem" title="Permalink to this headline">#</a></h4>
<p>Returning to the general case</p>
<p><span class="math notranslate nohighlight">\(\begin{cases}
    y'(t) + \lambda y(t) = g(y(t), t), t \in (t_0, T) \\
    y(t_0) = y_0 
    \text{,}
\end{cases}\)</span></p>
<p>there is the variation of constants formula:</p>
<div class="math notranslate nohighlight">
\[
    y(t) = e^{-t \lambda}y_0 + \int_{t_0}^t e^{-\lambda(t-\tau)} g(y(\tau), \tau) d\tau.
\]</div>
<p>This well known implicit function, gives a solution of the problem.</p>
<p>If the integral part can be solved, there is a explicit solution, and if the problem satisfies the hypotesis of the Piccard problem, being Lipschitz in <span class="math notranslate nohighlight">\(t\)</span>, this is the only solution.</p>
<p>This formula is the basis of all the exponential methods.</p>
</section>
<section id="phi-functions">
<h4><span class="math notranslate nohighlight">\(\phi\)</span> functions<a class="headerlink" href="#phi-functions" title="Permalink to this headline">#</a></h4>
<p>Before introducing exponential methods, it is useful to present the <span class="math notranslate nohighlight">\(\phi\)</span> functions.</p>
<p>They are <span class="math notranslate nohighlight">\(\mathbb{C} \rightarrow \mathbb{C}\)</span> functions defined as:</p>
<div class="math notranslate nohighlight">
\[
  \phi_0 (z) = e^z;
\]</div>
<div class="math notranslate nohighlight">
\[
  \phi_n (z) = \int_{0}^{1} e^{(1-\tau)z} \frac{\tau^{n-1}}{(n-1)!} \,d\tau, n \geq 1.
\]</div>
<p>By integration by parts,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \phi_{n+1} (z) = \int_{0}^{1} e^{(1-\tau)z} \frac{\tau^n}{n!} \,d\tau \\
  = - \frac{e^{(1-1)z}}{z} \frac{1^n}{n!} + \frac{e^{(1-0)z}}{z} \frac{0^n}{l!} - \int_{0}^{1} -\frac{e^{(1-\tau)z}}{z} \frac{\tau^{n-1}}{(n-1)!} \,d\tau \\
  = - \frac{1}{n!z} + \frac{1}{z}\int_{0}^{1} e^{(1-\tau)z} \frac{\tau^{n-1}}{(n-1)!} \,d\tau.
\end{split}\]</div>
<p>Since</p>
<div class="math notranslate nohighlight">
\[
  \phi_n(0) = \int_{0}^{1} e^0 \frac{\tau^{n-1}}{(n-1)!} \,d\tau = \int_{0}^{1} \frac{\tau^{n-1}}{(n-1)!} \,d\tau = \frac{1^n}{n!} - 0 = \frac{1}{n!},
\]</div>
<div class="math notranslate nohighlight">
\[
  \phi_{n+1}(z) = \frac{\phi_n(z) - \phi_n(0)}{z}, \textbf{the recursive characterization}.
\]</div>
<p>By the properties of integral [5], if <span class="math notranslate nohighlight">\(h \in \mathbb{R}^*, t_k \in \mathbb{R}, t_k+h = t_{k+1},\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \phi_n (z) = \int_{0}^{1} e^{(1-\tau)z} \frac{\tau^{n-1}}{(n-1)!} \,d\tau \\
  = \frac{1}{h}\int_{0}^{h} e^{\frac{(h-\tau)z}{h}} \frac{\tau^{n-1}}{h^{n-1}(n-1)!} \,d\tau \\
  = \frac{1}{h}\int_{t_k}^{t_k + h} e^{\frac{(h-\tau+t_k)z}{h}} \frac{(\tau - t_k)^{n-1}}{h^{n-1}(n-1)!} \,d\tau,
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
  \phi_n (z) = \frac{1}{h^l}\int_{t_k}^{t_{k+1}} e^{\frac{1}{h}(t_{k+1}-\tau)z} \frac{(\tau - t_k)^{n-1}}{(n-1)!} \,d\tau.
\]</div>
<p>Information from [1].</p>
</section>
</section>
<span id="document-cap4"></span><section class="tex2jax_ignore mathjax_ignore" id="exponential-methods">
<h3>Exponential methods<a class="headerlink" href="#exponential-methods" title="Permalink to this headline">#</a></h3>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from basecode import *
</pre></div>
</div>
</div>
</div>
<p>In this chapter, exponential methods are introduced, with further analysis of some of them, being tested and compared to more classical equivalents.</p>
<p>All the codes that created the convergence and deduction tables are in the appendix.</p>
<p>The stiff problem used in all the convergence tables is the following one taken from [1]:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    u'(t) + 100 u(t) = \sin(t)\\
    u(0) = 1.
\end{split}\]</div>
<p>Solution:</p>
<div class="math notranslate nohighlight">
\[
u(t) = \exp(-100t)+\frac{\exp(-100t)+100\sin(t)-\cos(t)}{1+100^2}.
\]</div>
<section id="exponential-euler-method">
<h4>Exponential Euler method<a class="headerlink" href="#exponential-euler-method" title="Permalink to this headline">#</a></h4>
<p>Expression:</p>
<div class="math notranslate nohighlight">
\[
  y(t_{k+1}) = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h \lambda}}{\lambda} + O(h^2).
\]</div>
<p>Table of convergence:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>n</p></th>
<th class="head"><p>h = <span class="math notranslate nohighlight">\(\frac{1}{h}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\tau(0,h)\)</span></p></th>
<th class="head"><p>q = <span class="math notranslate nohighlight">\(\frac{tau(0,h)}{tau(0, 2h)}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128</p></td>
<td><p>0.0078125</p></td>
<td><p>4.398075514689716e-05</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>256</p></td>
<td><p>0.00390625</p></td>
<td><p>2.074422525626487e-05</p></td>
<td><p>1.0841625981445133</p></td>
</tr>
<tr class="row-even"><td><p>512</p></td>
<td><p>0.001953125</p></td>
<td><p>1.0056221183126109e-05</p></td>
<td><p>1.0446214904461004</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>0.0009765625</p></td>
<td><p></p></td>
<td><p></p></td>
</tr>
</tbody>
</table>
<p>The table proved the order of conergence given by the deduction, and, comparing to the one of the classic Euler method:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>n</p></th>
<th class="head"><p>h = <span class="math notranslate nohighlight">\(\frac{1}{h}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\tau(0,h)\)</span></p></th>
<th class="head"><p>q = <span class="math notranslate nohighlight">\(\frac{tau(0,h)}{tau(0, 2h)}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128</p></td>
<td><p>0.0078125</p></td>
<td><p>0.2391072699739873</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>256</p></td>
<td><p>0.00390625</p></td>
<td><p>0.08650412059872986</p></td>
<td><p>1.466817233501749</p></td>
</tr>
<tr class="row-even"><td><p>512</p></td>
<td><p>0.001953125</p></td>
<td><p>0.039214210532948934</p></td>
<td><p>1.1413923006132296</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>0.0009765625</p></td>
<td><p>0.018739566082401515</p></td>
<td><p>1.0652890085799935</p></td>
</tr>
</tbody>
</table>
<p>the exponential one has much better approximations since the beginning, proving the efficiency of the exponential method.</p>
</section>
<section id="exponential-time-differencing-methods-etd">
<h4>Exponential time differencing methods (ETD)<a class="headerlink" href="#exponential-time-differencing-methods-etd" title="Permalink to this headline">#</a></h4>
<p>Expression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h \lambda}y(t_k) +
h\phi_1(-\lambda h) g(y(t_k), t_k) +
h^2\phi_2(-\lambda h) \frac{dg}{dt}(y(t_k), t_k) +
h^3\phi_3(-\lambda h)\frac{d^2g}{dt^2} (y(t_k), t_k) +
\dotsi + \\
h^n\phi_n(-\lambda h) \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k)+
O(h^{n+1}).
\end{split}\]</div>
<p>It is possible to note that the exponential euler is essentially the exponential time differencing method of order 1.</p>
<p>In the same way as Taylor methods, the problem here is that at the expense of a higher order of convergence, ends up requiring the evaluation and implementation of multiple derivatives that may not even be easy to calculate. It can be avoided using Runge-Kutta methods, the next to be analyzed.</p>
</section>
<section id="exponential-time-differencing-methods-with-runge-kutta-time-stepping">
<h4>Exponential time differencing methods with Runge-Kutta time stepping<a class="headerlink" href="#exponential-time-differencing-methods-with-runge-kutta-time-stepping" title="Permalink to this headline">#</a></h4>
<p>Here, the exponential Runge-Kutta were compared to methods from deductions following the classic runge-kutta approach in the constant variation formula.</p>
<p>All convergence tables prove the deduced order.</p>
<p>However, it is remarkable how much better the exponential methods are in relation to the new ones presented here, showing that we cannot be naive and apply the integral approximations expecting an exponential Runge-Kutta performance, the treatment must be exact for the linear part, as is done in all exponential methods.</p>
<section id="exponential-trapezoidal-rule">
<h5>Exponential - Trapezoidal rule<a class="headerlink" href="#exponential-trapezoidal-rule" title="Permalink to this headline">#</a></h5>
<p>Expression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  h \phi_1 (-\lambda h) g(y(t_k), t_k) +
  \left[g(a_k, t_{k+1}) - g(y(t_k), t_k) \right] h \phi_2 (-\lambda h) + \\
  + O(h^3) \\
  \text{with } a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h\lambda}}{\lambda}.
\end{split}\]</div>
<p>Convergence table:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>n</p></th>
<th class="head"><p>h = <span class="math notranslate nohighlight">\(\frac{1}{h}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\tau(0,h)\)</span></p></th>
<th class="head"><p>q = <span class="math notranslate nohighlight">\(\frac{tau(0,h)}{tau(0, 2h)}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128</p></td>
<td><p>0.0078125</p></td>
<td><p>4.186569175362864e-08</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>256</p></td>
<td><p>0.00390625</p></td>
<td><p>1.0575183428604418e-08</p></td>
<td><p>1.985085775819591</p></td>
</tr>
<tr class="row-even"><td><p>512</p></td>
<td><p>0.001953125</p></td>
<td><p>2.652380943352073e-09</p></td>
<td><p>1.9953227875115886</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>0.0009765625</p></td>
<td><p>6.638462730912398e-10</p></td>
<td><p>1.9983668943519293</p></td>
</tr>
</tbody>
</table>
</section>
<section id="naive-deduction-trapezoidal-rule">
<h5>Naive deduction - Trapezoidal rule<a class="headerlink" href="#naive-deduction-trapezoidal-rule" title="Permalink to this headline">#</a></h5>
<p>Expression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h \lambda}y(t_k) + \frac{h}{2} \left[ e^{-\lambda h} g(y(t_k), t_k) + g(a_k, t_{k+1}) \right] +  O(h^3) \\
    \text{with } a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) h \phi_1 (-\lambda h).
\end{split}\]</div>
<p>Convergence table:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>n</p></th>
<th class="head"><p>h = <span class="math notranslate nohighlight">\(\frac{1}{h}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\tau(0,h)\)</span></p></th>
<th class="head"><p>q = <span class="math notranslate nohighlight">\(\frac{tau(0,h)}{tau(0, 2h)}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128</p></td>
<td><p>0.0078125</p></td>
<td><p>0.0004242643044311458</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>256</p></td>
<td><p>0.00390625</p></td>
<td><p>0.00010714498082271644</p></td>
<td><p>1.9853990333325726</p></td>
</tr>
<tr class="row-even"><td><p>512</p></td>
<td><p>0.001953125</p></td>
<td><p>2.6871031228085582e-05</p></td>
<td><p>1.9954406751889993</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>0.0009765625</p></td>
<td><p>6.725136514989377e-06</p></td>
<td><p>1.9984162299862431</p></td>
</tr>
</tbody>
</table>
</section>
<section id="exponential-third-order">
<h5>Exponential - Third order<a class="headerlink" href="#exponential-third-order" title="Permalink to this headline">#</a></h5>
<p>Expression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  g\left(c'_k, t_{k+\frac{1}{2}}\right)
  h \phi_1(-h \lambda) + 
  \\
  \left[g(c_k, t_{k+1}) - g(y(t_k), t_k)\right]
  \left( h \phi_2 (-h \lambda) - \frac{h \phi_1(-h \lambda)}{2} \right) +
  \\
  + 4 \left[g(c_k, t_{k+1}) + g(y(t_k), t_k) - 2 g\left(c'_k, t_{k+\frac{1}{2}}\right) \right]
  \left( h \phi_3 (-h \lambda) + 
  \\
  \frac{h \phi_1(-h \lambda)}{8} - \frac{h \phi_2(-h \lambda)}{2} \right) + O(h^4),
\end{split}\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  c_k = e^{-h \lambda} y(t_k) +
  h \phi_1 (-\lambda h) g(y(t_k), t_k) +
  \\
  \left[g(a_k, t_{k+1}) - g(y(t_k), t_k) \right] h \phi_2 (-\lambda h),
  \\
  a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) h \phi_1(-h\lambda),
  \\
  c'_k = e^{- \frac{h \lambda}{2}} y(t_k) +
  \\
  \frac{h}{2} \phi_1 \left(- \frac{\lambda h}{2} \right) g(y(t_k), t_k) +
  \\
  \left[g\left(a'_k, t_{k+\frac{1}{2}}\right) - g(y(t_k), t_k) \right] \frac{h}{2} \phi_2 \left(-\frac{\lambda h}{2}\right),
  \\
  a'_k = e^{-\frac{h \lambda}{2}}y(t_k) + 
  \\
  g(y(t_k), t_k) \frac{h}{2} \phi_1\left(-\frac{h \lambda}{2}\right).
\end{split}\]</div>
<p>Convergence table:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>n</p></th>
<th class="head"><p>h = <span class="math notranslate nohighlight">\(\frac{1}{h}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\tau(0,h)\)</span></p></th>
<th class="head"><p>q = <span class="math notranslate nohighlight">\(\frac{tau(0,h)}{tau(0, 2h)}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128</p></td>
<td><p>0.0078125</p></td>
<td><p>5.0853024048669315e-12</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>256</p></td>
<td><p>0.00390625</p></td>
<td><p>3.212833644961055e-13</p></td>
<td><p>3.9844153810116354</p></td>
</tr>
<tr class="row-even"><td><p>512</p></td>
<td><p>0.001953125</p></td>
<td><p>2.0132983821752326e-14</p></td>
<td><p>3.996213373698299</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>0.0009765625</p></td>
<td><p>1.2602766052971504e-15</p></td>
<td><p>3.997748687591092</p></td>
</tr>
</tbody>
</table>
<p>Better than what expected, giving order 4.</p>
</section>
<section id="id1">
<h5>Naive deduction - Trapezoidal rule<a class="headerlink" href="#id1" title="Permalink to this headline">#</a></h5>
<p>Expression:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda}y(t_k) + 
  \\
  \frac{h}{6} \left[ e^{-\lambda h} g(y(t_k), t_k) + 
  \\
  4 e^{-\frac{ \lambda h}{2}} g\left( b'_{k}, t_k + 
  \\
  \frac{h}{2} \right) + g(b_k, t_{k+1}) \right] +  O(h^4), 
  \\
  \text{with } b'_{k} = e^{- \frac{h \lambda}{2}}y(t_k) + 
  \\
  \frac{h}{4} \left[ e^{- \frac{h \lambda}{2}} g(y(t_k), t_k) + 
  \\
  g \left(a'_{k}, t_k + \frac{h}{2} \right) \right], 
  \\
  b_k = e^{-h \lambda}y(t_k) + 
  \\
  \frac{h}{2} \left[ e^{-\lambda h} g(y(t_k), t_k) + 
  \\
  g(a_k, t_{k+1}) \right], \\
  a'_{k} = e^{- \frac{h \lambda}{2}} y(t_k) + 
  \\
  g(y(t_k), t_k) \frac{h}{2} \phi_1 \left(-\lambda \frac{h}{2} \right), 
  \\
  a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) h \phi_1 (-\lambda h).
\end{split}\]</div>
<p>Convergence table:</p>
<table class="colwidths-auto docutils align-default">
<thead>
<tr class="row-odd"><th class="head"><p>n</p></th>
<th class="head"><p>h = <span class="math notranslate nohighlight">\(\frac{1}{h}\)</span></p></th>
<th class="head"><p><span class="math notranslate nohighlight">\(\tau(0,h)\)</span></p></th>
<th class="head"><p>q = <span class="math notranslate nohighlight">\(\frac{tau(0,h)}{tau(0, 2h)}\)</span></p></th>
</tr>
</thead>
<tbody>
<tr class="row-even"><td><p>128</p></td>
<td><p>0.0078125</p></td>
<td><p>1.083876968009309e-06</p></td>
<td><p>-</p></td>
</tr>
<tr class="row-odd"><td><p>256</p></td>
<td><p>0.00390625</p></td>
<td><p>6.883813637344194e-08</p></td>
<td><p>3.9768491535433466</p></td>
</tr>
<tr class="row-even"><td><p>512</p></td>
<td><p>0.001953125</p></td>
<td><p>4.322307012305515e-09</p></td>
<td><p>3.9933345852265947</p></td>
</tr>
<tr class="row-odd"><td><p>1024</p></td>
<td><p>0.0009765625</p></td>
<td><p>2.705360744453822e-10</p></td>
<td><p>3.9979086629155343</p></td>
</tr>
</tbody>
</table>
<p>Also with order 4, better than what expected.</p>
</section>
</section>
<section id="graphics">
<h4>Graphics<a class="headerlink" href="#graphics" title="Permalink to this headline">#</a></h4>
<p>Next, it is shown graphics from the same problem, but first showing the error as the linear part, <span class="math notranslate nohighlight">\(100\)</span>, changes from <span class="math notranslate nohighlight">\(0\)</span> to <span class="math notranslate nohighlight">\(100\)</span> and next with <span class="math notranslate nohighlight">\(\lambda = 100\)</span> but changng the time step.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n = 128
lmba0 = 1
lmbaf = 100
t0 = 0.0
tf = 1.0
x0 = np.array([1])
lmba_1D_classic, domain = errors_for_lambdas_array(n, classic_euler, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_exponential, domain = errors_for_lambdas_array(n, exponential_euler, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_etd2rk, domain = errors_for_lambdas_array(n, etd2rk, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_etd2rk_trapezoidal_naive, domain = errors_for_lambdas_array(n, etd2rk_trapezoidal_naive, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_etd3rk_similar, domain = errors_for_lambdas_array(n, etd3rk_similar, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_etd3rk_naive, domain = errors_for_lambdas_array(n, etd3rk_naive, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_rk2, domain = errors_for_lambdas_array(n, rk2, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_rk4, domain = errors_for_lambdas_array(n, rk4, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/miki/IC/Relatorio_github/basecode.py:247: ComplexWarning: Casting complex values to real discards the imaginary part
  return np.sqrt(float(np.sum(v)/x_approx.size)) #normalized
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>matrix_1D = [lmba_1D_classic, lmba_1D_exponential, lmba_1D_rk2, lmba_1D_etd2rk_trapezoidal_naive, lmba_1D_etd2rk, lmba_1D_rk4, lmba_1D_etd3rk_naive, lmba_1D_etd3rk_similar]
names = [&#39;classic euler&#39;, &#39;exponential euler&#39;, &#39;rk2&#39;, &#39;etd2rk naive&#39;, &#39;etd2rk&#39;, &#39;rk4&#39;, &#39;etd3rk naive&#39;, &quot;etd3rk (similar)&quot;]
fig, ax = graphic_2D(8*[domain], matrix_1D, names, &quot;lambda&quot;, &quot;error&quot;, &quot;1D problem from [1]&quot;, False, True)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cap4_14_0.png" src="_images/cap4_14_0.png" />
</div>
</div>
<p>Here it is notable that as the lambda increases, and so does the stiffness, the exponential methods deal really well, even dropping the error, since the exponential part is precisely solved, so, as it gains more relevance, the method perfoms better. Meanwhile, the other methods (classic and naive) start to decline, dealing badly with the stiffness. Just as predicted.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 10
k = 10
lmba = 100
A = lmba * np.array([[1]])
t0 = 0.0
tf = 1.0
x0 = np.array([1])
n_1D_classic, domain = errors_2x(n0, k, classic_euler, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_exponential, domain = errors_2x(n0, k, exponential_euler, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_etd2rk, domain = errors_2x(n0, k, etd2rk, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_etd2rk_trapezoidal_naive, domain = errors_2x(n0, k, etd2rk_trapezoidal_naive, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_etd3rk_similar, domain = errors_2x(n0, k, etd3rk_similar, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_etd3rk_naive, domain = errors_2x(n0, k, etd3rk_naive, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_rk2, domain = errors_2x(n0, k, rk2, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_rk4, domain = errors_2x(n0, k, rk4, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>matrix_2D = [n_1D_classic, n_1D_exponential, n_1D_rk2, n_1D_etd2rk_trapezoidal_naive, n_1D_etd2rk, n_1D_rk4, n_1D_etd3rk_naive, n_1D_etd3rk_similar]
names = [&#39;classic euler&#39;, &#39;exponential euler&#39;, &#39;rk2&#39;, &#39;etd2rk naive&#39;, &#39;etd2rk&#39;, &#39;rk4&#39;, &#39;etd3rk naive&#39;, &quot;etd3rk (similar)&quot;]
fig_2D, ax_2D = graphic_2D(8*[1/domain], matrix_2D, names, &quot;h&quot;, &quot;error&quot;, &quot;1D problem with lmba = &quot;+str(lmba), False, True)
plt.xscale(&#39;log&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/cap4_17_0.png" src="_images/cap4_17_0.png" />
</div>
</div>
<p>Here is visually clear the orders already confirmed by the convergence tables.</p>
</section>
</section>
<span id="document-cap5"></span><section class="tex2jax_ignore mathjax_ignore" id="swing-spring-application">
<h3>Swing spring application<a class="headerlink" href="#swing-spring-application" title="Permalink to this headline">#</a></h3>
<p>Finally, an application is done for the swing spring problem.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from basecode import *
</pre></div>
</div>
</div>
</div>
<p>From [9], the model is, given <span class="math notranslate nohighlight">\(m, k, l\)</span> and <span class="math notranslate nohighlight">\(g\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \dot{\theta} = \frac{\rho_\theta}{mr^2}\\
  \dot{\rho_\theta} = -mgr \sin{\theta}\\
  \dot{r} = \frac{\rho_r}{m} \\
  \dot{\rho_r} = \frac{\rho^2_\theta}{mr^3} - k(r - l) + mg \cos{\theta}.
\end{split}\]</div>
<p>In matrix form,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \frac{d}{dt}\begin{bmatrix}
              \theta\\
              \rho_\theta\\
              r\\
              \rho_r
              \end{bmatrix}
  =
  \begin{bmatrix}
  0 &amp; \frac{1}{ml^2} &amp; 0 &amp; 0\\
  -mgl &amp; 0 &amp; 0 &amp; 0\\
  0 &amp; 0 &amp; 0 &amp; \frac{1}{m}\\
  0 &amp; 0 &amp; -k &amp; 0
  \end{bmatrix}
  \begin{bmatrix}
  \theta\\
  \rho_\theta\\
  r\\
  \rho_r
  \end{bmatrix}
  +
  \begin{bmatrix}
  \frac{\rho_\theta}{m} \left(\frac{1}{r^2}-\frac{1}{l^2}\right)\\
  -mg(r\sin{\theta}-l\theta)\\
  0\\
  \frac{\rho^2_\theta}{mr^3} + kl + mg \cos{\theta}
  \end{bmatrix}.
\end{split}\]</div>
<p>For <span class="math notranslate nohighlight">\(m = 1, l = 1, g = \pi^2, k = 100 \pi^2\)</span>, using the etd3rk method deduced, the best one tested,</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>def g_swing_spring(x, t):
  m = 1
  l = 1
  gr = np.pi**2
  k = 100 * np.pi**2
  vector = np.zeros(x.size)
  theta = x[0]
  p_theta = x[1]
  r = x[2]
  pr = x[3]
  vector[0] = p_theta/m * (1/r**2 - 1/l**2)
  vector[1] = -m*gr*(r*np.sin(theta)-l*theta)
  vector[3] = p_theta**2/(m*r**3) + k*l + m*gr*np.cos(theta)
  return vector
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>m = 1
l = 1
g = np.pi**2
k = 100 * np.pi**2
A = np.matrix([[0,-1/(m*l**2),0,0], [m*g*l,0,0,0], [0,0,0,-1/m], [0,0,k,0]])
n = 10000
t0 = 0.0
tf = 5.0
x0 = np.array([np.pi/2,3,l,1])
domains = 4*[np.arange(t0, tf, (tf-t0)/n)]
x = etd3rk_similar(t0, tf, n, x0, A, g_swing_spring)
names = [&#39;theta&#39;,&#39;p_theta&#39;, &#39;r&#39;, &#39;pr&#39;]
matrix1 = [x[0,:], x[1,:], x[2,:], x[3,:]]

fig1, ax1 = graphic_2D(domains, matrix1, names, &#39;t&#39;, &#39; &#39;, &#39;swing spring etdrk3 n = &#39;+str(n), False, False)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_12326/2307441015.py:11: ComplexWarning: Casting complex values to real discards the imaginary part
  vector[0] = p_theta/m * (1/r**2 - 1/l**2)
/tmp/ipykernel_12326/2307441015.py:12: ComplexWarning: Casting complex values to real discards the imaginary part
  vector[1] = -m*gr*(r*np.sin(theta)-l*theta)
/tmp/ipykernel_12326/2307441015.py:13: ComplexWarning: Casting complex values to real discards the imaginary part
  vector[3] = p_theta**2/(m*r**3) + k*l + m*gr*np.cos(theta)
</pre></div>
</div>
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/home/miki/.local/lib/python3.10/site-packages/matplotlib/cbook/__init__.py:1369: ComplexWarning: Casting complex values to real discards the imaginary part
  return np.asarray(x, float)
</pre></div>
</div>
<img alt="_images/cap5_6_2.png" src="_images/cap5_6_2.png" />
</div>
</div>
</section>
</div>
</section>
<span id="document-References"></span><section class="tex2jax_ignore mathjax_ignore" id="references">
<h2>References<a class="headerlink" href="#references" title="Permalink to this headline">#</a></h2>
<ol class="simple">
<li><p>HOCHBRUCK, M.; OSTERMANN, A. Exponential integrators. Acta Numer, Cambridge Univ Press, v. 19, p. 209–286, 2010.</p></li>
<li><p>BURDEN, Richard L.; FAIRES, J. Douglas. Numerical Analysis. 9.ed. Boston:Brooks/Cole, 2010. p.348-353.</p></li>
<li><p>ROMA, Alexandre. Lecture notes. Introdução à Resolução Numérica do Problema de Cauchy (Introduction to numerical resolution of Cauchy problem), MAP5002. Jan. and Feb. 2023. IME-USP University of São Paulo.</p></li>
<li><p>TAL, Fábio A. Lecture notes. Técnicas em Teoria do Controle (techniques in control theory), MAP2321. Aug. to Dez. 2022. IME-USP University from São Paulo.</p></li>
<li><p>Apostol, T.M. Calculus v. 1. Blaisdell book in pure and applied mathematics. <a class="reference external" href="https://books.google.com.br/books?id=sR_vAAAAMAAJ">https://books.google.com.br/books?id=sR_vAAAAMAAJ</a>. 1961. Blaisdell Publishing Company.</p></li>
<li><p>S.M. Cox, P.C. Matthews, Exponential Time Differencing for Stiff Systems, Journal of Computational Physics, Volume 176, Issue 2, 2002, Pages 430-455, ISSN 0021-9991, <a class="reference external" href="https://doi.org/10.1006/jcph.2002.6995">https://doi.org/10.1006/jcph.2002.6995</a>.</p></li>
<li><p>Hochbruck, Marlis, and Alexander Ostermann. “Explicit Exponential Runge-Kutta Methods for Semilinear Parabolic Problems.” SIAM Journal on Numerical Analysis, vol. 43, no. 3, 2006, pp. 1069–90. JSTOR, <a class="reference external" href="http://www.jstor.org/stable/4101280">http://www.jstor.org/stable/4101280</a>. Accessed 27 June 2023.</p></li>
<li><p>DOBRUSHKIN, Vladimir. “MATHEMATICA TUTORIAL
for the Second Course. Part III: Spring Pendulum”. Monday, September 4, 2023 10:24:40 PM. <a class="reference external" href="https://www.cfm.brown.edu/people/dobrush/am34/Mathematica/ch3/spendulum.html">https://www.cfm.brown.edu/people/dobrush/am34/Mathematica/ch3/spendulum.html</a></p></li>
<li><p>Lynch, Peter. (2000). The Swinging Spring: A Simple Model of Atmospheric Balance. <a class="reference external" href="https://maths.ucd.ie/~plynch/Publications/AOD_Paper.pdf">https://maths.ucd.ie/~plynch/Publications/AOD_Paper.pdf</a></p></li>
</ol>
</section>
<span id="document-Participation_in_scientific_event,_list_of_publications_and_list_of_papers_prepared_or_submitted"></span><section class="tex2jax_ignore mathjax_ignore" id="participation-in-scientific-event-list-of-publications-and-list-of-papers-prepared-or-submitted">
<h2>Participation in scientific event, list of publications and list of papers prepared or submitted<a class="headerlink" href="#participation-in-scientific-event-list-of-publications-and-list-of-papers-prepared-or-submitted" title="Permalink to this headline">#</a></h2>
<p>Nothing to declare.</p>
</section>
<span id="document-appendix"></span><section class="tex2jax_ignore mathjax_ignore" id="appendix">
<h2>Appendix<a class="headerlink" href="#appendix" title="Permalink to this headline">#</a></h2>
<section id="code">
<h3>Code<a class="headerlink" href="#code" title="Permalink to this headline">#</a></h3>
<p>All the functions coded are in the following environment.</p>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>from math import *
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
from scipy.linalg import expm
from scipy import linalg

stab_lim = 1000.0

def classic_euler(t0, tf, n, x0, A, g):
    &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function) -&gt; np.matrix&#39;&#39;&#39;
    h = (tf-t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex_)
    x[:,0]=x0
    t = t0
    for i in range(1, n):
        x[:,i] = x[:,i-1] + h*(np.matmul(-A,x[:,i-1]) + g(x[:,i-1],t))
        t = t0 + i*h
        if np.any(x[:,i].real &gt; stab_lim):
            x[:,i] = np.nan
    return x

def exponential_euler(t0, tf, n, x0, A, g):
    &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function) -&gt; np.matrix&#39;&#39;&#39;
    h = (tf-t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex_)
    x[:,0] = x0
    t = t0
    exponential_matrix = expm(-h*A)
    hphi1 = calculate_hphi1(h, A)
    for i in range(1, n):
        x[:,i] = np.matmul(exponential_matrix, x[:,i-1]) + np.matmul(hphi1,g(x[:,i-1],t))
        t = t0 + i*h
    return x

def calculate_hphi1(h, A):
  &#39;&#39;&#39;(float, np.matrix) -&gt; np.matrix&#39;&#39;&#39;
  dim = A.shape[0]
  hphi1 = np.matmul(np.eye(dim)-expm(-h*A), linalg.inv(A))
  return hphi1

def calculate_hphi2(h, A, hphi1):
    #IT IS NOT H2PHI2
    &#39;&#39;&#39;(float, np.matrix, np.matrix) -&gt; np.matrix&#39;&#39;&#39;
    dim = A.shape[0]
    hphi2 = np.matmul(np.eye(dim)-hphi1/h, linalg.inv(A))
    return hphi2

def calculate_hphi3(h, A, hphi2):
    &#39;&#39;&#39;(float, np.matrix, np.matrix) -&gt; np.matrix&#39;&#39;&#39;
    dim = A.shape[0]
    hphi3 = np.matmul(1/2*np.eye(dim)-hphi2/h, linalg.inv(A))
    return hphi3

def etd2(t0, tf, n, x0, A, g, derivate_of_g):
    &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function, function) -&gt; np.matrix&#39;&#39;&#39;
    h = (tf-t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex_)
    x[:,0] = x0
    t = t0
    exponential_matrix = expm(-h*A)
    hphi1 = calculate_hphi1(h, A)
    hphi2 = calculate_hphi2(h, A, hphi1)
    for i in range(1, n):
        x[:,i] = np.matmul(exponential_matrix, x[:,i-1]) + np.matmul(hphi1,g(x[:,i-1],t)) + h*np.matmul(hphi2,derivate_of_g(x[:,i-1],t))
        t = t0 + i*h
    return x

def rk2(t0, tf, n, x0, A, g): #heun s method
    &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function) -&gt; np.matrix&#39;&#39;&#39;
    h = (tf-t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex_)
    x[:,0]=x0
    t = t0
    for i in range(1, n):
        a = x[:,i-1] + h*(np.matmul(-A,x[:,i-1]) + g(x[:,i-1],t))
        f1 = np.matmul(-A,x[:,i-1]) + g(x[:,i-1],t)
        f2 = np.matmul(-A,a) + g(a,t)
        x[:,i] = x[:,i-1] + .5 * h * (f1 + f2)
        t = t0 + i*h
        if np.any(x[:,i].real &gt; stab_lim):
            x[:,i] = np.nan
    return x

def etd2rk(t0, tf, n, x0, A, g):
    &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function) -&gt; np.matrix&#39;&#39;&#39;
    h = (tf-t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex_)
    x[:,0]=x0
    t = t0
    exponential_matrix = expm(-h*A)
    hphi1 = calculate_hphi1(h, A)
    hphi2 = calculate_hphi2(h, A, hphi1)
    for i in range(1, n):
        a = np.matmul(exponential_matrix, x[:,i-1]) + np.matmul(hphi1,g(x[:,i-1],t))
        x[:,i] = np.matmul(exponential_matrix, x[:,i-1]) + np.matmul(hphi1,g(x[:,i-1],t)) + np.matmul(hphi2,g(a, t0 + i*h)-g(x[:,i-1],t))
        t = t0 + i*h
    return x

def etd2rk_midpoint_rule(t0, tf, n, x0, A, g):
    &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function) -&gt; np.matrix&#39;&#39;&#39;
    h = (tf-t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex_)
    x[:,0]=x0
    t = t0
    exponential_matrix = expm(-h*A)
    exponential_matrix_2 = expm(-h/2*A)
    h_2phi1_2 = calculate_hphi1(h/2, A)
    hphi1 = calculate_hphi1(h, A)
    hphi2 = calculate_hphi2(h, A, hphi1)
    for i in range(1, n):
        b = np.matmul(exponential_matrix_2, x[:,i-1]) + np.matmul(h_2phi1_2,g(x[:,i-1],t))
        x[:,i] = np.matmul(exponential_matrix, x[:,i-1]) + np.matmul(hphi1,g(x[:,i-1],t)) + 2*np.matmul(hphi2,g(b, t + h/2)-g(x[:,i-1],t))
        t = t0 + i*h
    return x

def etd2rk_trapezoidal_naive(t0, tf, n, x0, A, g):
    &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function) -&gt; np.matrix&#39;&#39;&#39;
    h = (tf-t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex_)
    x[:,0]=x0
    t = t0
    exponential_matrix = expm(-h*A)
    hphi1 = calculate_hphi1(h, A)
    for i in range(1, n):
        a = np.matmul(exponential_matrix, x[:,i-1]) + np.matmul(hphi1,g(x[:,i-1],t))
        x[:,i] = np.matmul(exponential_matrix, x[:,i-1]) + .5 * h * (np.matmul(exponential_matrix, g(x[:,i-1],t)) + g(a, t0 + i*h))
        t = t0 + i*h
    return x

def etd2rk_midpoint_rule_naive(t0, tf, n, x0, A, g):
    &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function, np.matrix) -&gt; np.matrix&#39;&#39;&#39;
    h = (tf-t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex_)
    x[:,0]=x0
    t = t0
    exponential_matrix = expm(-h*A)
    exponential_matrix_2 = expm(-h/2*A)
    h_2phi1_2 = calculate_hphi1(h/2, A)
    for i in range(1, n):
        b = np.matmul(exponential_matrix_2, x[:,i-1]) + np.matmul(h_2phi1_2,g(x[:,i-1],t))
        x[:,i] = np.matmul(exponential_matrix, x[:,i-1]) + h * np.matmul(exponential_matrix_2, g(b, t+h/2))
        t = t0 + i*h
    return x

def rk4(t0, tf, n, x0, A, g):
    &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function) -&gt; np.matrix&#39;&#39;&#39;
    h = (tf-t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex_)
    x[:,0]=x0
    t = t0
    for i in range(1, n):
        k1 = np.matmul(-A,x[:,i-1]) + g(x[:,i-1],t)
        x2 = x[:,i-1] + h * k1 / 2
        k2 = np.matmul(-A,x2) + g(x2,t+h/2)
        x3 = x[:,i-1] + h * k2 / 2
        k3 = np.matmul(-A,x3) + g(x3,t+h/2)
        x4 = x[:,i-1] + h * k3
        k4 = np.matmul(-A,x4) + g(x4,t0 + i*h)
        x[:,i] = x[:,i-1] + h / 6 * (k1 + 2*k2 + 2*k3 + k4)
        t = t0 + i*h
        if np.any(x[:,i].real &gt; stab_lim):
            x[:,i] = np.nan
    return x

def etd3rk_similar(t0, tf, n, x0, A, g):
  &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function, np.matrix) -&gt; np.matrix&#39;&#39;&#39;
  h = (tf-t0)/n
  x = np.zeros((x0.size,n), dtype=np.complex_)
  x[:,0]=x0
  t = t0
  exponential_matrix = expm(-h*A)
  exponential_matrix_2 = expm(-h/2*A)
  hphi1 = calculate_hphi1(h, A)
  h_2phi1_2 = calculate_hphi1(h/2, A)
  hphi2 = calculate_hphi2(h, A, hphi1)
  h_2phi2_2 = calculate_hphi2(h/2, A, h_2phi1_2)
  hphi3 = calculate_hphi3(h, A, hphi2)
  for i in range(1, n):
    fst_term = np.matmul(exponential_matrix, x[:,i-1])
    fst_term_2 = np.matmul(exponential_matrix_2, x[:,i-1])
    a = fst_term + np.matmul(hphi1,g(x[:,i-1],t))
    a_ = fst_term_2 + np.matmul(h_2phi1_2,g(x[:,i-1],t))
    c = fst_term + np.matmul(hphi1,g(x[:,i-1],t)) + np.matmul(hphi2,g(a, t0 + i*h)-g(x[:,i-1],t))
    c_ = fst_term_2 + np.matmul(h_2phi1_2,g(x[:,i-1],t)) + np.matmul(h_2phi2_2,g(a_, t0 + i*h)-g(x[:,i-1],t))
    snd_term = np.matmul(hphi1, g(c_, t+h/2))
    trd_term = np.matmul(hphi2 - hphi1/2,g(c, t0 + i*h)-g(x[:,i-1],t))
    fth_term = 4 * np.matmul(hphi3+hphi1/8-hphi2/2, g(c, t0 + i*h)+g(x[:,i-1],t)-2*g(c_, t + h/2))
    x[:,i] = fst_term + snd_term + trd_term + fth_term
    t = t0 + i*h
  return x

def etd3rk(t0, tf, n, x0, A, g):
  &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function, np.matrix) -&gt; np.matrix&#39;&#39;&#39;
  h = (tf-t0)/n
  x = np.zeros((x0.size,n), dtype=np.complex_)
  x[:,0]=x0
  t = t0
  exponential_matrix = expm(-h*A)
  exponential_matrix_2 = expm(-h/2*A)
  hphi1 = calculate_hphi1(h, A)
  h_2phi1_2 = calculate_hphi1(h/2, A)
  hphi2 = calculate_hphi2(h, A, hphi1)
  h_2phi2_2 = calculate_hphi2(h/2, A, h_2phi1_2)
  hphi3 = calculate_hphi3(h, A, hphi2)
  for i in range(1, n):
    fst_term = np.matmul(exponential_matrix, x[:,i-1])
    fst_term_2 = np.matmul(exponential_matrix_2, x[:,i-1])
    a = fst_term_2 + np.matmul(h_2phi1_2,g(x[:,i-1],t))
    b = fst_term + np.matmul(hphi1,2*g(a,t+h/2)-g(x[:,i-1],t))
    snd_term = np.matmul(hphi1, g(a, t+h/2))
    trd_term = np.matmul(hphi2 - hphi1/2,g(b, t0 + i*h)-g(x[:,i-1],t))
    fth_term = 4 * np.matmul(hphi3+hphi1/8-hphi2/2, g(b, t0 + i*h)+g(x[:,i-1],t)-2*g(a, t + h/2))
    x[:,i] = fst_term + snd_term + trd_term + fth_term
    t = t0 + i*h
  return x

def etd3rk_naive(t0, tf, n, x0, A, g):
  &#39;&#39;&#39;(float, float, int, np.array, np.matrix, function, np.matrix) -&gt; np.matrix&#39;&#39;&#39;
  h = (tf-t0)/n
  x = np.zeros((x0.size,n), dtype=np.complex_)
  x[:,0]=x0
  t = t0
  exponential_matrix = expm(-h*A)
  exponential_matrix_2 = expm(-h/2*A)
  hphi1 = calculate_hphi1(h, A)
  h_2phi1_2 = calculate_hphi1(h/2, A)
  for i in range(1, n):
    fst_term = np.matmul(exponential_matrix, x[:,i-1])
    fst_term_2 = np.matmul(exponential_matrix_2, x[:,i-1])
    a = fst_term + np.matmul(hphi1,g(x[:,i-1],t))
    a_ = fst_term_2 + np.matmul(h_2phi1_2,g(x[:,i-1],t))
    c = fst_term + .5 * h * (np.matmul(exponential_matrix, g(x[:,i-1],t)) + g(a, t0 + i*h))
    c_ = fst_term_2 + .25 * h * (np.matmul(exponential_matrix_2, g(x[:,i-1],t)) + g(a_, t0 + i*h))
    snd_term = np.matmul(exponential_matrix, g(x[:,i-1],t))
    trd_term = 4*np.matmul(exponential_matrix_2, g(c_,t+h/2))
    fth_term = g(c, t0 + i*h)
    x[:,i] = fst_term + h*(snd_term + trd_term + fth_term)/6
    t = t0 + i*h
  return x

def error_2(x_approx, x_exact):
    &#39;&#39;&#39; (np.vector, np.vector) -&gt; float &#39;&#39;&#39;
    #make sure that x_approx and x_exact have the same lenght
    v = (x_approx - x_exact)*(x_approx - x_exact).conjugate()
    #^certainly pure real
    return np.sqrt(float(np.sum(v)/x_approx.size)) #normalized

def error_sup(x_approx, x_exact):
    &#39;&#39;&#39; (np.vector, np.vector) -&gt; float &#39;&#39;&#39;
    #make sure that x_approx and x_exact have the same lenght
    v = abs(x_approx - x_exact)
    return np.amax(v)

def g(x, t):
    &#39;&#39;&#39; (np.array, float) -&gt; float
        (x, t) -&gt; g(x, t)
    &#39;&#39;&#39;
    g = np.array([np.sin(t)])
    return g

def g_linear( x, t ):
    &#39;&#39;&#39; (np.array, float) -&gt; np.array
        (x, t) -&gt; g(x, t)
    &#39;&#39;&#39;
    g = np.zeros(x.size)
    return g

def sol( t ):
    &#39;&#39;&#39; (float, float) -&gt; float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    &#39;&#39;&#39;
    lmba = 100
    sol = np.exp(-lmba*t)+(np.exp(-lmba*t)+lmba*np.sin(t)-np.cos(t))/(1+lmba*lmba)
    return sol

def sol_given_lmba(lmba, t ):
    &#39;&#39;&#39; (float, float) -&gt; float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    &#39;&#39;&#39;
    sol = np.exp(-lmba*t)+(np.exp(-lmba*t)+lmba*np.sin(t)-np.cos(t))/(1+lmba*lmba)
    return sol

def vectorize_sol_given_lmba(lmba, t0, t1, n, sol):
    &#39;&#39;&#39;
    (float, float, float, int, function) -&gt; np.vector
    n is the number of steps
    &#39;&#39;&#39;
    x = np.zeros((sol(lmba,t0).size,n), dtype=np.complex_)
    h = (t1-t0)/n
    for i in range(n):
        x[:,i] = sol(lmba, t0+i*h)
    return x

def vectorize_sol(t0, t1, n, sol):
    &#39;&#39;&#39;
    (float, float, int, function) -&gt; np.vector
    n is the number of steps
    &#39;&#39;&#39;
    x = np.zeros((sol(t0).size,n), dtype=np.complex_)
    h = (t1-t0)/n
    for i in range(n):
        x[:,i] = sol(t0+i*h)
    return x

def A_1D(lmba):
  &#39;&#39;&#39;(int) -&gt; np.matrix&#39;&#39;&#39;
  return np.array([[lmba]])

def A_2D(lmba):
  &#39;&#39;&#39;(int) -&gt; np.matrix&#39;&#39;&#39;
  return np.array([[0, -lmba],[lmba, 0]])

def errors_for_lambdas_array(n, method, t0, tf, x0, lmba0, lmbaf, Af, g, sol_given_lmba, vectorize_sol_given_lmba, error):
    &#39;&#39;&#39;
    This function is a variation of the errors_array function. Here, the linear
    part of the problem is varying instead of the number of steps, which is now
    fixed.
    This function will RETURN 2 arrays.
    The first one has the errors of the approximations given by the method with
    coefficient of the linear part of the ploblem
    A = Af(lmba0), Af(lmba0+1), Af(lmba0+2), ..., Af(lmbaf-1).
    The second is [lmba0, lmba0+1, lmba0+2, ..., lmbaf-1]

    RECEIVES:
    n is the number of steps. (int)
    method have arguments (t0, tf, n, x0, lmba, g) and return a
    np.vector of length n (0, 1, 2, ..., n-1), n is the number of steps. (function)
    t0 is the initial point of the approximation. (float)
    tf is the last one. (float)
    x0 is the initial value of the Cauchy problem. (np.array)
    lmba0 and lmbaf are integers as described before. (int)
    Af is a function that receives the stiffness parameter and returns the
    corresponding linear coefficient. (function)
    g is a function (float, float) -&gt; (float). (function)
    sol is a function (float) -&gt; (float). (function)
    vectorize_sol is a function that &quot;transforms sol in a vector&quot; (function)
    (float, float, int, function) -&gt; (np.array)
    (t0, tf, n, sol) -&gt; np.array([sol[t0], sol[t0+h], sol[t0+2h], ..., sol[tf-1]])
    error is a function (np.array, np.array) -&gt; (float) (function)
    &#39;&#39;&#39;
    v = np.zeros(lmbaf-lmba0)
    domain = np.arange(lmba0, lmbaf)
    for i in range(lmbaf-lmba0):
        lmba = lmba0 + i
        m = method(t0, tf, n, x0, Af(lmba), g)
        exact = vectorize_sol_given_lmba(lmba, t0, tf, n, sol_given_lmba)
        if np.max(np.abs(m))&gt;1000:
            v[lmba-lmba0]=np.nan
        else:
            v[lmba-lmba0] = error(m, exact)
    return v, domain

def errors_array(n0, nf, method, t0, tf, x0, A, g, sol, vectorize_sol, error):
  &#39;&#39;&#39;
  This function will RETURN 2 arrays.
  The first one has the errors of the approximations given by the method with
  number of steps n = n0, n0+1, n0+2, ..., nf-1.
  The second is [n0, n0+1, n0+2, ..., nf-1]

  RECEIVES:
  n0 is the first number of steps. (int)
  nf is the last one plus 1. (int)
  method have arguments (t0, tf, n, x0, A, lmba, g) and return a
  np.vector of length n (0, 1, 2, ..., n-1), n is the number of steps. (function)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  x0 is the initial value of the Cauchy problem. (float)
  A is the coefficient os the linear part of the ploblem. (float)
  g is a function (int, float, float) -&gt; (float). (function)
  sol is a function (int, float) -&gt; (float). (function)
  vectorize_sol is a function that &quot;transforms sol in a vector&quot; (function)
  (float, float, int, function) -&gt; (np.array)
  (t0, tf, n, sol) -&gt; np.array([sol[t0], sol[t0+h], sol[t0+2h], ..., sol[tf-1]])
  error is a function (np.array, np.array) -&gt; (float) (function)
  &#39;&#39;&#39;
  v = np.zeros(nf-n0)
  domain = np.arange(n0, nf)
  for n in range(n0, nf):
    m = method(t0, tf, n, x0, A, g)
    exact = vectorize_sol(t0, tf, n, sol)
    if np.max(np.abs(m))&gt;1000:
        v[n-n0]=np.nan
    else:
        v[n-n0] = error(m, exact)
  return v, domain

def graphic_2D(domain, matrix, names, labelx, labely, title, key1, key2):
  &#39;&#39;&#39;
  domain is a list of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines. (list)
  matrix is a list of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines - same length that domain. (list)
  names is a list of the labels for the graphs, must have the same length that
  the number of lines in matrix. (list of Strings)
  labelx is the name of the x coordinate. (String)
  labely is the name of the y coordinate. (String)
  title is the title of the graph. (String)
  key1 is a boolean that indicates if the last graph must be black. (bool)
  key2 is a boolean that indicates if it should use the log scale. (bool)
  &#39;&#39;&#39;
  fig, ax = plt.subplots()

  colors = [&#39;red&#39;, &#39;orange&#39;, &#39;brown&#39;, &#39;green&#39;, &#39;cyan&#39;, &#39;blue&#39;, &#39;pink&#39;, &#39;yellow&#39;, &#39;gold&#39;, &#39;maroon&#39;]
  for i in range(len(names)-1):
    ax.plot(domain[i], matrix[i], color=colors[i], label=names[i])
  if key1:
    ax.plot(domain[len(names)-1], matrix[len(names)-1], color=&#39;black&#39;, label=names[len(names)-1])
  else:
    ax.plot(domain[len(names)-1], matrix[len(names)-1], color=colors[len(names)-1], label=names[len(names)-1])
  if key2:
    plt.yscale(&#39;log&#39;)
  ax.legend()
  ax.set_xlabel(labelx)
  ax.set_ylabel(labely)
  ax.set_title(title)
  return fig, ax

def graphic_3D(domain, matrix1, matrix2, names, labelx, labely, labelz, title, key1, key2):
  &#39;&#39;&#39;
  domain is a list of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines. (list)
  matrix1 and matrix2 are lists of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines - same length that domain. (list)
  names is a list of the labels for the graphs, must have the same length that
  the number of lines in matrix. (list of Strings)
  labelx is the name of the x coordinate. (String)
  labely is the name of the y coordinate. (String)
  labelz is the name of the z coordinate. (String)
  title is the title of the graph. (String)
  key1 is a boolean that indicates if the last graph must be black. (bool)
  key2 is a boolean that indicates if it should use the log scale. (bool)
  &#39;&#39;&#39;
  fig = plt.figure()
  ax = plt.figure().add_subplot(projection=&#39;3d&#39;)

  colors = [&#39;blue&#39;, &#39;green&#39;, &#39;red&#39;, &#39;cyan&#39;, &#39;magenta&#39;, &#39;yellow&#39;]
  for i in range(len(names)-1):
    ax.plot(domain[i], matrix1[i], matrix2[i], color=colors[i], label=names[i])
  if key1:
    ax.plot(domain[len(names)-1], matrix1[len(names)-1], matrix2[len(names)-1], color=&#39;black&#39;, label=names[len(names)-1])
  else:
    ax.plot(domain[len(names)-1], matrix1[len(names)-1], matrix2[len(names)-1], color=colors[len(names)-1], label=names[len(names)-1])
  if key2:
    plt.yscale(&#39;log&#39;)
  ax.legend()
  ax.set_xlabel(labelx)
  ax.set_ylabel(labely)
  ax.set_zlabel(labelz)
  ax.set_title(title)
  return fig, ax

def errors_2x(n0, k, method, t0, tf, x0, A, g, sol, vectorize_sol, error):
  &#39;&#39;&#39;
  This function will RETURN a np.array with the errors of the approximations given
  by the method with number of steps n = n0, 2*n0, 2**2*n0, ..., 2**(k-1)*n0.

  RECEIVES:
  n0 is the first number of steps. (int)
  k is the number of errors in the final array. (int)
  method have arguments (t0, tf, n, x0, lmba, g) and return a
  np.vector of length n (0, 1, 2, ..., n-1), n is the number of steps. (function)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  x0 is the initial value of the Cauchy problem. (float)
  A is the coefficient of the linear part of the ploblem. (np.matrix)
  g is a function (float, float) -&gt; (float). (function)
  sol is a function (float) -&gt; (float). (function)
  vectorize_sol is a function that &quot;transforms sol in a vector&quot; (function)
  (float, float, int, function) -&gt; (np.array)
  (t0, tf, n, sol) -&gt; np.array([sol[t0], sol[t0+h], sol[t0+2h], ..., sol[tf-1]])
  error is a function (np.array, np.array) -&gt; (float) (function)
  &#39;&#39;&#39;
  v = np.zeros(k)
  domain = np.zeros(k)
  for i in range(k):
    domain[i] = n0*2**i
    m = method(t0, tf, n0*2**i, x0, A, g)
    exact = vectorize_sol(t0, tf, n0*2**i, sol)
    v[i] = error(m, exact)
  return v, domain

def convergence_table(errors_2x, n0, k, t0, tf):
  &#39;&#39;&#39;
  RECEIVES:
  errors_2x is a array with the errors of the approximations given
  by a method with number of steps n = n0, 2*n0, 2**2*n0, ..., 2**(k-1)*n0. (np.array)
  n0 is the first number of steps. (int)
  k is the number of errors in the final array. (int)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  &#39;&#39;&#39;
  n = n0
  print(n, (tf-t0)/n, errors_2x[0], &quot;-&quot;, sep=&quot; &amp; &quot;, end=&quot; \\\\ \n&quot;)
  for i in range(1, k):
      n = n0 * 2 ** i
      h = (tf-t0)/n
      q = errors_2x[i-1]/errors_2x[i] #q=erro(h)/erro(h)
      r = ((tf-t0)/(n/2))/((tf-t0)/n)
      print(n, h, errors_2x[i], log(q,2)/log(r,2), sep=&quot; &amp; &quot;, end=&quot; \\\\ \n&quot;)

def convergence_table(errors_2x, n0, k, t0, tf):
  &#39;&#39;&#39;
  RECEIVES:
  errors_2x is a array with the errors of the approximations given
  by a method with number of steps n = n0, 2*n0, 2**2*n0, ..., 2**(k-1)*n0. (np.array)
  n0 is the first number of steps. (int)
  k is the number of errors in the final array. (int)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  &#39;&#39;&#39;
  n = n0
  print(&quot;| n | h = $\\frac{1}{h}$ | $\\tau(0,h)$ | q = $\\frac{tau(0,h)}{tau(0, 2h)}$ |&quot;)
  print(&quot;|---|-----------------|-----------|---------------------------------|&quot;)
  print(&quot;&quot;, n, (tf-t0)/n, errors_2x[0], &quot;-&quot;, sep=&quot; | &quot;, end=&quot; | \n&quot;)
  for i in range(1, k):
      n = n0 * 2 ** i
      h = (tf-t0)/n
      q = errors_2x[i-1]/errors_2x[i] #q=erro(h)/erro(h)
      r = ((tf-t0)/(n/2))/((tf-t0)/n)
      print( &quot;&quot;, n, h, errors_2x[i], log(q,2)/log(r,2), sep=&quot; | &quot;, end=&quot; | \n&quot;)

def lmba_n_error(errors_for_lambdas_array, method, x0, Af, g, sol_given_lmba, vectorize_sol_given_lmba, error, method_name):
  lmba0 = 5
  lmbaf = 100
  n0 = 10
  nf = 128
  t0 = 0.0
  tf = 1.0
  # Create data for X, Y
  lmba_values = np.arange(lmba0, lmbaf)
  n_values = np.arange(n0, nf)
  X, Y = np.meshgrid(lmba_values, 1/n_values)
  # Create a matrix of zeros for Z
  Z = np.zeros_like(X)
  # Populate the Z matrix with data using a function
  for n in range(n0, nf):
    Z[n-n0], domain = errors_for_lambdas_array(n, method, t0, tf, x0, lmba0, lmbaf, Af, g, sol_given_lmba, vectorize_sol_given_lmba, error)
  # Create filled contour plot
  plt.contourf(X, Y, Z)
  # Add color bar for the contour plot
  plt.colorbar()
  # Add labels and title (optional)
  plt.xlabel(&#39;lambda&#39;)
  plt.ylabel(&#39;h&#39;)
  plt.title(&#39;errors for the &#39;+method_name+&#39; method &#39;)
  # Show the plot
  plt.show()
</pre></div>
</div>
</div>
</div>
</section>
<section id="convergence-tables">
<h3>Convergence tables<a class="headerlink" href="#convergence-tables" title="Permalink to this headline">#</a></h3>
<section id="classic-euler">
<h4>Classic Euler<a class="headerlink" href="#classic-euler" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])
errors_2x_vector, domain = errors_2x(n0, k, classic_euler, t0, tf, x0, A, g, sol, vectorize_sol, error_sup)
convergence_table(errors_2x_vector, n0, k, t0, tf)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| n | h = $\frac{1}{h}$ | $\tau(0,h)$ | q = $\frac{tau(0,h)}{tau(0, 2h)}$ |
|---|-----------------|-----------|---------------------------------|
 | 128 | 0.0078125 | 0.2391072699739873 | - | 
 | 256 | 0.00390625 | 0.08650412059872986 | 1.466817233501749 | 
 | 512 | 0.001953125 | 0.039214210532948934 | 1.1413923006132296 | 
 | 1024 | 0.0009765625 | 0.018739566082401515 | 1.0652890085799935 | 
</pre></div>
</div>
</div>
</div>
</section>
<section id="exponential-euler">
<h4>Exponential Euler<a class="headerlink" href="#exponential-euler" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])
errors_2x_vector, domain = errors_2x(n0, k, exponential_euler, t0, tf, x0, A, g, sol, vectorize_sol, error_sup)
convergence_table(errors_2x_vector, n0, k, t0, tf)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| n | h = $\frac{1}{h}$ | $\tau(0,h)$ | q = $\frac{tau(0,h)}{tau(0, 2h)}$ |
|---|-----------------|-----------|---------------------------------|
 | 128 | 0.0078125 | 4.398075514689716e-05 | - | 
 | 256 | 0.00390625 | 2.074422525626487e-05 | 1.0841625981445133 | 
 | 512 | 0.001953125 | 1.0056221183126109e-05 | 1.0446214904461004 | 
 | 1024 | 0.0009765625 | 4.948885884282876e-06 | 1.0229126060177947 | 
</pre></div>
</div>
</div>
</div>
</section>
<section id="rk2">
<h4>rk2<a class="headerlink" href="#rk2" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors_2x_vector, domain = errors_2x(n0, k, rk2, t0, tf, x0, A, g, sol, vectorize_sol, error_sup)
convergence_table(errors_2x_vector, n0, k, t0, tf)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| n | h = $\frac{1}{h}$ | $\tau(0,h)$ | q = $\frac{tau(0,h)}{tau(0, 2h)}$ |
|---|-----------------|-----------|---------------------------------|
 | 128 | 0.0078125 | 0.06606851127601271 | - | 
 | 256 | 0.00390625 | 0.01256096444797522 | 2.395015596211044 | 
 | 512 | 0.001953125 | 0.0027104154026279526 | 2.212361357172686 | 
 | 1024 | 0.0009765625 | 0.0006264383048139033 | 2.1132696413977325 | 
</pre></div>
</div>
</div>
</div>
</section>
<section id="etd2rk-trapezoidal">
<h4>etd2rk (trapezoidal)<a class="headerlink" href="#etd2rk-trapezoidal" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors_2x_vector, domain = errors_2x(n0, k, etd2rk, t0, tf, x0, A, g, sol, vectorize_sol, error_sup)
convergence_table(errors_2x_vector, n0, k, t0, tf)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| n | h = $\frac{1}{h}$ | $\tau(0,h)$ | q = $\frac{tau(0,h)}{tau(0, 2h)}$ |
|---|-----------------|-----------|---------------------------------|
 | 128 | 0.0078125 | 4.186569175362864e-08 | - | 
 | 256 | 0.00390625 | 1.0575183428604418e-08 | 1.985085775819591 | 
 | 512 | 0.001953125 | 2.652380943352073e-09 | 1.9953227875115886 | 
 | 1024 | 0.0009765625 | 6.638462730912398e-10 | 1.9983668943519293 | 
</pre></div>
</div>
</div>
</div>
</section>
<section id="naive-version-of-etd2rk-trapezoidal">
<h4>Naive version of etd2rk (trapezoidal)<a class="headerlink" href="#naive-version-of-etd2rk-trapezoidal" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors_2x_vector, domain = errors_2x(n0, k, etd2rk_trapezoidal_naive, t0, tf, x0, A, g, sol, vectorize_sol, error_sup)
convergence_table(errors_2x_vector, n0, k, t0, tf)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| n | h = $\frac{1}{h}$ | $\tau(0,h)$ | q = $\frac{tau(0,h)}{tau(0, 2h)}$ |
|---|-----------------|-----------|---------------------------------|
 | 128 | 0.0078125 | 0.0004242643044311458 | - | 
 | 256 | 0.00390625 | 0.00010714498082271644 | 1.9853990333325726 | 
 | 512 | 0.001953125 | 2.6871031228085582e-05 | 1.9954406751889993 | 
 | 1024 | 0.0009765625 | 6.725136514989377e-06 | 1.9984162299862431 | 
</pre></div>
</div>
</div>
</div>
</section>
<section id="rk4">
<h4>rk4<a class="headerlink" href="#rk4" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors_2x_vector, domain = errors_2x(n0, k, rk4, t0, tf, x0, A, g, sol, vectorize_sol, error_sup)
convergence_table(errors_2x_vector, n0, k, t0, tf)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| n | h = $\frac{1}{h}$ | $\tau(0,h)$ | q = $\frac{tau(0,h)}{tau(0, 2h)}$ |
|---|-----------------|-----------|---------------------------------|
 | 128 | 0.0078125 | 0.002141816843239275 | - | 
 | 256 | 0.00390625 | 9.770249694801558e-05 | 4.4542958704375835 | 
 | 512 | 0.001953125 | 5.250705130854794e-06 | 4.21781234893684 | 
 | 1024 | 0.0009765625 | 3.024340525237257e-07 | 4.1178186851779905 | 
</pre></div>
</div>
</div>
</div>
</section>
<section id="deduced-like-etd3rk">
<h4>Deduced like etd3rk<a class="headerlink" href="#deduced-like-etd3rk" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors_2x_vector, domain = errors_2x(n0, k, etd3rk_similar, t0, tf, x0, A, g, sol, vectorize_sol, error_sup)
convergence_table(errors_2x_vector, n0, k, t0, tf)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| n | h = $\frac{1}{h}$ | $\tau(0,h)$ | q = $\frac{tau(0,h)}{tau(0, 2h)}$ |
|---|-----------------|-----------|---------------------------------|
 | 128 | 0.0078125 | 5.0853024048669315e-12 | - | 
 | 256 | 0.00390625 | 3.212833644961055e-13 | 3.9844153810116354 | 
 | 512 | 0.001953125 | 2.0132983821752326e-14 | 3.996213373698299 | 
 | 1024 | 0.0009765625 | 1.2602766052971504e-15 | 3.997748687591092 | 
</pre></div>
</div>
</div>
</div>
</section>
<section id="naive-version-of-etd3rk">
<h4>Naive version of etd3rk<a class="headerlink" href="#naive-version-of-etd3rk" title="Permalink to this headline">#</a></h4>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors_2x_vector, domain = errors_2x(n0, k, etd3rk_naive, t0, tf, x0, A, g, sol, vectorize_sol, error_sup)
convergence_table(errors_2x_vector, n0, k, t0, tf)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stream highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>| n | h = $\frac{1}{h}$ | $\tau(0,h)$ | q = $\frac{tau(0,h)}{tau(0, 2h)}$ |
|---|-----------------|-----------|---------------------------------|
 | 128 | 0.0078125 | 1.083876968009309e-06 | - | 
 | 256 | 0.00390625 | 6.883813637344194e-08 | 3.9768491535433466 | 
 | 512 | 0.001953125 | 4.322307012305515e-09 | 3.9933345852265947 | 
 | 1024 | 0.0009765625 | 2.705360744453822e-10 | 3.9979086629155343 | 
</pre></div>
</div>
</div>
</div>
</section>
</section>
<section id="some-graphics">
<h3>Some graphics<a class="headerlink" href="#some-graphics" title="Permalink to this headline">#</a></h3>
<p>The following notation is used</p>
<p>\begin{cases}
u’(t) + A u(t) = g(u(t), t)\
u(0) = u_0.
\end{cases}</p>
<p>A Stiff problem shown in [1] is</p>
<p>\begin{cases}
u’(t) + 100 u(t) = \sin(t)\
u(0) = u_0,
\end{cases}</p>
<p>with solution</p>
<div class="math notranslate nohighlight">
\[
u(t) = u_0 \exp(-100t)+\frac{\exp(-100t)+100\sin(t)-\cos(t)}{1+100^2}.
\]</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n = 128
lmba0 = 1
lmbaf = 100
t0 = 0.0
tf = 1.0
x0 = np.array([1])
lmba_1D_classic, domain = errors_for_lambdas_array(n, classic_euler, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_exponential, domain = errors_for_lambdas_array(n, exponential_euler, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_etd2rk, domain = errors_for_lambdas_array(n, etd2rk, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_etd2rk_trapezoidal_naive, domain = errors_for_lambdas_array(n, etd2rk_trapezoidal_naive, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_etd3rk_similar, domain = errors_for_lambdas_array(n, etd3rk_similar, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_etd3rk_naive, domain = errors_for_lambdas_array(n, etd3rk_naive, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_rk2, domain = errors_for_lambdas_array(n, rk2, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
lmba_1D_rk4, domain = errors_for_lambdas_array(n, rk4, t0, tf, x0, lmba0, lmbaf, A_1D, g, sol_given_lmba, vectorize_sol_given_lmba, error_2)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_11908/3543048019.py:247: ComplexWarning: Casting complex values to real discards the imaginary part
  return np.sqrt(float(np.sum(v)/x_approx.size)) #normalized
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>matrix_1D = [lmba_1D_classic, lmba_1D_exponential, lmba_1D_rk2, lmba_1D_etd2rk_trapezoidal_naive, lmba_1D_etd2rk, lmba_1D_rk4, lmba_1D_etd3rk_naive, lmba_1D_etd3rk_similar]
names = [&#39;classic euler&#39;, &#39;exponential euler&#39;, &#39;rk2&#39;, &#39;etd2rk naive&#39;, &#39;etd2rk&#39;, &#39;rk4&#39;, &#39;etd3rk naive&#39;, &quot;etd3rk (similar)&quot;]
fig, ax = graphic_2D(8*[domain], matrix_1D, names, &quot;lambda&quot;, &quot;error&quot;, &quot;1D problem from [1]&quot;, False, True)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/appendix_24_0.png" src="_images/appendix_24_0.png" />
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>n0 = 10
k = 10
lmba = 100
A = lmba * np.array([[1]])
t0 = 0.0
tf = 1.0
x0 = np.array([1])
n_1D_classic, domain = errors_2x(n0, k, classic_euler, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_exponential, domain = errors_2x(n0, k, exponential_euler, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_etd2rk, domain = errors_2x(n0, k, etd2rk, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_etd2rk_trapezoidal_naive, domain = errors_2x(n0, k, etd2rk_trapezoidal_naive, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_etd3rk_similar, domain = errors_2x(n0, k, etd3rk_similar, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_etd3rk_naive, domain = errors_2x(n0, k, etd3rk_naive, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_rk2, domain = errors_2x(n0, k, rk2, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
n_1D_rk4, domain = errors_2x(n0, k, rk4, t0, tf, x0, A, g, sol, vectorize_sol, error_2)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<div class="output stderr highlight-myst-ansi notranslate"><div class="highlight"><pre><span></span>/tmp/ipykernel_11908/3543048019.py:247: ComplexWarning: Casting complex values to real discards the imaginary part
  return np.sqrt(float(np.sum(v)/x_approx.size)) #normalized
</pre></div>
</div>
</div>
</div>
<div class="cell docutils container">
<div class="cell_input docutils container">
<div class="highlight-ipython3 notranslate"><div class="highlight"><pre><span></span>matrix_2D = [n_1D_classic, n_1D_exponential, n_1D_rk2, n_1D_etd2rk_trapezoidal_naive, n_1D_etd2rk, n_1D_rk4, n_1D_etd3rk_naive, n_1D_etd3rk_similar]
names = [&#39;classic euler&#39;, &#39;exponential euler&#39;, &#39;rk2&#39;, &#39;etd2rk naive&#39;, &#39;etd2rk&#39;, &#39;rk4&#39;, &#39;etd3rk naive&#39;, &quot;etd3rk (similar)&quot;]
fig_2D, ax_2D = graphic_2D(8*[1/domain], matrix_2D, names, &quot;h&quot;, &quot;error&quot;, &quot;1D problem with lmba = &quot;+str(lmba), False, True)
plt.xscale(&#39;log&#39;)
</pre></div>
</div>
</div>
<div class="cell_output docutils container">
<img alt="_images/appendix_26_0.png" src="_images/appendix_26_0.png" />
</div>
</div>
</section>
<section id="some-deductions">
<h3>Some deductions<a class="headerlink" href="#some-deductions" title="Permalink to this headline">#</a></h3>
<p>Here is used informations from [1], [6], [7].</p>
<section id="exponential-euler-method">
<h4>Exponential Euler method<a class="headerlink" href="#exponential-euler-method" title="Permalink to this headline">#</a></h4>
<p>For</p>
<p><span class="math notranslate nohighlight">\(\begin{cases}
    y'(t) + \lambda y(t) = g(y(t), t), t \in (t_0, T) \\
    y(0) = y_0
\end{cases}\)</span></p>
<p>the domain is evenly discretized:</p>
<div class="math notranslate nohighlight">
\[
    N \in \mathbb{N}; h = \frac{T-t_0}{N}; \text{Domain: }\{t_k = t_0 + k h : k = 0, 1, ...\}.
\]</div>
<p>The discretization of the ODE takes the exact solution of the Cauchy problem, given by the variation of constants formula</p>
<div class="math notranslate nohighlight">
\[
    y(t) = e^{-(t-t_0) \lambda}y_0 + \int_{t_0}^t [e^{-\lambda(t-\tau)} g(y(\tau), \tau)] d\tau
\]</div>
<p>and, by Taylor expansion on <span class="math notranslate nohighlight">\(g\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\tau \in (t_k, t_{k+1})\)</span></p>
<div class="math notranslate nohighlight">
\[
    g(y(\tau), \tau) = g(y(t_k), t_k) + (\tau - t_k) \frac{dg}{dt} (y(\theta_k), \theta_k)
\]</div>
<p>for a <span class="math notranslate nohighlight">\(\theta_k \in (t_k, t_{k+1}),\)</span></p>
<div class="math notranslate nohighlight">
\[
    y(t_{k+1}) = e^{-(t_{k+1}-t_k) \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} [e^{-\lambda(t_{k+1}-\tau)} g(y(\tau), \tau)] d\tau
\]</div>
<div class="math notranslate nohighlight">
\[
    = e^{-h \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} \left[e^{-\lambda(t_{k+1}-\tau)} \left( g(y(t_k), t_k) + (\tau - t_k) \frac{dg}{dt} (y(\theta_k), \theta_k)\right)\right] d\tau
\]</div>
<div class="math notranslate nohighlight">
\[
    = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d\tau + \frac{dg}{dt} (y(\theta_k), \theta_k) \int_{t_k}^{t_{k+1}} (\tau - t_k) e^{-\lambda(t_{k+1}-\tau)} d\tau.
\]</div>
<p>Since</p>
<div class="math notranslate nohighlight">
\[
    \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d\tau = h\phi_1(-\lambda h)= \frac{1-e^{-h \lambda}}{\lambda}
\]</div>
<p>and, by the Taylor expansion of <span class="math notranslate nohighlight">\(e^{-\lambda h}\)</span> in the point zero</p>
<div class="math notranslate nohighlight">
\[
    e^{-\lambda h} = 1 - \lambda h + \frac{1}{2}\lambda^2h^2 - \frac{1}{3!}\lambda^3h^3 + \dotsi + \frac{1}{n!} (-\lambda h)^n + \dotsi, n \in \mathbb{N}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
     \int_{t_k}^{t_{k+1}} (\tau - t_k) e^{-\lambda(t_{k+1}-\tau)} d\tau =
     h^2 \phi_2 (-\lambda h) =
     h \frac{\phi_1(0) - \phi_1(-\lambda h)}{\lambda} =
     \frac{h}{\lambda} - \frac{1-e^{-h \lambda}}{\lambda^2} = \\
     \frac{h}{\lambda} - \frac{1-(1 - \lambda h + \frac{1}{2}\lambda^2h^2 - \frac{1}{3!}\lambda^3h^3 + \dotsi + \frac{1}{n!} (-\lambda h)^n + \dotsi)}{\lambda^2} = \\
     \frac{h^2}{2} - \frac{h^3}{3!} \lambda + \dotsi + \frac{h^n}{n!} (-\lambda)^{n-2} + \dotsi  =  O(h^2),
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
    y(t_{k+1}) = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h \lambda}}{\lambda} + \frac{dg}{dt} (y(\theta_k), \theta_k) O(h^2),
\]</div>
<div class="math notranslate nohighlight">
\[
  y(t_{k+1}) = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h \lambda}}{\lambda} + O(h^2).
\]</div>
<p>That inspires the <span class="math notranslate nohighlight">\(\textbf{Exponential Euler method}\)</span> :</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y_0 = y(t_0)\\
\textbf{for } k = 0, 1, 2, ..., N-1 :\\
    y_{k+1} = e^{-h \lambda}y_k + g(y_k, t_k) \frac{1-e^{-h \lambda}}{\lambda}\\
    t_{k+1} = t_k + h
\end{split}\]</div>
<p>with <span class="math notranslate nohighlight">\(y_k \thickapprox y(t_k)\)</span>.</p>
</section>
<section id="exponential-time-differencing-methods-etd">
<h4>Exponential time differencing methods (ETD)<a class="headerlink" href="#exponential-time-differencing-methods-etd" title="Permalink to this headline">#</a></h4>
<p>In the same conditions as above, it is taken a general Taylor expansion of <span class="math notranslate nohighlight">\(g\)</span>:</p>
<p><span class="math notranslate nohighlight">\(\tau \in (t_k, t_{k+1}), n \in \mathbb{N}\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    g(y(\tau), \tau) = g(y(t_k), t_k) + (\tau - t_k) \frac{dg}{dt} (y(t_k), t_k) + \frac{(\tau - t_k)^2}{2!} \frac{d^2g}{dt^2} (y(t_k), t_k) + \\
    \dotsi + \frac{(\tau - t_k)^{n-1}}{(n-1)!} \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k) + \frac{(\tau - t_k)^n}{n!} \frac{d^ng}{dt^n} (y(\theta_k), \theta_k)
\end{split}\]</div>
<p>for a <span class="math notranslate nohighlight">\(\theta_k \in (t_k, t_{k+1})\)</span></p>
<p>In</p>
<div class="math notranslate nohighlight">
\[
    y(t_{k+1}) = e^{-h \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} g(y(\tau), \tau) d\tau
\]</div>
<p>It will now become</p>
<div class="math notranslate nohighlight">
\[
y(t_{k+1}) = e^{-h \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)}  g(y(t_k), t_k) +
(\tau - t_k) \frac{dg}{dt} (y(t_k), t_k) +
\]</div>
<div class="math notranslate nohighlight">
\[
 \frac{(\tau - t_k)^2}{2!} \frac{d^2g}{dt^2} (y(t_k), t_k) + \dotsi + 
\]</div>
<div class="math notranslate nohighlight">
\[
 + \frac{(\tau - t_k)^{n-1}}{(n-1)!} \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k) + \frac{(\tau - t_k)^n}{n!} \frac{d^ng}{dt^n} (y(\theta_k), \theta_k)  d\tau,
\]</div>
<div class="math notranslate nohighlight">
\[
y(t_{k+1}) = e^{-h \lambda}y(t_k) +
g(y(t_k), t_k)\int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d \tau +
\]</div>
<div class="math notranslate nohighlight">
\[
+ \frac{dg}{dt}(y(t_k), t_k)\int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} (\tau - t_k)d\tau + \frac{d^2g}{dt^2} (y(t_k), t_k)\int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} \frac{(\tau - t_k)^2}{2!}d\tau +
\]</div>
<div class="math notranslate nohighlight">
\[
+ \dotsi +
\]</div>
<div class="math notranslate nohighlight">
\[
+ \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k)\int_{t_k}^{t_{k+1}}  e^{-\lambda(t_{k+1}-\tau)} \frac{(\tau - t_k)^{n-1}}{(n-1)!} d\tau + \frac{d^ng}{dt^n} (y(\theta_k), \theta_k) \int_{t_k}^{t_{k+1}}  e^{-\lambda(t_{k+1}-\tau)} \frac{(\tau - t_k)^n}{n!} d\tau,
\]</div>
<div class="math notranslate nohighlight">
\[
y(t_{k+1}) = e^{-h \lambda}y(t_k) +
h\phi_1(-\lambda h) g(y(t_k), t_k) +
h^2\phi_2(-\lambda h) \frac{dg}{dt}(y(t_k), t_k) +
h^3\phi_3(-\lambda h)\frac{d^2g}{dt^2} (y(t_k), t_k)
\]</div>
<div class="math notranslate nohighlight">
\[
+ \dotsi +
\]</div>
<div class="math notranslate nohighlight">
\[
+ h^n\phi_n(-\lambda h) \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k)+
h^{n+1}\phi_{n+1}(-\lambda h) \frac{d^ng}{dt^n} (y(\theta_k), \theta_k).
\]</div>
<p>From the discussion about the exponential Euler, that is known that</p>
<div class="math notranslate nohighlight">
\[
h^2\phi_2(-\lambda h) = \frac{h^2}{2} - \frac{h^3}{3!} \lambda + \dotsi + \frac{h^l}{l!} (-\lambda)^{l-2} + \dotsi = \frac{1}{(-\lambda)^2} \sum\limits_{i=2}^{\infty} \frac{(-\lambda h)^i}{i!}.
\]</div>
<p>Since</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  \phi_{n+1}(-\lambda h) = \frac{\phi_n(-\lambda h) - \phi_n(0)}{-\lambda h} \text{ and}\\
  \phi_n(0) = \frac{1}{n!},
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
  h^3 \phi_3(-\lambda h) = h^2 \frac{\phi_2(0) - \phi_2(-\lambda h)}{\lambda} = \frac{\frac{h^2}{2} - (\frac{h^2}{2} - \frac{h^3}{3!} \lambda + \dotsi + \frac{h^l}{l!} (-\lambda)^{l-2} + O(h^{l+1}))}{\lambda} = \frac{1}{(-\lambda)^3} \sum\limits_{i=3}^{\infty} \frac{(-\lambda h)^i}{i!}.
\]</div>
<p>And if</p>
<div class="math notranslate nohighlight">
\[
h^l \phi_l(-\lambda h) = \frac{1}{(-\lambda)^l} \sum\limits_{i=l}^{\infty} \frac{(-\lambda h)^i}{i!}, \text{for a } l \in \mathbb{N},
\]</div>
<div class="math notranslate nohighlight">
\[
  h^{l+1}\phi_{l+1}(-\lambda h) = h^{l+1} \frac{\phi_l(-\lambda h) - \phi_l(0)}{-\lambda h} = \frac{h^l \phi_l(0) - h^l \phi_l(-\lambda h)}{\lambda} = \frac{h^l}{l! \lambda} - \frac{1}{\lambda} \frac{1}{(-\lambda)^l} \sum\limits_{i=l}^{\infty} \frac{(-\lambda h)^i}{i!} = \frac{1}{(-\lambda)^{l+1}} \sum\limits_{i=l+1}^{\infty} \frac{(-\lambda h)^i}{i!}.
\]</div>
<p>So, by induction,</p>
<div class="math notranslate nohighlight">
\[
h^n \phi_n(-\lambda h) = \frac{1}{(-\lambda)^n} \sum\limits_{i=n}^{\infty} \frac{(-\lambda h)^i}{i!} = O(h^n), \forall n \geq 2.
\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h \lambda}y(t_k) +
h\phi_1(-\lambda h) g(y(t_k), t_k) +
h^2\phi_2(-\lambda h) \frac{dg}{dt}(y(t_k), t_k) +
h^3\phi_3(-\lambda h)\frac{d^2g}{dt^2} (y(t_k), t_k) +
\dotsi + \\
h^n\phi_n(-\lambda h) \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k)+
O(h^{n+1}).
\end{split}\]</div>
</section>
<section id="exponential-time-differencing-methods-with-runge-kutta-time-stepping-order-2-cox-and-matthews-trapezoidal-rule">
<h4>Exponential time differencing methods with Runge-Kutta time stepping - order 2 - Cox and Matthews - Trapezoidal rule<a class="headerlink" href="#exponential-time-differencing-methods-with-runge-kutta-time-stepping-order-2-cox-and-matthews-trapezoidal-rule" title="Permalink to this headline">#</a></h4>
<p>For the second order method, that is used the approximation</p>
<div class="math notranslate nohighlight">
\[
    g(y(\tau), \tau) = g(y(t_k), t_k) + (\tau - t_k) \frac{dg}{dt} (y(t_k), t_k) + O(h^2),
\]</div>
<p><span class="math notranslate nohighlight">\(\forall \tau \in (t_k, t_{k+1}).\)</span></p>
<p>The first derivative is discretized with the Taylor expansion</p>
<div class="math notranslate nohighlight">
\[
g(y(t_{k+1}), t_{k+1}) = g(y(t_k), t_k) + h \frac{dg}{dt} (y(t_k), t_k) + O(h^2)
\]</div>
<p>and the exponential Euler expression</p>
<div class="math notranslate nohighlight">
\[
  y(t_{k+1}) = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h \lambda}}{\lambda} + O(h^2),
\]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{dg}{dt} (y(t_k), t_k)  = \frac{g(a_k, t_{k+1}) - g(y(t_k), t_k)}{h} + O(h), \\
\text{with } a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h\lambda}}{\lambda},
\end{split}\]</div>
<p>which results in the expression</p>
<div class="math notranslate nohighlight">
\[\begin{split}
g(y(\tau), \tau) = g(y(t_k), t_k) + (\tau - t_k) \frac{g(a_k, t_{k+1}) - g(y(t_k), t_k)}{h} + (\tau - t_k)O(h) \\
\text{with } a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h\lambda}}{\lambda}.
\end{split}\]</div>
<p>Putting in the variation of constants formula</p>
<div class="math notranslate nohighlight">
\[
y(t) = e^{-(t-t_0) \lambda}y_0 + \int_{t_0}^t e^{-\lambda(t-\tau)} g(y(\tau), \tau) d\tau,
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda}y(t_k) + \\
  + \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} \left[ g(y(t_k), t_k) + (\tau - t_k)  \frac{g(a_k, t_{k+1}) - g(y(t_k), t_k)}{h}  + (\tau - t_k)O(h) \right] d\tau
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h \lambda} y(t_k) + g(y(t_k), t_k)\int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d \tau + \frac{g(a_k, t_{k+1}) - g(y(t_k), t_k)}{h} \int_{t_k}^{t_{k+1}} (\tau - t_k) e^{-\lambda(t_{k+1}-\tau)} d \tau + \\
+ O(h)\int_{t_k}^{t_{k+1}} (\tau - t_k) e^{-\lambda(t_{k+1}-\tau)} d \tau \\
\text{with } a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h\lambda}}{\lambda}.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  h \phi_1 (-\lambda h) g(y(t_k), t_k) +
  \frac{g(a_k, t_{k+1}) - g(y(t_k), t_k)}{h} h^2 \phi_2 (-\lambda h) + \\
  + O(h)h^2 \phi_2 (-\lambda h) \\
  y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  h \phi_1 (-\lambda h) g(y(t_k), t_k) +
  \left[g(a_k, t_{k+1}) - g(y(t_k), t_k) \right] h \phi_2 (-\lambda h) + \\
  + O(h^3) \\
  \text{with } a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h\lambda}}{\lambda}.
\end{split}\]</div>
<p>Butcher tableau:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}
{c|cc}
0\\
1 &amp; \phi_1(-\lambda h)\\
\hline
&amp; \phi_1 (-\lambda h) - \phi_2 (-\lambda h) &amp; \phi_2 (-\lambda h)
\end{array}
\end{split}\]</div>
</section>
<section id="exponential-time-differencing-methods-with-runge-kutta-time-stepping-order-2-cox-and-matthews-midpoint-rule">
<h4>Exponential time differencing methods with Runge-Kutta time stepping - order 2 - Cox and Matthews - Midpoint rule<a class="headerlink" href="#exponential-time-differencing-methods-with-runge-kutta-time-stepping-order-2-cox-and-matthews-midpoint-rule" title="Permalink to this headline">#</a></h4>
<p>From the same expression:</p>
<div class="math notranslate nohighlight">
\[
    g(y(\tau), \tau) = g(y(t_k), t_k) + (\tau - t_k) \frac{dg}{dt} (y(t_k), t_k) + O(h^2),
\]</div>
<p><span class="math notranslate nohighlight">\(\forall \tau \in (t_k, t_{k+1}).\)</span></p>
<p>The first derivative is now discretized with the Taylor expansion</p>
<div class="math notranslate nohighlight">
\[
g\left(y\left(t_k + \frac{h}{2}\right), t_k + \frac{h}{2} \right) = g(y(t_k), t_k) + \frac{h}{2} \frac{dg}{dt} (y(t_k), t_k) + O(h^2)
\]</div>
<p>and the exponential Euler expression taken is with time step <span class="math notranslate nohighlight">\(\frac{h}{2}\)</span></p>
<div class="math notranslate nohighlight">
\[
  y\left(t_k + \frac{h}{2}\right) = e^{-\frac{h \lambda}{2}}y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1\left( -\frac{\lambda h}{2} \right) + O(h^2),
\]</div>
<p>so that</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\frac{dg}{dt} (y(t_k), t_k)  = 2 \frac{g\left(b_k, t_k + \frac{h}{2} \right) - g(y(t_k), t_k)}{h} + O(h), \\
\text{with } b_k =e^{-\frac{h \lambda}{2}}y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1\left( -\frac{\lambda h}{2} \right),
\end{split}\]</div>
<p>which results in the expression</p>
<div class="math notranslate nohighlight">
\[\begin{split}
g(y(\tau), \tau) = g(y(t_k), t_k) + 2(\tau - t_k) \frac{g\left(b_k, t_k + \frac{h}{2}\right) - g(y(t_k), t_k)}{h} + (\tau - t_k)O(h) \\
\text{with } b_k =e^{-\frac{h \lambda}{2}}y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1\left( -\frac{\lambda h}{2} \right).
\end{split}\]</div>
<p>Putting in the variation of constants formula</p>
<div class="math notranslate nohighlight">
\[
y(t) = e^{-(t-t_0) \lambda}y_0 + \int_{t_0}^t e^{-\lambda(t-\tau)} g(y(\tau), \tau) d\tau,
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda}y(t_k) + \\
    + \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} \left[ g(y(t_k), t_k) + 2(\tau - t_k) \frac{g\left(b_k, t_k + \frac{h}{2}\right) - g(y(t_k), t_k)}{h} + (\tau - t_k)O(h) \right] d\tau
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h \lambda} y(t_k) + g(y(t_k), t_k)\int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d \tau + \\
  + 2\frac{g\left(b_k, t_k + \frac{h}{2}\right) - g(y(t_k), t_k)}{h} \int_{t_k}^{t_{k+1}} (\tau - t_k) e^{-\lambda(t_{k+1}-\tau)} d \tau + O(h)\int_{t_k}^{t_{k+1}} (\tau - t_k) e^{-\lambda(t_{k+1}-\tau)} d \tau \\
  \text{with } b_k =e^{-\frac{h \lambda}{2}}y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1\left( -\frac{\lambda h}{2} \right).
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  h \phi_1 (-\lambda h) g(y(t_k), t_k) +
  2\frac{g\left(b_k, t_k + \frac{h}{2}\right) - g(y(t_k), t_k)}{h} h^2 \phi_2 (-\lambda h) + \\
  + O(h)h^2 \phi_2 (-\lambda h) \\
  y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  h \phi_1 (-\lambda h) g(y(t_k), t_k) +
  2 \left[g\left(b_k, t_k + \frac{h}{2}\right) - g(y(t_k), t_k) \right] h \phi_2 (-\lambda h) + \\
  + O(h^3) \\
  \text{with } b_k =e^{-\frac{h \lambda}{2}}y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1\left( -\frac{\lambda h}{2} \right).
\end{split}\]</div>
<p>Butcher tableau:</p>
<p>\begin{array}
{c|cc}
0\
\frac{1}{2} &amp; \frac{1}{2}\phi_1\left(-\frac{\lambda h}{2}\right)\
\hline
&amp; \phi_1 (-\lambda h) - 2 \phi_2 (-\lambda h) &amp; -2 \phi_2 (-\lambda h)
\end{array}</p>
</section>
<section id="exponential-time-differencing-methods-with-runge-kutta-time-stepping-order-2-classical-approach-trapezoidal-rule">
<h4>Exponential time differencing methods with Runge-Kutta time stepping - order 2 - Classical approach - Trapezoidal rule<a class="headerlink" href="#exponential-time-differencing-methods-with-runge-kutta-time-stepping-order-2-classical-approach-trapezoidal-rule" title="Permalink to this headline">#</a></h4>
<p>It is also possible to think the exponential time differencing methods with Runge-Kutta time stepping using the numerical integration, for example, for the one with second order, it starts with the trapezoidal rule (which was taken from [2]) on the variation of constants formula:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h \lambda}y(t_k) + \frac{h}{2} \left[ e^{-\lambda(t_{k+1}-t_k)} g(y(t_k), t_k) + e^{-\lambda(t_{k+1}-t_{k+1})} g(y(t_{k+1}), t_{k+1}) \right] + O(h^3), \\
    y(t_{k+1}) = e^{-h \lambda}y(t_k) + \frac{h}{2} \left[ e^{-\lambda h} g(y(t_k), t_k) + g(y(t_{k+1}), t_{k+1}) \right] +  O(h^3).
\end{split}\]</div>
<p>And then, from the expression seen before:</p>
<div class="math notranslate nohighlight">
\[
y(t_{k+1}) = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h \lambda}}{\lambda} + O(h^2),
\]</div>
<div class="math notranslate nohighlight">
\[
    g(y(t_{k+1}), t_{k+1}) = g(a_k, t_{k+1}) + O(h^2) \text{, with } a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h\lambda}}{\lambda}.
\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[
y(t_{k+1}) = e^{-h \lambda}y(t_k) + \frac{h}{2} \left[ e^{-\lambda h} g(y(t_k), t_k) + g(a_k, t_{k+1}) + O(h^2) \right] +  O(h^3),
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h \lambda}y(t_k) + \frac{h}{2} \left[ e^{-\lambda h} g(y(t_k), t_k) + g(a_k, t_{k+1}) \right] +  O(h^3) \\
    \text{with } a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) h \phi_1 (-\lambda h).
\end{split}\]</div>
<p>Butcher tableau:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}
{c|cc}
0\\
1 &amp; \phi_1(-\lambda h)\\
\hline
&amp; \frac{1}{2} e^{-h \lambda} &amp; \frac{1}{2}
\end{array}
\end{split}\]</div>
</section>
<section id="exponential-time-differencing-methods-with-runge-kutta-time-stepping-order-2-classical-approach-midpoint-rule">
<h4>Exponential time differencing methods with Runge-Kutta time stepping - order 2 - Classical approach - Midpoint rule<a class="headerlink" href="#exponential-time-differencing-methods-with-runge-kutta-time-stepping-order-2-classical-approach-midpoint-rule" title="Permalink to this headline">#</a></h4>
<p>Besides that, using the midpoint rule, also known as rectangle rule, again taken from [2],</p>
<div class="math notranslate nohighlight">
\[
y(t_{k+1}) = e^{-(t_{k+1}-t_k) \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} [e^{-\lambda(t_{k+1}-\tau)} g(y(\tau), \tau)] d\tau,
\]</div>
<div class="math notranslate nohighlight">
\[
y(t_{k+1}) = e^{-h\lambda}y(t_k) + h e^{-\lambda\left(t_{k+1}-\frac{t_{k+1}+t_k}{2}\right)} g\left(y\left(\frac{t_{k+1}+t_k}{2}\right), \frac{t_{k+1}+t_k}{2}\right) + O(h^3),
\]</div>
<div class="math notranslate nohighlight">
\[
y(t_{k+1}) = e^{-h\lambda}y(t_k) + h e^{- \frac{h\lambda}{2}} g\left(y\left(t_k + \frac{h}{2}\right), t_k+\frac{h}{2}\right) + O(h^3),
\]</div>
<p>and Exponential Euler with time step <span class="math notranslate nohighlight">\(\frac{h}{2}\)</span></p>
<div class="math notranslate nohighlight">
\[
y\left(t_k + \frac{h}{2}\right) = e^{-\frac{h \lambda}{2}}y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1\left( -\frac{\lambda h}{2} \right) + O(h^2),
\]</div>
<p>results in</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h\lambda}y(t_k) + h e^{- \frac{h\lambda}{2}} g\left(b_k + O(h^2), t_k + \frac{h}{2}\right) + O(h^3) \\
    \text{with } b_k =e^{-\frac{h \lambda}{2}}y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1\left( -\frac{\lambda h}{2} \right),
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
y(t_{k+1}) = e^{-h\lambda}y(t_k) + h e^{- \frac{h\lambda}{2}} g\left(b_k , t_k + \frac{h}{2}\right) + O(h^3) \\
    \text{with } b_k =e^{-\frac{h \lambda}{2}}y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1\left( -\frac{\lambda h}{2} \right),
\end{split}\]</div>
<p>Butcher tableau:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
\begin{array}
{c|cc}
0\\
\frac{1}{2} &amp;  \frac{1}{2} \phi_1( -\frac{\lambda h}{2})\\
\hline
&amp; 0 &amp; e^{-\frac{h \lambda}{2}}
\end{array}
\end{split}\]</div>
</section>
<section id="third-order-exponential-time-differencing-methods-with-runge-kutta-time-stepping-etdrk-3">
<h4>Third order exponential time differencing methods with Runge-Kutta time stepping (ETDRK-3)<a class="headerlink" href="#third-order-exponential-time-differencing-methods-with-runge-kutta-time-stepping-etdrk-3" title="Permalink to this headline">#</a></h4>
<div class="math notranslate nohighlight">
\[\begin{split}
    g(y(\tau), \tau) = g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) +
    \left(\tau - t_{k+\frac{1}{2}}\right) \frac{dg}{dt} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + \\
    + \frac{\left(\tau - t_{k+\frac{1}{2}}\right)^2}{2!} \frac{d^2g}{dt^2} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + \\
    + \frac{\left(\tau - t_{k+\frac{1}{2}}\right)^3}{3!} \frac{d^3g}{dt^3} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
    + O((\tau - t_k)^4),
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\forall \tau \in \mathbb{R}.\)</span></p>
<div class="math notranslate nohighlight">
\[\begin{split}
    g(y(t_{k+1}), t_{k+1}) = g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) +
    \left(t_{k+1} - t_{k+\frac{1}{2}}\right) \frac{dg}{dt} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + \\
    + \frac{\left(t_{k+1} - t_{k+\frac{1}{2}}\right)^2}{2!} \frac{d^2g}{dt^2} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + \\
    + \frac{\left(t_{k+1} - t_{k+\frac{1}{2}}\right)^3}{3!} \frac{d^3g}{dt^3} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
    + O(h^4),
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
    g(y(t_{k+1}), t_{k+1}) = g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) +
    \frac{h}{2} \frac{dg}{dt} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + \\
    +\frac{h^2}{8} \frac{d^2g}{dt^2} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
    + \frac{h^3}{48} \frac{d^3g}{dt^3} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
    + O(h^4),
\end{split}\]</div>
<p>and</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    g(y(t_k), t_k) = g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) +
    \left(t_k - t_{k+\frac{1}{2}}\right) \frac{dg}{dt} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + \\
    + \frac{\left(t_k - t_{k+\frac{1}{2}}\right)^2}{2!} \frac{d^2g}{dt^2} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + \\
    + \frac{\left(t_k - t_{k+\frac{1}{2}}\right)^3}{3!} \frac{d^3g}{dt^3} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
    + O(h^4),
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
    g(y(t_k), t_k) = g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) -
    \frac{h}{2} \frac{dg}{dt} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + \\
    + \frac{h^2}{8} \frac{d^2g}{dt^2} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
    - \frac{h^3}{48} \frac{d^3g}{dt^3} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
    + O(h^4).
\end{split}\]</div>
<p>Subtracting the two expressions,</p>
<div class="math notranslate nohighlight">
\[
  g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k) = h \frac{dg}{dt} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + O(h^3).
\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[
  \frac{dg}{dt} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) = \frac{g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k)}{h} + O(h^2).
\]</div>
<p>And summing them</p>
<div class="math notranslate nohighlight">
\[
  g(y(t_{k+1}), t_{k+1}) + g(y(t_k), t_k) =
  2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
  + \frac{h^2}{4} \frac{d^2g}{dt^2} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) + O(h^4).
\]</div>
<p>So,</p>
<div class="math notranslate nohighlight">
\[
  \frac{d^2g}{dt^2} \left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) =
  4\frac{ g(y(t_{k+1}), t_{k+1}) + g(y(t_k), t_k) -
  2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)}{h^2}
  + O(h^2).
\]</div>
<p>This results in the expression</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    g(y(\tau), \tau) = g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) +
    \left(\tau - t_{k+\frac{1}{2}}\right)  \frac{g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k)}{h} + \\
    + \frac{\left(\tau - t_{k+\frac{1}{2}}\right)^2}{2!} \frac{ 4 \left[g(y(t_{k+1}), t_{k+1}) + g(y(t_k), t_k) -
  2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]}{h^2} + \\
    + O((\tau - t_k)^3),
\end{split}\]</div>
<p><span class="math notranslate nohighlight">\(\forall \tau \in \mathbb{R}.\)</span></p>
<p>Putting in the variation of constants formula</p>
<div class="math notranslate nohighlight">
\[
y(t) = e^{-(t-t_0) \lambda}y_0 + \int_{t_0}^t e^{-\lambda(t-\tau)} g(y(\tau), \tau) d\tau,
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda}y(t_k) + \\
  + \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} \left[ g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) +
    \left(\tau - t_{k+\frac{1}{2}}\right)  \frac{g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k)}{h} + \\
    + \frac{\left(\tau - t_{k+\frac{1}{2}}\right)^2}{2!} \frac{ 4 \left[g(y(t_{k+1}), t_{k+1}) + g(y(t_k), t_k) -
  2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]}{h^2} + O((\tau - t_k)^3) \right] d\tau,
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  \\
  g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
  \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d \tau +
  \\
  \frac{g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k)}{h}
  \int_{t_k}^{t_{k+1}} \left(\tau - t_{k+\frac{1}{2}}\right) e^{-\lambda(t_{k+1}-\tau)} d \tau +
  \\
  + \frac{ 4 \left[g(y(t_{k+1}), t_{k+1}) + 
  \\
  g(y(t_k), t_k) - 2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]}{h^2}
  \int_{t_k}^{t_{k+1}} \frac{\left(\tau - t_{k+\frac{1}{2}}\right)^2}{2!} e^{-\lambda(t_{k+1}-\tau)} d \tau
  \\
  + \int_{t_k}^{t_{k+1}} O((\tau - t_k)^3) e^{-\lambda(t_{k+1}-\tau)} d \tau,
\end{split}\]</div>
<p>Since <span class="math notranslate nohighlight">\(\tau - t_{k+ \frac{1}{2}} = \tau - t_k - \frac{h}{2}\)</span> and <span class="math notranslate nohighlight">\(\left(\tau - t_{k+ \frac{1}{2}} \right)^2 = (\tau - t_k)^2 + \frac{h^2}{4} - h (\tau - t_k)\)</span>,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
  \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d \tau +
  \\
  \frac{g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k)}{h}
  \int_{t_k}^{t_{k+1}} \left(\tau - t_{k}\right) e^{-\lambda(t_{k+1}-\tau)} d \tau +
  \\
  + \frac{ 4 \left[g(y(t_{k+1}), t_{k+1}) + 
  \\
  g(y(t_k), t_k) - 2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]}{h^2}
  \int_{t_k}^{t_{k+1}} \frac{(\tau - t_k)^2}{2!} e^{-\lambda(t_{k+1}-\tau)} d \tau +
  \\
  - \frac{g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k)}{h}
  \int_{t_k}^{t_{k+1}} \frac{h}{2} e^{-\lambda(t_{k+1}-\tau)} d \tau +
  \\
  + \frac{ 4 \left[g(y(t_{k+1}), t_{k+1}) + g(y(t_k), t_k) - 2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]}{h^2}
  \int_{t_k}^{t_{k+1}} \frac{h^2}{4 \cdot 2!} e^{-\lambda(t_{k+1}-\tau)} d \tau +
  \\
  - \frac{ 4 \left[g(y(t_{k+1}), t_{k+1}) + g(y(t_k), t_k) - 2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]}{h^2}
  \int_{t_k}^{t_{k+1}} \frac{h (\tau - t_k)}{2!} e^{-\lambda(t_{k+1}-\tau)} d \tau +
  \\
  + \int_{t_k}^{t_{k+1}} O((\tau - t_k)^3) e^{-\lambda(t_{k+1}-\tau)} d \tau.
\end{split}\]</div>
<p>Then,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  \\
  g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
  h \phi_1(-h \lambda) +
  \\
  \frac{g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k)}{h}
  h^2 \phi_2 (-h \lambda) +
  \\
  + \frac{ 4 \left[g(y(t_{k+1}), t_{k+1}) + g(y(t_k), t_k) - 2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]}{h^2}
  h^3 \phi_3 (-h \lambda) +
  \\
  - \frac{g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k)}{h}
  \frac{h^2 \phi_1(-h \lambda)}{2} +
  \\
  + \frac{ 4 \left[g(y(t_{k+1}), t_{k+1}) + g(y(t_k), t_k) - 2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]}{h^2}
  \frac{h^3 \phi_1(-h \lambda)}{8} +
  \\
  - \frac{ 4 \left[g(y(t_{k+1}), t_{k+1}) + g(y(t_k), t_k) - 2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]}{h^2}
  \frac{h^3 \phi_2(-h \lambda)}{2} +
  \\
  + O(h^4 \phi_4(-h \lambda)).
\end{split}\]</div>
<p>i.e.</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  \\
  g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right)
  h \phi_1(-h \lambda) + %ok
  \\
  \left[g(y(t_{k+1}), t_{k+1}) - g(y(t_k), t_k)\right]
  \left( h \phi_2 (-h \lambda) - \frac{h \phi_1(-h \lambda)}{2} \right) +
  \\
  + 4 \left[g(y(t_{k+1}), t_{k+1}) + 
  \\
  g(y(t_k), t_k) - 2 g\left(y\left(t_{k+\frac{1}{2}}\right), t_{k+\frac{1}{2}}\right) \right]
  \left( h \phi_3 (-h \lambda) + \frac{h \phi_1(-h \lambda)}{8} - \frac{h \phi_2(-h \lambda)}{2} \right) + O(h^4).
\end{split}\]</div>
<p>Using the Cox and Mathhews’s ETDRK-2 expressions to approximate <span class="math notranslate nohighlight">\(y\left(t_{k+\frac{1}{2}}\right)\)</span> and <span class="math notranslate nohighlight">\(y(t_{k+1})\)</span>, since those are of order 2, i.e., <span class="math notranslate nohighlight">\(O(h^3)\)</span>, the expression of the method is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda} y(t_k) +
  \\
  g\left(c'_k, t_{k+\frac{1}{2}}\right)
  h \phi_1(-h \lambda) + %ok
  \\
  \left[g(c_k, t_{k+1}) - g(y(t_k), t_k)\right]
  \left( h \phi_2 (-h \lambda) - \frac{h \phi_1(-h \lambda)}{2} \right) +
  \\
  + 4 \left[g(c_k, t_{k+1}) + g(y(t_k), t_k) - 2 g\left(c'_k, t_{k+\frac{1}{2}}\right) \right]
  \left( h \phi_3 (-h \lambda) + \frac{h \phi_1(-h \lambda)}{8} - \frac{h \phi_2(-h \lambda)}{2} \right) + O(h^4),
\end{split}\]</div>
<p>with</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  c_k = e^{-h \lambda} y(t_k) +
  \\
  h \phi_1 (-\lambda h) g(y(t_k), t_k) +
  \\
  \left[g(a_k, t_{k+1}) - g(y(t_k), t_k) \right] h \phi_2 (-\lambda h),
  \\
  a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) h \phi_1(-h\lambda),
  \\
  c'_k = e^{- \frac{h \lambda}{2}} y(t_k) +
  \\
  \frac{h}{2} \phi_1 \left(- \frac{\lambda h}{2} \right) g(y(t_k), t_k) +
  \\
  \left[g\left(a'_k, t_{k+\frac{1}{2}}\right) - g(y(t_k), t_k) \right] \frac{h}{2} \phi_2 \left(-\frac{\lambda h}{2}\right),
  \\
  a'_k = e^{-\frac{h \lambda}{2}}y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1\left(-\frac{h \lambda}{2}\right).
\end{split}\]</div>
<p>Here de deducing isn’t exactly in Runge Kutta form, differing on the approximations for steps in minor order, so it cannot be a Butcher tableau, but doing language abuse only on the part that would form the triangle, it would be:</p>
<p>\begin{array}
{c|cccc}
0 \
\frac{1}{2} &amp; \frac{1}{2} \left( \phi_1\left(- \frac{\lambda h}{2} \right) - \phi_2\left(- \frac{\lambda h}{2} \right) \right) &amp; \frac{1}{2}\phi_2\left(- \frac{\lambda h}{2} \right) \
1 &amp; \phi_1\left(- \lambda h \right) - \phi_2\left(- \lambda h \right) &amp; 0 &amp; \phi_2\left(- \lambda h \right)  \
\hline
&amp; 4 \phi_3(-h \lambda)-3\phi_2(-h\lambda)+\phi_1(-h\lambda) &amp; -8\phi_3(-h\lambda)+4\phi_2(-h\lambda) &amp; 4 \phi_3(-h\lambda)-\phi_2(-h\lambda) \text{   }.
\end{array}</p>
</section>
<section id="naive-etd3rk">
<h4>Naive etd3rk<a class="headerlink" href="#naive-etd3rk" title="Permalink to this headline">#</a></h4>
<p>Here, that is taken the variation of constants formula:</p>
<div class="math notranslate nohighlight">
\[
    y(t_{k+1}) = e^{-h \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} g(y(\tau), \tau) d\tau,
\]</div>
<p>and applied the Simpson’s rule (here was used the order of convergence from Burden) so that it will be:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    y(t_{k+1}) = e^{-h \lambda}y(t_k) + 
    \\
    \frac{h}{6} \left[ e^{-\lambda(t_{k+1}-t_k)} g(y(t_k), t_k) + 4 e^{-\lambda \left(t_{k+1}-t_{k + \frac{1}{2}} \right)} g\left(y\left(t_{k+\frac{1}{2}}\right), t_k + \frac{h}{2} \right) \\ + e^{-\lambda(t_{k+1}-t_{k+1})} g(y(t_{k+1}), t_{k+1}) \right] 
    \\
    + O(h^5), \\
    y(t_{k+1}) = e^{-h \lambda}y(t_k) + 
    \\
    \frac{h}{6} \left[ e^{-\lambda h} g(y(t_k), t_k) + 4 e^{-\frac{ \lambda h}{2}} g\left(y\left(t_k + \frac{h}{2} \right), t_k + \frac{h}{2} \right) + g(y(t_{k+1}), t_{k+1}) \right] 
    \\
    +  O(h^5).
\end{split}\]</div>
<p>To approximate <span class="math notranslate nohighlight">\(y\left(t_k + \frac{h}{2} \right)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    y\left(t_k + \frac{h}{2} \right) = e^{- \frac{h \lambda}{2}}y(t_k) + \frac{h}{4} \left[ e^{- \frac{h \lambda}{2}} g(y(t_k), t_k) + g \left(a'_{k}, t_k + \frac{h}{2} \right) \right] +  O(h^3), \\
    \text{with } a'_{k} = e^{- \frac{h \lambda}{2}} y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1 \left(-\lambda \frac{h}{2} \right),
\end{split}\]</div>
<p>and, for <span class="math notranslate nohighlight">\(y\left(t_{k+1} \right)\)</span>:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    y(t_{k+1}) = e^{-h \lambda}y(t_k) + \frac{h}{2} \left[ e^{-\lambda h} g(y(t_k), t_k) + g(a_k, t_{k+1}) \right] +  O(h^3), \\
    \text{with } a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) h \phi_1 (-\lambda h).
\end{split}\]</div>
<p>So, the expression is</p>
<div class="math notranslate nohighlight">
\[\begin{split}
  y(t_{k+1}) = e^{-h \lambda}y(t_k) + \frac{h}{6} \left[ e^{-\lambda h} g(y(t_k), t_k) + 4 e^{-\frac{ \lambda h}{2}} g\left( b'_{k}, t_k + \frac{h}{2} \right) + g(b_k, t_{k+1}) \right] +  O(h^4), \\
  \text{with } b'_{k} = e^{- \frac{h \lambda}{2}}y(t_k) + \frac{h}{4} \left[ e^{- \frac{h \lambda}{2}} g(y(t_k), t_k) + g \left(a'_{k}, t_k + \frac{h}{2} \right) \right], \\
  b_k = e^{-h \lambda}y(t_k) + \frac{h}{2} \left[ e^{-\lambda h} g(y(t_k), t_k) + g(a_k, t_{k+1}) \right], \\
  a'_{k} = e^{- \frac{h \lambda}{2}} y(t_k) + g(y(t_k), t_k) \frac{h}{2} \phi_1 \left(-\lambda \frac{h}{2} \right), \\
  a_k = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) h \phi_1 (-\lambda h).
\end{split}\]</div>
</section>
</section>
<section id="matrix-exponential">
<h3>Matrix exponential<a class="headerlink" href="#matrix-exponential" title="Permalink to this headline">#</a></h3>
<p>This part has information from [4].</p>
<p>Based on the Maclaurin series of the exponential function</p>
<div class="math notranslate nohighlight">
\[
    e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!},
\]</div>
<p>the <span class="math notranslate nohighlight">\(\textbf{exponential of a square complex matrix }A\)</span> is defined as</p>
<div class="math notranslate nohighlight">
\[
    e^A \doteq \sum_{i=0}^{\infty} \frac{A^i}{i!}.
\]</div>
<p>This is well defined because it has been proven that the sequence <span class="math notranslate nohighlight">\({p_k}\)</span> with, <span class="math notranslate nohighlight">\(\forall k \in \mathbb{N}\)</span>:</p>
<div class="math notranslate nohighlight">
\[
    p_k = \sum_{i=0}^{k} \frac{A^i}{i!}, \forall A \text{ as decribed above,}
\]</div>
<p>is a Cauchy sequence, and therefore converge to a limit matrix which was denoted <span class="math notranslate nohighlight">\(e^A\)</span>, since the set of the square complex matrix with fixed lenght with the norm</p>
<div class="math notranslate nohighlight">
\[
||A|| = \max_{||x||=1} ||Ax||
\]</div>
<p>is a Banach space.</p>
<section id="exponential-of-a-zeros-matrix">
<h4>Exponential of a zeros matrix<a class="headerlink" href="#exponential-of-a-zeros-matrix" title="Permalink to this headline">#</a></h4>
<p>If <span class="math notranslate nohighlight">\(A =   
\left[ {\begin{array}{ccccc}
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
\end{array} } \right] \)</span>,</p>
<div class="math notranslate nohighlight">
\[
    e^A \doteq \sum_{i=0}^{\infty} \frac{A^i}{i!} = I + A + \frac{A^2}{2} + \dotsm = I + 0 + 0 + \dotsm = I.
\]</div>
</section>
<section id="exponential-of-a-diagonal-matrix">
<h4>Exponential of a diagonal matrix<a class="headerlink" href="#exponential-of-a-diagonal-matrix" title="Permalink to this headline">#</a></h4>
<p>If <span class="math notranslate nohighlight">\(A =   
\left[ {\begin{array}{ccccc}
    \lambda_1 &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; \lambda_2 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; \lambda_3 &amp; \dotsm &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; \lambda_{N}\\
\end{array} } \right] 
  = diag(\lambda_1, \lambda_2, \lambda_3, \dotsm, \lambda_N)\)</span>,</p>
<p>it is easy to note that</p>
<div class="math notranslate nohighlight">
\[
    A^2 = diag \left(\lambda_1^2, \lambda_2^2, \lambda_3^2, \dotsc, \lambda_N^2 \right)
\]</div>
<div class="math notranslate nohighlight">
\[
    A^3 = diag \left(\lambda_1^3, \lambda_2^3, \lambda_3^3, \dotsc, \lambda_N^3 \right)
\]</div>
<div class="math notranslate nohighlight">
\[
\vdots
\]</div>
<div class="math notranslate nohighlight">
\[
    A^j = diag \left(\lambda_1^j, \lambda_2^j, \lambda_3^j, \dotsc, \lambda_N^j \right) , \forall j \in \mathbb{N}
\]</div>
<div class="math notranslate nohighlight">
\[
\vdots
\]</div>
<p>so</p>
<div class="math notranslate nohighlight">
\[
    e^A \doteq \sum_{i=0}^{\infty} \frac{A^i}{i!} = diag\left(\sum_{i=0}^{\infty} \frac{\lambda_1^i}{i!}, \sum_{i=0}^{\infty} \frac{\lambda_2^i}{i!}, \sum_{i=0}^{\infty} \frac{\lambda_3^i}{i!}, \dotsc, \sum_{i=0}^{\infty} \frac{\lambda_N^i}{i!}\right)
\]</div>
<div class="math notranslate nohighlight">
\[
    = diag \left( e^{\lambda_1}, e^{\lambda_2}, e^{\lambda_3}, \dotsc, e^{\lambda_N} \right).
\]</div>
<p>In the same way, if B is a diagonal by blocks matrix:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
B =   
\left[ {\begin{array}{ccccc}
    B_1 &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; B_2 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; B_3 &amp; \dotsm &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; B_{N}\\
\end{array} } \right] 
  = diag(B_1, B_2, B_3, \dotsm, B_N),
\end{split}\]</div>
<p>then</p>
<div class="math notranslate nohighlight">
\[
e^B = diag(e^{B_1}, e^{B_2}, e^{B_3}, \dotsm, e^{B_N}).
\]</div>
</section>
<section id="exponential-of-a-matrix-of-ones-above-the-diagonal">
<h4>Exponential of a matrix of ones above the diagonal<a class="headerlink" href="#exponential-of-a-matrix-of-ones-above-the-diagonal" title="Permalink to this headline">#</a></h4>
<p>If <span class="math notranslate nohighlight">\(A = A_{N \times N} =   
\left[ {\begin{array}{ccccccc}
    0 &amp; 1 &amp;  &amp;  &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 1 &amp;  &amp;  &amp;  &amp;\\
     &amp;  &amp; 0 &amp; 1 &amp;  &amp;  &amp;\\
     &amp;  &amp;  &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp;  &amp;  &amp; 0 &amp; \ddots &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; 1 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
\end{array} } \right] \)</span>,</p>
<p>one can calculate</p>
<div class="math notranslate nohighlight">
\[\begin{split}
A^2 = A \cdot A =  
\left[ {\begin{array}{ccccccc}
    0 &amp; 1 &amp;  &amp;  &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 1 &amp;  &amp;  &amp;  &amp;\\
     &amp;  &amp; 0 &amp; 1 &amp;  &amp;  &amp;\\
     &amp;  &amp;  &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp;  &amp;  &amp; 0 &amp; \ddots &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; 1 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
\end{array} } \right]  \cdot 
\left[ {\begin{array}{ccccccc}
    0 &amp; 1 &amp;  &amp;  &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 1 &amp;  &amp;  &amp;  &amp;\\
     &amp;  &amp; 0 &amp; 1 &amp;  &amp;  &amp;\\
     &amp;  &amp;  &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp;  &amp;  &amp; 0 &amp; \ddots &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; 1 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
\end{array} } \right] 
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
  =   \left[ {\begin{array}{ccccccc}
    0 &amp; 0 &amp; 1 &amp;  &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 0 &amp; 1 &amp;  &amp;  &amp;\\
     &amp;  &amp; 0 &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp;  &amp; 0 &amp; 0 &amp; \ddots &amp;\\
     &amp;  &amp;  &amp;  &amp; 0 &amp; \ddots &amp; 1 \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; 0 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
\end{array} } \right], 
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
    A^3 = A \cdot A^2 = \left[ {\begin{array}{ccccccc}
    0 &amp; 1 &amp;  &amp;  &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 1 &amp;  &amp;  &amp;  &amp;\\
     &amp;  &amp; 0 &amp; 1 &amp;  &amp;  &amp;\\
     &amp;  &amp;  &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp;  &amp;  &amp; 0 &amp; \ddots &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; 1 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccccc}
    0 &amp; 0 &amp; 1 &amp;  &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 0 &amp; 1 &amp;  &amp;  &amp;\\
     &amp;  &amp; 0 &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp;  &amp; 0 &amp; 0 &amp; \ddots &amp;\\
     &amp;  &amp;  &amp;  &amp; 0 &amp; \ddots &amp; 1 \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; 0 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
\end{array} } \right] 
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
= \left[ {\begin{array}{ccccccc}
    0 &amp; 0 &amp; 0 &amp; 1 &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 0 &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp; 0 &amp; 0 &amp; 0 &amp; \ddots &amp;\\
     &amp;  &amp;  &amp; 0 &amp; 0 &amp; \ddots &amp; 1\\
     &amp;  &amp;  &amp;  &amp; 0 &amp; \ddots &amp; 0 \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; 0 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
\end{array} } \right],
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
    \vdots
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
    A^{N-2} = \left[ {\begin{array}{ccccccc}
     &amp;  &amp;  &amp;  &amp; 0 &amp; 1 &amp; 0\\
     &amp;  &amp;  &amp;  &amp;  &amp; 0 &amp; 1 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
\end{array} } \right],
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
    A^{N-1} = \left[ {\begin{array}{ccccccc}
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 1\\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
\end{array} } \right],
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
    A^{N} = 0.
\]</div>
<p>And then, with <span class="math notranslate nohighlight">\(t \in \mathbb{R}\)</span></p>
<div class="math notranslate nohighlight">
\[
    e^{tA} \doteq \sum_{i=0}^{\infty} \frac{tA^i}{i!}
\]</div>
<div class="math notranslate nohighlight">
\[
    = Id + tA + \frac{t^2 A^2}{2} + \frac{t^3 A^3}{6} + \dotsc + \frac{t^{N-2} A^{N-2}}{(N-2)!} + \frac{t^{N-1} A^{N-1}}{(N-1)!} + 0 + 0 + \dotsc + 0
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
    = \left[ {\begin{array}{ccccccc}
    1 &amp;  &amp;  &amp;  &amp;  &amp;  &amp; \\
     &amp; 1 &amp;  &amp;  &amp;  &amp;  &amp;\\
     &amp;  &amp; 1 &amp;  &amp;  &amp;  &amp;\\
     &amp;  &amp;  &amp; 1 &amp;  &amp;  &amp;\\
     &amp;  &amp;  &amp;  &amp; 1 &amp;  &amp;\\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp;\\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 1 \\
\end{array} } \right] + \left[ {\begin{array}{ccccccc}
    0 &amp; t &amp;  &amp;  &amp;  &amp;  &amp; \\
     &amp; 0 &amp; t &amp;  &amp;  &amp;  &amp;\\
     &amp;  &amp; 0 &amp; t &amp;  &amp;  &amp;\\
     &amp;  &amp;  &amp; 0 &amp; t &amp;  &amp;\\
     &amp;  &amp;  &amp;  &amp; 0 &amp; \ddots &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; t \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
\end{array} } \right] + 
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
+ \left[ {\begin{array}{ccccccc}
    0 &amp; 0 &amp; \frac{t^2}{2} &amp;  &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 0 &amp; \frac{t^2}{2} &amp;  &amp;  &amp;\\
     &amp;  &amp; 0 &amp; 0 &amp; \frac{t^2}{2} &amp;  &amp;\\
     &amp;  &amp;  &amp; 0 &amp; 0 &amp; \ddots &amp;\\
     &amp;  &amp;  &amp;  &amp; 0 &amp; \ddots &amp; \frac{t^2}{2} \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; 0 \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 0 \\
\end{array} } \right] + \dotsc + \left[ {\begin{array}{ccccccc}
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; \frac{t^{N-1}}{(N-1)!}\\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp;  \\
\end{array} } \right]
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
    = \left[ {\begin{array}{ccccccc}
    1 &amp; t &amp; \frac{t^2}{2} &amp; \frac{t^3}{3!} &amp; \frac{t^4}{4!} &amp; \dotsc &amp; \frac{t^{N-1}}{(N-1)!}\\
     &amp; 1 &amp; t &amp; \frac{t^2}{2} &amp; \frac{t^3}{3!} &amp; \ddots &amp; \vdots \\
     &amp;  &amp; 1 &amp; t &amp; \frac{t^2}{2} &amp; \ddots &amp; \frac{t^4}{4!}\\
     &amp;  &amp;  &amp; 1 &amp; t &amp; \ddots &amp; \frac{t^3}{3!}\\
     &amp;  &amp;  &amp;  &amp; 1 &amp; \ddots &amp; \frac{t^2}{2} \\
     &amp;  &amp;  &amp;  &amp;  &amp; \ddots &amp; t \\
     &amp;  &amp;  &amp;  &amp;  &amp;  &amp; 1 \\
\end{array} } \right].
\end{split}\]</div>
</section>
<section id="exponential-of-a-jordan-block">
<h4>Exponential of a Jordan block<a class="headerlink" href="#exponential-of-a-jordan-block" title="Permalink to this headline">#</a></h4>
<p><span class="math notranslate nohighlight">\(\textbf{Proposition:}\)</span> <span class="math notranslate nohighlight">\(A_1, A_2 \in \mathscr{M}_{N \times N}(\mathbb{C})\)</span>. If <span class="math notranslate nohighlight">\(A_1 \cdot A_2 = A_2 \cdot A_1\)</span>, then <span class="math notranslate nohighlight">\(e^{A_1+A_2} = e^{A_1} \cdot e^{A_2}\)</span>.</p>
<p>A Jordan block is of the form:
$<span class="math notranslate nohighlight">\(
J = \left[ {\begin{array}{ccccc}
    \lambda_i &amp; 1 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; \lambda_i &amp; 1 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; \lambda_i &amp; \ddots &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; 1\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; \lambda_i\\
\end{array} } \right] 
\)</span>$</p>
<div class="math notranslate nohighlight">
\[\begin{split}
= \left[ {\begin{array}{ccccc}
    \lambda_i &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; \lambda_i &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; \lambda_i &amp; \dotsm &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; \lambda_i\\
\end{array} } \right] + \left[ {\begin{array}{ccccc}
    0 &amp; 1 &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp; 0 &amp; \ddots &amp;\\
     &amp;  &amp;  &amp; \ddots &amp; 1\\
     &amp;  &amp;  &amp;  &amp; 0\\
\end{array} } \right] 
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[
    = D + N,
\]</div>
<p>and
$<span class="math notranslate nohighlight">\(
\left[ {\begin{array}{ccccc}
    \lambda_i &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; \lambda_i &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; \lambda_i &amp; \dotsm &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; \lambda_i\\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccc}
    0 &amp; 1 &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp; 0 &amp; \ddots &amp;\\
     &amp;  &amp;  &amp; \ddots &amp; 1\\
     &amp;  &amp;  &amp;  &amp; 0\\
\end{array} } \right] 
\)</span>$</p>
<div class="math notranslate nohighlight">
\[\begin{split}
= \left[ {\begin{array}{ccccc}
    0 &amp; \lambda_i &amp;  &amp;  &amp; \\
     &amp; 0 &amp; \lambda_i &amp;  &amp;\\
     &amp;  &amp; 0 &amp; \ddots &amp;\\
     &amp;  &amp;  &amp; \ddots &amp; \lambda_i\\
     &amp;  &amp;  &amp;  &amp; 0\\
\end{array} } \right] 
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
= \left[ {\begin{array}{ccccc}
    0 &amp; 1 &amp;  &amp;  &amp; \\
     &amp; 0 &amp; 1 &amp;  &amp;\\
     &amp;  &amp; 0 &amp; \ddots &amp;\\
     &amp;  &amp;  &amp; \ddots &amp; 1\\
     &amp;  &amp;  &amp;  &amp; 0\\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccc}
    \lambda_i &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; \lambda_i &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; \lambda_i &amp; \dotsm &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; \lambda_i\\
\end{array} } \right],
\end{split}\]</div>
<p>so</p>
<div class="math notranslate nohighlight">
\[
    e^{tJ} = e^{tD+tN} = e^{tD} \cdot e^{tN}
\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
= \left[ {\begin{array}{ccccc}
    e^{t \lambda_i} &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; e^{t \lambda_i} &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; e^{t \lambda_i} &amp; \dotsm &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; e^{t \lambda_i}\\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccc}
    1 &amp; t &amp; \frac{t^2}{2} &amp; \dotsc &amp; \frac{t^{N-1}}{(N-1)!}\\
     &amp; 1 &amp; t &amp; \ddots &amp; \vdots\\
     &amp;  &amp; 1 &amp; \ddots &amp; \frac{t^2}{2} \\
     &amp;  &amp;  &amp; \ddots &amp; t \\
     &amp;  &amp;  &amp;  &amp; 1 \\
\end{array} } \right]
\end{split}\]</div>
<div class="math notranslate nohighlight">
\[\begin{split}
= \left[ {\begin{array}{ccccc}
    e^{t \lambda_i} &amp; e^{t \lambda_i}t &amp; \frac{e^{t \lambda_i} t^2}{2} &amp; \dotsc &amp; \frac{e^{t \lambda_i} t^{N-1}}{(N-1)!}\\
     &amp; e^{t \lambda_i} &amp; e^{t \lambda_i} t &amp; \ddots &amp; \vdots\\
     &amp;  &amp; e^{t \lambda_i} &amp; \ddots &amp; \frac{e^{t \lambda_i} t^2}{2} \\
     &amp;  &amp;  &amp; \ddots &amp; e^{t \lambda_i} t \\
     &amp;  &amp;  &amp;  &amp; e^{t \lambda_i} \\
\end{array} } \right], t \in \mathbb{R}.
\end{split}\]</div>
</section>
<section id="exponential-of-any-matrix">
<h4>Exponential of any matrix<a class="headerlink" href="#exponential-of-any-matrix" title="Permalink to this headline">#</a></h4>
<p><span class="math notranslate nohighlight">\(\textbf{Proposition: } \forall A \in \mathscr{M}_{N \times N}(\mathbb{C}), \exists M \in \mathscr{M}_{N \times N}(\mathbb{C})\)</span> invertible, such that <span class="math notranslate nohighlight">\(A = MJM^{-1}\)</span>, with</p>
<div class="math notranslate nohighlight">
\[\begin{split} 
J = \left[ {\begin{array}{ccccc}
    J_1 &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; J_2 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; J_3 &amp; \dotsm &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; J_{N}\\
\end{array} } \right]
\end{split}\]</div>
<p>and each <span class="math notranslate nohighlight">\(J_i\)</span>, <span class="math notranslate nohighlight">\(i = 1, 2, 3, \dotsc, N\)</span> being a Jordan block, i.e.,</p>
<div class="math notranslate nohighlight">
\[\begin{split}
J_i = \left[ {\begin{array}{ccccc}
    \lambda_i &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; \lambda_i &amp; 0 &amp; \dotsm &amp; 0\\
    0 &amp; 0 &amp; \lambda_i &amp; \dotsm &amp; 0\\
    \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
    0 &amp; 0 &amp; 0 &amp; \dotsm &amp; \lambda_i\\
\end{array} } \right]
\end{split}\]</div>
<p>for some <span class="math notranslate nohighlight">\(\lambda_i \in \mathbb{C}\)</span> .</p>
<p>Note that
$<span class="math notranslate nohighlight">\(
    (MJM^{-1})^k = MJM^{-1}MJM^{-1}MJM^{-1} \dotsc MJM^{-1} 
\)</span>$</p>
<div class="math notranslate nohighlight">
\[
    = MJIJIJM^{-1} \dotsc MJM^{-1} = MJJJ \dotsc JM^{-1} = MJ^kM^{-1}.
\]</div>
<p>Because of the formula of the series that defines the expansion, it implicates in <span class="math notranslate nohighlight">\(e^{MJM^{-1}} = M e^J M^{-1}\)</span>.</p>
<p>And then, using the same notation from the last proposition,
$<span class="math notranslate nohighlight">\(
e^{tA} = e^{tMJM^{-1}} = e^{MtJM^{-1}} = Me^{tJ}M^{-1} 
\)</span>$</p>
<div class="math notranslate nohighlight">
\[\begin{split}
    = M \left[ { \begin{array}{ccccc}
        e^{tJ_1} &amp; 0 &amp; 0 &amp; \dotsm &amp; 0\\
        0 &amp; e^{tJ_2} &amp; 0 &amp; \dotsm &amp; 0\\
        0 &amp; 0 &amp; e^{tJ_3} &amp; \dotsm &amp; 0\\
        \vdots &amp; \vdots &amp; \vdots &amp; \ddots &amp; \vdots\\
        0 &amp; 0 &amp; 0 &amp; \dotsm &amp; e^{tJ_{N}}\\
    \end{array} } \right] M^{-1}, t \in \mathbb{R},
\end{split}\]</div>
<p>with each block as the section above indicates.</p>
</section>
</section>
<section id="euler-method">
<h3>Euler method<a class="headerlink" href="#euler-method" title="Permalink to this headline">#</a></h3>
<p>Further detailing this explicit one-step method of</p>
<div class="math notranslate nohighlight">
\[
    \phi (t_{k},y_{k},h) = f(t_{k},y_{k}),
\]</div>
<p>an analysis on stability, convergence and order of convergence is done.</p>
<section id="stability">
<h4>Stability<a class="headerlink" href="#stability" title="Permalink to this headline">#</a></h4>
<p>For the problem
<span class="math notranslate nohighlight">\(\begin{cases}
    y'(t) = - \lambda y(t) \text{ ; } t \in [t_0 , T] \\
    y(t_0)=y_0,
\end{cases}\)</span></p>
<p>with known solution</p>
<div class="math notranslate nohighlight">
\[ y(t) = y_0e^{-\lambda (t-t_0)},\]</div>
<p>the method turn into:</p>
<div class="math notranslate nohighlight">
\[\begin{split}
y_0 = y(t_0)\\
\textbf{for } k = 0, 1, 2, ..., N-1 :\\
    y_{k+1} = y_k + h \lambda y_k \\
    t_{k+1} = t_k + h.
\end{split}\]</div>
<p>Then the amplification factor is:
$<span class="math notranslate nohighlight">\(
(1 - h \lambda).
\)</span>$</p>
<p>If</p>
<div class="math notranslate nohighlight">
\[
|1 - h \lambda| &gt; 1, \text{for fixed } N,
\]</div>
<p>it will be a divergent series</p>
<div class="math notranslate nohighlight">
\[
(k \rightarrow \infty \Rightarrow y_k \rightarrow \infty),
\]</div>
<p>so, since the computer has a limitant number that can represent, even if the number of steps is such that <span class="math notranslate nohighlight">\(h\)</span> is not small enought, it might have sufficient steps to reach the maximum number represented by the machine.</p>
<p>However, if</p>
<div class="math notranslate nohighlight">
\[
    |1 - h \lambda| &lt; 1 \text{ and } N \text{ is fixed,}
\]</div>
<p>it converges to zero</p>
<div class="math notranslate nohighlight">
\[
    (k \rightarrow \infty \Rightarrow y_k \rightarrow 0 ).
\]</div>
<p>Besides that,</p>
<div class="math notranslate nohighlight">
\[
|1 - h \lambda| &lt; 1
\]</div>
<p>is the same as</p>
<div class="math notranslate nohighlight">
\[
0 &lt; h \lambda &lt; 2.
\]</div>
<p>So the interval of stability is <span class="math notranslate nohighlight">\((0,2)\)</span>.</p>
<p>That’s why the method suddenly converged, it was when <span class="math notranslate nohighlight">\(h\)</span> got small enought to <span class="math notranslate nohighlight">\(h \lambda\)</span> be in the interval of stability, i.e.,</p>
<div class="math notranslate nohighlight">
\[
    h &lt; 2/\lambda.
\]</div>
<p>It is worth mentioning here that if</p>
<div class="math notranslate nohighlight">
\[
-1 &lt; 1 - h \lambda &lt; 0,
\]</div>
<p>the error will converge oscillating since it takes positive values with even exponents and negative with odd ones.</p>
</section>
<section id="convergence">
<h4>Convergence<a class="headerlink" href="#convergence" title="Permalink to this headline">#</a></h4>
<p>Since</p>
<div class="math notranslate nohighlight">
\[
\lim_{m \to +\infty} \left(1 + \frac{p}{m} \right)^m = e^p,
\]</div>
<p>and h = <span class="math notranslate nohighlight">\(\frac{T-t_0}{N}\)</span>, for <span class="math notranslate nohighlight">\(y_N\)</span> we have</p>
<div class="math notranslate nohighlight">
\[
\lim_{N \to +\infty} y_N = \lim_{N \to +\infty} \left(1 - h \lambda \right)^N y_0 = \lim_{N \to +\infty} \left(1 - \frac{(T-t_0) \lambda}{N} \right)^N y_0.
\]</div>
<p>It is reasonable to take <span class="math notranslate nohighlight">\(p = -(T-t_0) \lambda\)</span> and conclude that the last point estimated by the method will converge to</p>
<div class="math notranslate nohighlight">
\[
y_0e^{-\lambda (T-t_0)}.
\]</div>
<p>Which is precisely <span class="math notranslate nohighlight">\(y(T)\)</span> and proves the convergence.</p>
</section>
<section id="order-of-convergence">
<h4>Order of convergence<a class="headerlink" href="#order-of-convergence" title="Permalink to this headline">#</a></h4>
<p>Being <span class="math notranslate nohighlight">\(\tau(h, t_k)\)</span> the local truncation error.</p>
<p>From</p>
<div class="math notranslate nohighlight">
\[
    y(t_{k+1}) = y(t_k) + h f(y(t_k),t_k) + O(h^2),
\]</div>
<p>we have</p>
<div class="math notranslate nohighlight">
\[
    h \tau(h, t_k) \doteq \frac{y(t_{k+1}) - y(t_k)}{h} - f(t_k, y(t_k)) = O(h^2),
\]</div>
<p>so</p>
<div class="math notranslate nohighlight">
\[
    \tau(h, t_k) = O(h).
\]</div>
<p>Since for one step methods the order of convergence is the order of the local truncation error, the order is of <span class="math notranslate nohighlight">\(O(h)\)</span>, order 1.</p>
</section>
</section>
</section>
</div>
</section>

    <script type="text/x-thebe-config">
    {
        requestKernel: true,
        binderOptions: {
            repo: "binder-examples/jupyter-stacks-datascience",
            ref: "master",
        },
        codeMirrorConfig: {
            theme: "abcdef",
            mode: "python"
        },
        kernelOptions: {
            kernelName: "python3",
            path: "./."
        },
        predefinedOutput: true
    }
    </script>
    <script>kernelName = 'python3'</script>

              </div>
              
            </main>
            <footer class="footer-article noprint">
                
    <!-- Previous / next buttons -->
<div class='prev-next-area'>
</div>
            </footer>
        </div>
    </div>
    <div class="footer-content row">
        <footer class="col footer"><p>
  
    By Isabela Miki Suzuki<br/>
  
      &copy; Copyright 2022.<br/>
</p>
        </footer>
    </div>
    
</div>


      </div>
    </div>
  
  <!-- Scripts loaded after <body> so the DOM is not blocked -->
  <script src="_static/scripts/pydata-sphinx-theme.js?digest=1999514e3f237ded88cf"></script>


  </body>
</html>