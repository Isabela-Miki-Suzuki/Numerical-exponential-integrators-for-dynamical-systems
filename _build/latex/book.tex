%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Numerical exponential integrators for dynamical systems}
\date{Jun 27, 2023}
\release{}
\author{Isabela Miki Suzuki}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}

\begin{itemize}
\item {} 
\sphinxAtStartPar
Title:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Numerical exponential integrators for dynamical systems}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Researcher in charge:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{André Salles Carvalho, Prof. Dr.}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Beneficiary:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Isabela Miki Suzuki}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Host institution:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Instituto de Matemática e Estatística at the Universidade de São Paulo}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Research team:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Isabela Miki Suzuki}

\sphinxAtStartPar
\sphinxstylestrong{Pedro S. Peixoto, Prof. Dr.}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Number of the research project:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{2021/06678\sphinxhyphen{}5}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Duration:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{1 August 2021 to 31 July 2023}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Period covered by this research report:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{1 August 2022 to 26 June 2023}
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Summary_of_the_proposed_project::doc}]{\sphinxcrossref{1. Summary of the proposed project}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Summary_of_the_activities::doc}]{\sphinxcrossref{2. Summary of the activities}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Project_execution::doc}]{\sphinxcrossref{3. Project execution}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Description_and_evaluation_of_institutional_support_received_in_the_period::doc}]{\sphinxcrossref{Description and evaluation of institutional support received in the period}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Participation_in_scientific_event,_list_of_publications_and_list_of_papers_prepared_or_submitted::doc}]{\sphinxcrossref{Participation in scientific event, list of publications and list of papers prepared or submitted}}}

\end{itemize}

\sphinxstepscope


\chapter{1. Summary of the proposed project}
\label{\detokenize{Summary_of_the_proposed_project:summary-of-the-proposed-project}}\label{\detokenize{Summary_of_the_proposed_project::doc}}
\sphinxAtStartPar
This is a scientific initiation project that proposes the deep study of some of the main methods
of exponential integration for problems in dynamic systems, with emphasis on the paper {[}{]}.
Here, the undergraduate will study the construction, analysis, implementation and application
of them and at the end, it is expected that she is familiar with modern techniques of numerical
methods.

\sphinxAtStartPar
\sphinxstylestrong{Keywords:} exponential integrator, numerical methods, dynamical systems.

\sphinxstepscope


\chapter{2. Summary of the activities}
\label{\detokenize{Summary_of_the_activities:summary-of-the-activities}}\label{\detokenize{Summary_of_the_activities::doc}}
\sphinxAtStartPar
To be done.

\sphinxstepscope


\chapter{3. Project execution}
\label{\detokenize{Project_execution:project-execution}}\label{\detokenize{Project_execution::doc}}
\sphinxstepscope

\sphinxstepscope


\section{Motivation \sphinxhyphen{} Stiffness}
\label{\detokenize{cap1:motivation-stiffness}}\label{\detokenize{cap1::doc}}
\sphinxAtStartPar
The reason for studying exponential methods is that those are good with \(\textbf{stiff differential equations}\) in terms of precision and how small the time step is required to be to achieve good accuracy.


\subsection{Cauchy problem}
\label{\detokenize{cap1:cauchy-problem}}
\sphinxAtStartPar
A \(\textbf{Cauchy problem}\) is a ordinary differential equation (ODE) with initial conditions. Being its standard scalar form:

\sphinxAtStartPar
\(\begin{cases}
    y'(t) = f(y(t), t), t \in (t_0, T) \\
    y(t_0) = y_0 \in \mathbb{K} \text{,}
\end{cases}\)

\sphinxAtStartPar
with \(\mathbb{K}\) a field, \(f\) function with image in \(\mathbb{K}\) and \(t_0, T \in \mathbb{R}\).

\sphinxAtStartPar
Sometimes, it is convenient to separate the linear part of \(f\) as indicated below:
\begin{equation*}
\begin{split}
\begin{equation*}
    f(y(t), t) = g(y(t), t) - \lambda y(t) \text{,}
\end{equation*}
\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\lambda \in \mathbb{K}\) or \(\mathscr{M}_{N \times N}(\mathbb{K})\).

\sphinxAtStartPar
So the system is:

\sphinxAtStartPar
\(\begin{cases}
    y'(t) + \lambda y(t) = g(y(t), t), t \in (t_0, T) \\
    y(0) = y_0 
    \text{.}
\end{cases}\)

\sphinxAtStartPar
In this project, the stiff ones were those addressed.

\sphinxAtStartPar
Notation as in {[}1{]}.


\subsection{Stiffness}
\label{\detokenize{cap1:stiffness}}
\sphinxAtStartPar
The error of the approximation given by a method trying to estimate the solution of a Cauchy problem is always given by a term multiplied by a higher derivative of the exact solution, because of the Taylor expansion with Lagrange form of the remainder. In that way, if that is enough information about this derivative, the error can be estimated.

\sphinxAtStartPar
If the norm of the derivative increases with the time, but the exact solution doesn’t, that is possible that the error dominates the approximation and the precision is lost. Those problems are called \(\textbf{stiff equations}\).

\sphinxAtStartPar
Between them, there are the \(\textbf{stiff differential equations}\), that have exact solution given by the sum of a \(\textit{transient solution}\) with a \(\textit{steady state solution}\).

\sphinxAtStartPar
The \(\textbf{transient solution}\) is of the form:
\begin{equation*}
\begin{split}
\begin{align*}
e^{-ct} \text{, with c >>1, }
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
which is known to go to zero really fast as t increases. But its \(n\)th derivative
\begin{equation*}
\begin{split}
\begin{align*}
\mp c^{n}e^{-ct}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
doesn’t go as quickly and may increase in magnitude.

\sphinxAtStartPar
The \(\textbf{steady state solution}\), however, as its name implies, have small changes as time passes, with higher derivative being almost constant zero.

\sphinxAtStartPar
In a system of ODE’s, these characteristics are most common in problems in which the solution of the initial value problem is of the form
\begin{equation*}
\begin{split}
\begin{align*}
e^{A}
\end{align*}
\end{split}
\end{equation*}
\sphinxAtStartPar
being \(A\) a matrix such that \(\lambda_{min}\) and \(\lambda_{max}\) are the eigenvalue with minimum and maximum value in modulus and \(\lambda_{min} << \lambda_{max}\). On the bigger magnitude eigenvalue direction, the behaviour is very similar to the transient solution, having drastic changes over time and on the smaller one, comparing to that, changes almost nothing as times passes, like the steady state solution.

\sphinxAtStartPar
Work around these problems and being able to accurately approximate these so contrasting parts of the solutions requires more robust methods than the more classic and common one\sphinxhyphen{}step methods addressed at the beginning of the study of numerical methods for Cauchy problems. For the systems, it is also required that that is a precise way to calculate the exponential of a matrix.

\sphinxAtStartPar
In this project, we studied the \(\textbf{exponential methods}\), their capabilities to deal with these problems and the comparision with other simpler methods.

\sphinxAtStartPar
Definition from {[}2{]}.

\sphinxstepscope


\section{3.2 Chapter 2: Classical methods}
\label{\detokenize{cap2:chapter-2-classical-methods}}\label{\detokenize{cap2::doc}}
\sphinxAtStartPar
In order to show that the exponential methods improve in dealing with Stiff problems, that is necessary to know how the previows methods deal with them, so a review on the theory of the classical methods is made in this chapter. In particular there will be focus on the one step methods. All the information is from {[}3{]}.


\subsection{One step methods for ODE}
\label{\detokenize{cap2:one-step-methods-for-ode}}
\sphinxAtStartPar
In order to find a approximation for the solution of the problem
\textbackslash{}begin\{cases\}
y’(t) = f(t, y(t)), t \textbackslash{}in {[}t\_0,T{]} \textbackslash{}
y(t\_0)=y\_0 \textbackslash{}text\{,\}
\textbackslash{}end\{cases\}

\sphinxAtStartPar
they are of the form:
\$\(
y_{k+1} = y_{k} + h \phi (t_{k},y_{k},t_{k+1},y_{k+1},h) \text{,}
\)\$

\sphinxAtStartPar
with \$\(k = 0, 1, ..., n-1;\)\(
\)\(
N \in \mathbb{N}; h = \frac{T-t_0}{N}; \\
\{t_i = t_0 + ih : i = 0, 1, ..., N\}; \\ 
y_n \thickapprox y(t_n) .
\)\$

\sphinxAtStartPar
To analyse the method, there is a model problem
\textbackslash{}begin\{cases\}
y’(t) = \sphinxhyphen{} \textbackslash{}lambda y(t) \textbackslash{}text\{ ; \} t \textbackslash{}in{[}t\_0,T{]}\textbackslash{}
y(t\_0)=y\_0,\textbackslash{}
\textbackslash{}end\{cases\}

\sphinxAtStartPar
whose solution is \(y(t) = y_0 e^{-\lambda (t-t_0)}\)
with \(\lambda > 0.\)

\sphinxAtStartPar
If that is possible to manipulate the method so that, for this problem, can be written as
\$\(
y_{k+1} = \zeta(\lambda,h) y_k,
\)\(
then \)\(\zeta(\lambda,h)\)\( is called \)\textbackslash{}textbf\{amplification factor\}\$ of the method.

\sphinxAtStartPar
By induction, it gives
\$\(
y_{k+1} = \zeta(\lambda, h)^{k+1} y_0.
\)\$

\sphinxAtStartPar
It is well known that this expression only converges as k goes to infinity if
\$\(
|\zeta(\lambda, h)| < 1
\)\$
and then converges to zero.

\sphinxAtStartPar
When it occurs, i.e., \(k \rightarrow \infty \Rightarrow y_k \rightarrow 0\) such as the exact solution \(y(t) = y_0 e^{-\lambda (t-t_0)}\), it is said that there is \(\textbf{stability}\).

\sphinxAtStartPar
The inequation gives a interval for which values of \(\lambda h\), \(|\zeta(\lambda, h)|<1\), called \(\textbf{interval of stability}\).

\sphinxAtStartPar
And if the interval of stability contains all the points \(z\) such that \(Re(z) < 0\), the method is said \(\textbf{A-stable}\).

\sphinxAtStartPar
The reason for taking this specific problem is that it models the behaviour of the difference between the approximation and the solution on a small neighbourhood of any Cauchy problem:

\sphinxAtStartPar
Taking
\textbackslash{}begin\{cases\}
y’(t) = f(y(t), t), t \textbackslash{}in (t\_0, T) \textbackslash{}
y(t\_0) = y\_0 \textbackslash{}in \textbackslash{}mathbb\{K\}
\textbackslash{}end\{cases\}

\sphinxAtStartPar
and a approximation \(z\) of the solution \(y\), doing
\textbackslash{}begin\{align*\}
\textbackslash{}sigma(t) = z(t) \sphinxhyphen{} y(t) \textbackslash{}Rightarrow
\textbackslash{}end\{align*\}

\sphinxAtStartPar
\textbackslash{}begin\{align*\}
\textbackslash{}dot\{\textbackslash{}sigma\}(t) = \textbackslash{}dot\{z\}(t) \sphinxhyphen{} \textbackslash{}dot\{y\}(t) = f(z(t), t) \sphinxhyphen{} f(y(t), t) \textbackslash{}Rightarrow
\textbackslash{}end\{align*\}

\sphinxAtStartPar
\textbackslash{}begin\{align*\}
\textbackslash{}dot\{\textbackslash{}sigma\}(t) + \textbackslash{}dot\{y\}(t) = \textbackslash{}dot\{z\}(t) = f(z(t), t) = f(y(t)+\textbackslash{}sigma(t), t) = f(y(t), t) + \textbackslash{}sigma(t)\textbackslash{}frac\{\textbackslash{}partial f\}\{\textbackslash{}partial y\} + O(\textbackslash{}sigma\textasciicircum{}2(t)),
\textbackslash{}end\{align*\}

\sphinxAtStartPar
so
\textbackslash{}begin\{cases\}
\textbackslash{}dot\{\textbackslash{}sigma\}(t) \textbackslash{}approx \textbackslash{}sigma(t) \textbackslash{}frac\{\textbackslash{}partial f\}\{\textbackslash{}partial y\} (y(t), t) \textbackslash{}
\textbackslash{}sigma(t\_k) = \textbackslash{}sigma\_k.
\textbackslash{}end\{cases\}

\sphinxAtStartPar
Other important definitions are:

\sphinxAtStartPar
\(\textbf{Local truncation error:}\) Is the difference between the exact expression and its numerical approximation in a certain point and with a certain domain discretization. If the domain is equally spaced by \(h\) is often denoted by \(\tau(h,t_0)\) being \(t_0\) the point.

\sphinxAtStartPar
\(\textbf{Order of the local truncation error:}\) the local truncation error (which depends on the \(h\) spacing of the discretized domain) \(\tau(h)\) has order \(n \in \mathbb{N}\) if \(\tau(h) = O(h^n) \), i.e., if there is constant \(M \in \mathbb{R}\) and \(h_0 \in \mathbb{R}\) such that \(\tau(h) \leq M h^n\), \(\forall h \leq h_0\).

\sphinxAtStartPar
\(\textbf{Global error:}\) Is the difference between the approximation given by the method for the solution of the problem on a certain point and the exact one (unlike the local truncation error, here we take the solution we got, not the expression used to find the approximation).

\sphinxAtStartPar
\(\textbf{Consistency:}\) The method is said consistent if \(\lim _{h \to 0} \frac{1}{h}\tau(h,x_0) = 0\).

\sphinxAtStartPar
\(\textbf{Obs.:}\) For consistency, we usually only analyse for the linear part of the Cauchy problem, since this is the part that most influences in the consistency.

\sphinxAtStartPar
\(\textbf{Order of consistency:}\) is the smallest order (varying the points at which the local error is calculated) of the local truncation error.

\sphinxAtStartPar
\(\textbf{Convergence:}\) A numerical method is convergent if, and only if, for any well\sphinxhyphen{}posed Cauchy problem and for every \(t \in (t_0, T)\),
\$\(\lim_{h \to 0} e_k = 0\)\(
with \)t \sphinxhyphen{} t\_0 = kh\( fixed and \)e\_k\( denoting the global error on \)t\_k\$ (following the past notation).

\sphinxAtStartPar
\(\textbf{Theorem:}\) A one\sphinxhyphen{}step explicit method given by
\$\(
y_0 = y(t_0) \\
y_{k+1} = y_{k} + h \phi (t_{k},y_{k},h)
\)\(
such that \)\textbackslash{}phi\$ is Lipschitzian in y, continuous in their arguments, and consistent for any well\sphinxhyphen{}posed Cauchy problem is convergent. Besides that, the convergence order is greater or equal to the consistency order.

\sphinxAtStartPar
\(\textit{Prove:}\) {[}3{]} pág 29\sphinxhyphen{}31.


\subsection{Examples}
\label{\detokenize{cap2:examples}}
\sphinxAtStartPar
Euler method:

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
\textbackslash{}phi (t\_\{k\},y\_\{k\},h) = f(t\_\{k\},y\_\{k\})
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
Modified Euler method:

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
\textbackslash{}phi (t\_\{k\},y\_\{k\},h) = \textbackslash{}frac\{1\}\{2\} \textbackslash{}left{[} f(t\_\{k\},y\_\{k\}) + f(t\_\{k+1\},y\_\{k\} + h f(t\_\{k\},y\_\{k\})) \textbackslash{}right{]}
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
Midpoint method:

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
\textbackslash{}phi (t\_\{k\},y\_\{k\},h) = f(t\_\{k\} + \textbackslash{}frac\{h\}\{2\},y\_\{k\} + \textbackslash{}frac\{h\}\{2\} f(t\_\{k\},y\_\{k\}))
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
Classic Runge\sphinxhyphen{}Kutta (RK 4\sphinxhyphen{}4):

\sphinxAtStartPar
\textbackslash{}begin\{gather*\}
\textbackslash{}phi (t\_\{k\},y\_\{k\},h) = \textbackslash{}frac\{1\}\{6\} \textbackslash{}left( \textbackslash{}kappa\_1 + 2 \textbackslash{}kappa\_2 + 2 \textbackslash{}kappa\_3 + \textbackslash{}kappa\_4 \textbackslash{}right), \textbackslash{}text\{with \}\textbackslash{}
\textbackslash{}kappa\_1 = f(t\_\{k\},y\_\{k\})\textbackslash{}
\textbackslash{}kappa\_2 = f(t\_\{k\} + \textbackslash{}frac\{h\}\{2\},y\_\{k\} + \textbackslash{}frac\{h\}\{2\} \textbackslash{}kappa\_1)\textbackslash{}
\textbackslash{}kappa\_3 = f(t\_\{k\} + \textbackslash{}frac\{h\}\{2\},y\_\{k\} + \textbackslash{}frac\{h\}\{2\} \textbackslash{}kappa\_2)\textbackslash{}
\textbackslash{}kappa\_4 = f(t\_\{k\} + h, y\_\{k\} + h \textbackslash{}kappa\_3)
\textbackslash{}end\{gather*\}


\subsection{Euler method}
\label{\detokenize{cap2:euler-method}}
\sphinxAtStartPar
Further detailing this explicit one\sphinxhyphen{}step method of

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
\textbackslash{}phi (t\_\{k\},y\_\{k\},h) = f(t\_\{k\},y\_\{k\}),
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
an analysis on stability, convergence and order of convergence is done.


\subsubsection{Stability}
\label{\detokenize{cap2:stability}}
\sphinxAtStartPar
For the problem
\textbackslash{}begin\{cases\}
y’(t) = \sphinxhyphen{} \textbackslash{}lambda y(t) \textbackslash{}text\{ ; \} t \textbackslash{}in {[}t\_0 , T{]} \textbackslash{}
y(t\_0)=y\_0,
\textbackslash{}end\{cases\}

\sphinxAtStartPar
with known solution
\begin{equation*}
\begin{split} y(t) = y_0e^{-\lambda (t-t_0)},\end{split}
\end{equation*}
\sphinxAtStartPar
the method turn into:
\begin{equation*}
\begin{split}
y_0 = y(t_0)\\
\textbf{for } k = 0, 1, 2, ..., N-1 :\\
    y_{k+1} = y_k + h \lambda y_k \\
    t_{k+1} = t_k + h.
\end{split}
\end{equation*}
\sphinxAtStartPar
Then the amplification factor is:
\$\(
(1 - h \lambda).
\)\(
If \)|1 \sphinxhyphen{} h \textbackslash{}lambda| > 1\(, for fixed \)N\(, it will be a divergent series (\)k \textbackslash{}rightarrow \textbackslash{}infty \textbackslash{}Rightarrow y\_k \textbackslash{}rightarrow \textbackslash{}infty \(), so, since the computer has a limitant number that can represent, even if the number of steps is such that \)h\$ is not small enought, it might have sufficient steps to reach the maximum number represented by the machine.

\sphinxAtStartPar
However, if \(|1 - h \lambda| < 1\) and \(N\) is fixed, it converges to zero (\(k \rightarrow \infty \Rightarrow y_k \rightarrow 0 )\).

\sphinxAtStartPar
Besides that, \(|1 - h \lambda| < 1\) is the same as \(0 < h \lambda < 2\).
So the interval of stability is (0,2).
That’s why the method suddenly converged, it was when \(h\) got small enought to \(h \lambda\) be in the interval of stability, i.e., \(h < 2/\lambda\).

\sphinxAtStartPar
It is worth mentioning here that if \(-1 < 1 - h \lambda < 0\), the error will converge oscillating since it takes positive values with even exponents and negative with odd ones.


\subsubsection{Convergence}
\label{\detokenize{cap2:convergence}}
\sphinxAtStartPar
Since
\$\(
\lim_{m \to +\infty} \left(1 + \frac{p}{m} \right)^m = e^p,
\)\(
and h = \)\textbackslash{}frac\{T\sphinxhyphen{}t\_0\}\{N\}\(, for \)y\_N\( we have
\)\(
\lim_{N \to +\infty} y_N = \lim_{N \to +\infty} \left(1 - h \lambda \right)^N y_0 = \lim_{N \to +\infty} \left(1 - \frac{(T-t_0) \lambda}{N} \right)^N y_0.
\)\(
It is reasonable to take \)p = \sphinxhyphen{}(T\sphinxhyphen{}t\_0) \textbackslash{}lambda\( and conclude that the last point estimated by the method will converge to
\)\(
y_0e^{-\lambda (T-t_0)}.
\)\(
Which is precisely \)y(T)\$ and proves the convergence.


\subsubsection{Order of convergence}
\label{\detokenize{cap2:order-of-convergence}}
\sphinxAtStartPar
Being \(\tau(h, t_k)\) the local truncation error.

\sphinxAtStartPar
From
\textbackslash{}begin\{equation*\}
y(t\_\{k+1\}) = y(t\_k) + h f(y(t\_k),t\_k) + O(h\textasciicircum{}2),
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
we have
\textbackslash{}begin\{equation*\}
h \textbackslash{}tau(h, t\_k) \textbackslash{}doteq \textbackslash{}frac\{y(t\_\{k+1\}) \sphinxhyphen{} y(t\_k)\}\{h\} \sphinxhyphen{} f(t\_k, y(t\_k)) = O(h\textasciicircum{}2),
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
so
\textbackslash{}begin\{equation*\}
\textbackslash{}tau(h, t\_k) = O(h).
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
Since for one step methods the order of convergence is the order of the local truncation error, the order is of \(O(h)\), order 1.

\sphinxstepscope


\section{Chapter 3: important concepts for the study of exponential methods}
\label{\detokenize{cap3:chapter-3-important-concepts-for-the-study-of-exponential-methods}}\label{\detokenize{cap3::doc}}
\sphinxAtStartPar
In this chapter, a review on the theory of matrix exponential and in \(\phi\) functions is done because of its need when applying exponential methods in systems of ODE with initial value. Besides that, the format of the treated problem is shown.


\subsection{Matrix exponential}
\label{\detokenize{cap3:matrix-exponential}}
\sphinxAtStartPar
Based on the Maclaurin series of the exponential function
\textbackslash{}begin\{align*\}
e\textasciicircum{}x = \textbackslash{}sum\_\{i=0\}\textasciicircum{}\{\textbackslash{}infty\} \textbackslash{}frac\{x\textasciicircum{}i\}\{i!\},
\textbackslash{}end\{align*\}

\sphinxAtStartPar
the \(\textbf{exponential of a square complex matrix }A\) is defined as
\textbackslash{}begin\{align*\}
e\textasciicircum{}A \textbackslash{}doteq \textbackslash{}sum\_\{i=0\}\textasciicircum{}\{\textbackslash{}infty\} \textbackslash{}frac\{A\textasciicircum{}i\}\{i!\}.
\textbackslash{}end\{align*\}

\sphinxAtStartPar
This is well defined because it has been proven that the sequence \({p_k}\) with, \(\forall k \in \mathbb{N}\):
\textbackslash{}begin\{align*\}
p\_k = \textbackslash{}sum\_\{i=0\}\textasciicircum{}\{k\} \textbackslash{}frac\{A\textasciicircum{}i\}\{i!\}, \textbackslash{}text\{ \(\forall A\) as decribed above,\}
\textbackslash{}end\{align*\}
is a Cauchy sequence, and therefore converge to a limit matrix which was denoted \(e^A\), since the set of the square complex matrix with fixed lenght with the norm
\$\(
||A|| = \max_{||x||=1} ||Ax||
\)\$
is a Banach space.


\subsubsection{Exponential of a zeros matrix}
\label{\detokenize{cap3:exponential-of-a-zeros-matrix}}
\sphinxAtStartPar
If \(A =   
\left[ {\begin{array}{ccccc}
    0 & 0 & 0 & \dotsm & 0\\
    0 & 0 & 0 & \dotsm & 0\\
    0 & 0 & 0 & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & 0\\
\end{array} } \right] \),

\sphinxAtStartPar
\textbackslash{}begin\{align*\}
e\textasciicircum{}A \textbackslash{}doteq \textbackslash{}sum\_\{i=0\}\textasciicircum{}\{\textbackslash{}infty\} \textbackslash{}frac\{A\textasciicircum{}i\}\{i!\} = I + A + \textbackslash{}frac\{A\textasciicircum{}2\}\{2\} + \textbackslash{}dotsm = I + 0 + 0 + \textbackslash{}dotsm = I.
\textbackslash{}end\{align*\}


\subsubsection{Exponential of a diagonal matrix}
\label{\detokenize{cap3:exponential-of-a-diagonal-matrix}}
\sphinxAtStartPar
If \(A =   
\left[ {\begin{array}{ccccc}
    \lambda_1 & 0 & 0 & \dotsm & 0\\
    0 & \lambda_2 & 0 & \dotsm & 0\\
    0 & 0 & \lambda_3 & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_{N}\\
\end{array} } \right] 
  = diag(\lambda_1, \lambda_2, \lambda_3, \dotsm, \lambda_N)\),

\sphinxAtStartPar
it is easy to note that

\sphinxAtStartPar
\textbackslash{}begin\{align*\}
A\textasciicircum{}2 = diag \textbackslash{}left(\textbackslash{}lambda\_1\textasciicircum{}2, \textbackslash{}lambda\_2\textasciicircum{}2, \textbackslash{}lambda\_3\textasciicircum{}2, \textbackslash{}dotsc, \textbackslash{}lambda\_N\textasciicircum{}2 \textbackslash{}right)
\textbackslash{}end\{align*\}

\sphinxAtStartPar
\textbackslash{}begin\{align*\}
A\textasciicircum{}3 = diag \textbackslash{}left(\textbackslash{}lambda\_1\textasciicircum{}3, \textbackslash{}lambda\_2\textasciicircum{}3, \textbackslash{}lambda\_3\textasciicircum{}3, \textbackslash{}dotsc, \textbackslash{}lambda\_N\textasciicircum{}3 \textbackslash{}right)
\textbackslash{}end\{align*\}
\begin{equation*}
\begin{split}
\vdots
\end{split}
\end{equation*}
\sphinxAtStartPar
\textbackslash{}begin\{align*\}
A\textasciicircum{}j = diag \textbackslash{}left(\textbackslash{}lambda\_1\textasciicircum{}j, \textbackslash{}lambda\_2\textasciicircum{}j, \textbackslash{}lambda\_3\textasciicircum{}j, \textbackslash{}dotsc, \textbackslash{}lambda\_N\textasciicircum{}j \textbackslash{}right) , \textbackslash{}forall j \textbackslash{}in \textbackslash{}mathbb\{N\}
\textbackslash{}end\{align*\}
\begin{equation*}
\begin{split}
\vdots
\end{split}
\end{equation*}
\sphinxAtStartPar
so
\textbackslash{}begin\{align*\}
e\textasciicircum{}A \textbackslash{}doteq \textbackslash{}sum\_\{i=0\}\textasciicircum{}\{\textbackslash{}infty\} \textbackslash{}frac\{A\textasciicircum{}i\}\{i!\} = diag\textbackslash{}left(\textbackslash{}sum\_\{i=0\}\textasciicircum{}\{\textbackslash{}infty\} \textbackslash{}frac\{\textbackslash{}lambda\_1\textasciicircum{}i\}\{i!\}, \textbackslash{}sum\_\{i=0\}\textasciicircum{}\{\textbackslash{}infty\} \textbackslash{}frac\{\textbackslash{}lambda\_2\textasciicircum{}i\}\{i!\}, \textbackslash{}sum\_\{i=0\}\textasciicircum{}\{\textbackslash{}infty\} \textbackslash{}frac\{\textbackslash{}lambda\_3\textasciicircum{}i\}\{i!\}, \textbackslash{}dotsc, \textbackslash{}sum\_\{i=0\}\textasciicircum{}\{\textbackslash{}infty\} \textbackslash{}frac\{\textbackslash{}lambda\_N\textasciicircum{}i\}\{i!\}\textbackslash{}right) = diag \textbackslash{}left( e\textasciicircum{}\{\textbackslash{}lambda\_1\}, e\textasciicircum{}\{\textbackslash{}lambda\_2\}, e\textasciicircum{}\{\textbackslash{}lambda\_3\}, \textbackslash{}dotsc, e\textasciicircum{}\{\textbackslash{}lambda\_N\} \textbackslash{}right).
\textbackslash{}end\{align*\}

\sphinxAtStartPar
In the same way, if B is a diagonal by blocks matrix:
\begin{equation*}
\begin{split}
B =   
\left[ {\begin{array}{ccccc}
    B_1 & 0 & 0 & \dotsm & 0\\
    0 & B_2 & 0 & \dotsm & 0\\
    0 & 0 & B_3 & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & B_{N}\\
\end{array} } \right] 
  = diag(B_1, B_2, B_3, \dotsm, B_N),
\end{split}
\end{equation*}
\sphinxAtStartPar
then
\begin{equation*}
\begin{split}
e^B = diag(e^{B_1}, e^{B_2}, e^{B_3}, \dotsm, e^{B_N}).
\end{split}
\end{equation*}

\subsubsection{Exponential of a matrix of ones above the diagonal}
\label{\detokenize{cap3:exponential-of-a-matrix-of-ones-above-the-diagonal}}
\sphinxAtStartPar
If \(A = A_{N \times N} =   
\left[ {\begin{array}{ccccccc}
    0 & 1 &  &  &  &  & \\
     & 0 & 1 &  &  &  &\\
     &  & 0 & 1 &  &  &\\
     &  &  & 0 & 1 &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & 1 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] \),

\sphinxAtStartPar
one can calculate
\(A^2 = A \cdot A =  
\left[ {\begin{array}{ccccccc}
    0 & 1 &  &  &  &  & \\
     & 0 & 1 &  &  &  &\\
     &  & 0 & 1 &  &  &\\
     &  &  & 0 & 1 &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & 1 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right]  \cdot 
\left[ {\begin{array}{ccccccc}
    0 & 1 &  &  &  &  & \\
     & 0 & 1 &  &  &  &\\
     &  & 0 & 1 &  &  &\\
     &  &  & 0 & 1 &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & 1 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right]  =   
\left[ {\begin{array}{ccccccc}
    0 & 0 & 1 &  &  &  & \\
     & 0 & 0 & 1 &  &  &\\
     &  & 0 & 0 & 1 &  &\\
     &  &  & 0 & 0 & \ddots &\\
     &  &  &  & 0 & \ddots & 1 \\
     &  &  &  &  & \ddots & 0 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] \),
\begin{equation*}
\begin{split}
    A^3 = A \cdot A^2 = \left[ {\begin{array}{ccccccc}
    0 & 1 &  &  &  &  & \\
     & 0 & 1 &  &  &  &\\
     &  & 0 & 1 &  &  &\\
     &  &  & 0 & 1 &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & 1 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccccc}
    0 & 0 & 1 &  &  &  & \\
     & 0 & 0 & 1 &  &  &\\
     &  & 0 & 0 & 1 &  &\\
     &  &  & 0 & 0 & \ddots &\\
     &  &  &  & 0 & \ddots & 1 \\
     &  &  &  &  & \ddots & 0 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] = \left[ {\begin{array}{ccccccc}
    0 & 0 & 0 & 1 &  &  & \\
     & 0 & 0 & 0 & 1 &  &\\
     &  & 0 & 0 & 0 & \ddots &\\
     &  &  & 0 & 0 & \ddots & 1\\
     &  &  &  & 0 & \ddots & 0 \\
     &  &  &  &  & \ddots & 0 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right],
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    \vdots
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    A^{N-2} = \left[ {\begin{array}{ccccccc}
     &  &  &  & 0 & 1 & 0\\
     &  &  &  &  & 0 & 1 \\
     &  &  &  &  &  & 0 \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
\end{array} } \right],
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    A^{N-1} = \left[ {\begin{array}{ccccccc}
     &  &  &  &  &  & 1\\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
\end{array} } \right],
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    A^{N} = 0.
\end{split}
\end{equation*}
\sphinxAtStartPar
And then, with \(t \in \mathbb{R}\)
\textbackslash{}begin\{align*\}
e\textasciicircum{}\{tA\} \textbackslash{}doteq \textbackslash{}sum\_\{i=0\}\textasciicircum{}\{\textbackslash{}infty\} \textbackslash{}frac\{tA\textasciicircum{}i\}\{i!\} = Id + tA + \textbackslash{}frac\{t\textasciicircum{}2 A\textasciicircum{}2\}\{2\} + \textbackslash{}frac\{t\textasciicircum{}3 A\textasciicircum{}3\}\{6\} + \textbackslash{}dotsc + \textbackslash{}frac\{t\textasciicircum{}\{N\sphinxhyphen{}2\} A\textasciicircum{}\{N\sphinxhyphen{}2\}\}\{(N\sphinxhyphen{}2)!\} + \textbackslash{}frac\{t\textasciicircum{}\{N\sphinxhyphen{}1\} A\textasciicircum{}\{N\sphinxhyphen{}1\}\}\{(N\sphinxhyphen{}1)!\} + 0 + 0 + \textbackslash{}dotsc + 0
\textbackslash{}end\{align*\}
\begin{equation*}
\begin{split}
    = \left[ {\begin{array}{ccccccc}
    1 &  &  &  &  &  & \\
     & 1 &  &  &  &  &\\
     &  & 1 &  &  &  &\\
     &  &  & 1 &  &  &\\
     &  &  &  & 1 &  &\\
     &  &  &  &  & \ddots &\\
     &  &  &  &  &  & 1 \\
\end{array} } \right] + \left[ {\begin{array}{ccccccc}
    0 & t &  &  &  &  & \\
     & 0 & t &  &  &  &\\
     &  & 0 & t &  &  &\\
     &  &  & 0 & t &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & t \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] + \left[ {\begin{array}{ccccccc}
    0 & 0 & \frac{t^2}{2} &  &  &  & \\
     & 0 & 0 & \frac{t^2}{2} &  &  &\\
     &  & 0 & 0 & \frac{t^2}{2} &  &\\
     &  &  & 0 & 0 & \ddots &\\
     &  &  &  & 0 & \ddots & \frac{t^2}{2} \\
     &  &  &  &  & \ddots & 0 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] + \dotsc + \left[ {\begin{array}{ccccccc}
     &  &  &  &  &  & \frac{t^{N-1}}{(N-1)!}\\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
\end{array} } \right]
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = \left[ {\begin{array}{ccccccc}
    1 & t & \frac{t^2}{2} & \frac{t^3}{3!} & \frac{t^4}{4!} & \dotsc & \frac{t^{N-1}}{(N-1)!}\\
     & 1 & t & \frac{t^2}{2} & \frac{t^3}{3!} & \ddots & \vdots \\
     &  & 1 & t & \frac{t^2}{2} & \ddots & \frac{t^4}{4!}\\
     &  &  & 1 & t & \ddots & \frac{t^3}{3!}\\
     &  &  &  & 1 & \ddots & \frac{t^2}{2} \\
     &  &  &  &  & \ddots & t \\
     &  &  &  &  &  & 1 \\
\end{array} } \right].
\end{split}
\end{equation*}

\subsubsection{Exponential of a Jordan block}
\label{\detokenize{cap3:exponential-of-a-jordan-block}}
\sphinxAtStartPar
\(\textbf{Proposition:}\) \(A_1, A_2 \in \mathscr{M}_{N \times N}(\mathbb{C})\). If \(A_1 \cdot A_2 = A_2 \cdot A_1\), then \(e^{A_1+A_2} = e^{A_1} \cdot e^{A_2}\).

\sphinxAtStartPar
A Jordan block is of the form:
\$\(
J = \left[ {\begin{array}{ccccc}
    \lambda_i & 1 & 0 & \dotsm & 0\\
    0 & \lambda_i & 1 & \dotsm & 0\\
    0 & 0 & \lambda_i & \ddots & 0\\
    \vdots & \vdots & \vdots & \ddots & 1\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right] = \left[ {\begin{array}{ccccc}
    \lambda_i & 0 & 0 & \dotsm & 0\\
    0 & \lambda_i & 0 & \dotsm & 0\\
    0 & 0 & \lambda_i & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right] + \left[ {\begin{array}{ccccc}
    0 & 1 &  &  & \\
     & 0 & 1 &  &\\
     &  & 0 & \ddots &\\
     &  &  & \ddots & 1\\
     &  &  &  & 0\\
\end{array} } \right] = D + N,
\)\$

\sphinxAtStartPar
and
\$\(
\left[ {\begin{array}{ccccc}
    \lambda_i & 0 & 0 & \dotsm & 0\\
    0 & \lambda_i & 0 & \dotsm & 0\\
    0 & 0 & \lambda_i & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccc}
    0 & 1 &  &  & \\
     & 0 & 1 &  &\\
     &  & 0 & \ddots &\\
     &  &  & \ddots & 1\\
     &  &  &  & 0\\
\end{array} } \right] = \left[ {\begin{array}{ccccc}
    0 & \lambda_i &  &  & \\
     & 0 & \lambda_i &  &\\
     &  & 0 & \ddots &\\
     &  &  & \ddots & \lambda_i\\
     &  &  &  & 0\\
\end{array} } \right] = \left[ {\begin{array}{ccccc}
    0 & 1 &  &  & \\
     & 0 & 1 &  &\\
     &  & 0 & \ddots &\\
     &  &  & \ddots & 1\\
     &  &  &  & 0\\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccc}
    \lambda_i & 0 & 0 & \dotsm & 0\\
    0 & \lambda_i & 0 & \dotsm & 0\\
    0 & 0 & \lambda_i & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right],
\)\$

\sphinxAtStartPar
so
\$\(
    e^{tJ} = e^{tD+tN} = e^{tD} \cdot e^{tN} = \left[ {\begin{array}{ccccc}
    e^{t \lambda_i} & 0 & 0 & \dotsm & 0\\
    0 & e^{t \lambda_i} & 0 & \dotsm & 0\\
    0 & 0 & e^{t \lambda_i} & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & e^{t \lambda_i}\\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccc}
    1 & t & \frac{t^2}{2} & \dotsc & \frac{t^{N-1}}{(N-1)!}\\
     & 1 & t & \ddots & \vdots\\
     &  & 1 & \ddots & \frac{t^2}{2} \\
     &  &  & \ddots & t \\
     &  &  &  & 1 \\
\end{array} } \right] = \left[ {\begin{array}{ccccc}
    e^{t \lambda_i} & e^{t \lambda_i}t & \frac{e^{t \lambda_i} t^2}{2} & \dotsc & \frac{e^{t \lambda_i} t^{N-1}}{(N-1)!}\\
     & e^{t \lambda_i} & e^{t \lambda_i} t & \ddots & \vdots\\
     &  & e^{t \lambda_i} & \ddots & \frac{e^{t \lambda_i} t^2}{2} \\
     &  &  & \ddots & e^{t \lambda_i} t \\
     &  &  &  & e^{t \lambda_i} \\
\end{array} } \right], t \in \mathbb{R}.
\)\$


\subsubsection{Exponential of any matrix}
\label{\detokenize{cap3:exponential-of-any-matrix}}
\sphinxAtStartPar
\(\textbf{Proposition: } \forall A \in \mathscr{M}_{N \times N}(\mathbb{C}), \exists M \in \mathscr{M}_{N \times N}(\mathbb{C})\) invertible, such that \(A = MJM^{-1}\), with \( J = \left[ {\begin{array}{ccccc}
    J_1 & 0 & 0 & \dotsm & 0\\
    0 & J_2 & 0 & \dotsm & 0\\
    0 & 0 & J_3 & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & J_{N}\\
\end{array} } \right] \) and each \(J_i\), \(i = 1, 2, 3, \dotsc, N\) being a Jordan block, i.e., \(J_i = \left[ {\begin{array}{ccccc}
    \lambda_i & 0 & 0 & \dotsm & 0\\
    0 & \lambda_i & 0 & \dotsm & 0\\
    0 & 0 & \lambda_i & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right]\) for some \(\lambda_i \in \mathbb{C}\) .

\sphinxAtStartPar
Note that
\textbackslash{}begin\{align\}
(MJM\textasciicircum{}\{\sphinxhyphen{}1\})\textasciicircum{}k = MJM\textasciicircum{}\{\sphinxhyphen{}1\}MJM\textasciicircum{}\{\sphinxhyphen{}1\}MJM\textasciicircum{}\{\sphinxhyphen{}1\} \textbackslash{}dotsc MJM\textasciicircum{}\{\sphinxhyphen{}1\} = MJIJIJM\textasciicircum{}\{\sphinxhyphen{}1\} \textbackslash{}dotsc MJM\textasciicircum{}\{\sphinxhyphen{}1\} = MJJJ \textbackslash{}dotsc JM\textasciicircum{}\{\sphinxhyphen{}1\} = MJ\textasciicircum{}kM\textasciicircum{}\{\sphinxhyphen{}1\}.
\textbackslash{}end\{align\}

\sphinxAtStartPar
Because of the formula of the series that defines the expansion, it implicates in \(e^{MJM^{-1}} = M e^J M^{-1}\).

\sphinxAtStartPar
And then, using the same notation from the last proposition,
\textbackslash{}begin\{align*\}
e\textasciicircum{}\{tA\} = e\textasciicircum{}\{tMJM\textasciicircum{}\{\sphinxhyphen{}1\}\} = e\textasciicircum{}\{MtJM\textasciicircum{}\{\sphinxhyphen{}1\}\} = Me\textasciicircum{}\{tJ\}M\textasciicircum{}\{\sphinxhyphen{}1\} = M \textbackslash{}left{[} \{ \textbackslash{}begin\{array\}\{ccccc\}
e\textasciicircum{}\{tJ\_1\} \& 0 \& 0 \& \textbackslash{}dotsm \& 0\textbackslash{}
0 \& e\textasciicircum{}\{tJ\_2\} \& 0 \& \textbackslash{}dotsm \& 0\textbackslash{}
0 \& 0 \& e\textasciicircum{}\{tJ\_3\} \& \textbackslash{}dotsm \& 0\textbackslash{}
\textbackslash{}vdots \& \textbackslash{}vdots \& \textbackslash{}vdots \& \textbackslash{}ddots \& \textbackslash{}vdots\textbackslash{}
0 \& 0 \& 0 \& \textbackslash{}dotsm \& e\textasciicircum{}\{tJ\_\{N\}\}\textbackslash{}
\textbackslash{}end\{array\} \} \textbackslash{}right{]} M\textasciicircum{}\{\sphinxhyphen{}1\}, t \textbackslash{}in \textbackslash{}mathbb\{R\},
\textbackslash{}end\{align*\}
with each block as the section above indicates.


\subsection{Linear problem}
\label{\detokenize{cap3:linear-problem}}
\sphinxAtStartPar
The linear problem is, following with the used notation:
\textbackslash{}begin\{cases\}
y’(t) + \textbackslash{}lambda y(t) = g(y(t), t), t \textbackslash{}in (t\_0, T) \textbackslash{}
y(t\_0) = y\_0
\textbackslash{}text\{,\}
\textbackslash{}end\{cases\}
the one with \(g \equiv 0.\)

\sphinxAtStartPar
So, generaly, it is of the form:
\textbackslash{}begin\{cases\}
y’(t) = A y(t), t \textbackslash{}in (t\_0, T) \textbackslash{}
y(t\_0) = y\_0
\textbackslash{}text\{,\}
\textbackslash{}end\{cases\}
with \(A \in \mathscr{M}_{N \times N}(\mathbb{C}), N \in \mathbb{N}\)  (remembering that a matrix \(1 \times 1\) is simply a number).

\sphinxAtStartPar
Because \(A y(t)\) is a \(C^1\) function in \(y\), continuous in \(t\) and \(t \in (t_0, T)\), a limited interval, by the existence and uniqueness theorem, there is a single solution of the problem.

\sphinxAtStartPar
Since
\textbackslash{}begin\{equation*\}
\textbackslash{}frac\{d\}\{dt\}y\_0e\textasciicircum{}\{A(t\sphinxhyphen{}t\_0)\} \textbackslash{}doteq \textbackslash{}lim\_\{h\textbackslash{}to0\} \textbackslash{}frac\{y\_0e\textasciicircum{}\{A(t\sphinxhyphen{}t\_0+h)\}\sphinxhyphen{}y\_0e\textasciicircum{}\{A(t\sphinxhyphen{}t\_0)\}\}\{h\} = y\_0e\textasciicircum{}\{(t\sphinxhyphen{}t\_0)A\}\textbackslash{}lim\_\{h\textbackslash{}to0\} \textbackslash{}frac\{e\textasciicircum{}\{Ah\}\sphinxhyphen{}I\}\{h\} = y\_0e\textasciicircum{}\{(t\sphinxhyphen{}t\_0)A\}\textbackslash{}lim\_\{h\textbackslash{}to0\} \textbackslash{}frac\{Ae\textasciicircum{}\{Ah\}\}\{1\} = y\_0e\textasciicircum{}\{(t\sphinxhyphen{}t\_0)A\} \textbackslash{}frac\{Ae\textasciicircum{}\{A0\}\}\{1\} = y\_0e\textasciicircum{}\{(t\sphinxhyphen{}t\_0)A\} A I = A y\_0e\textasciicircum{}\{(t\sphinxhyphen{}t\_0)A\}
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
using L’Hôpital’s rule on the second equality and noting that \(A(t-t_0+h) = A(t-t_0)+Ah\) and \(A(t-t_0) \cdot Ah = (t-t_0)hAA = Ah \cdot A(t-t_0)\), so it was possible to apply the last proposition and make \(e^{A(t-t_0+h)} = e^{A(t-t_0)} \cdot e^{Ah}\),

\sphinxAtStartPar
taking
\textbackslash{}begin\{equation*\}
y(t) = y\_0e\textasciicircum{}\{A(t\sphinxhyphen{}t\_0)\},
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
y’(t) = A y\_0 e\textasciicircum{}\{(t\sphinxhyphen{}t\_0)A\} = A y(t) \textbackslash{}text\{ and \} y(t\_0) = y\_0 e\textasciicircum{}\{(t\_0\sphinxhyphen{}t\_0)A\} = y\_0 I = y\_0.
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
So, the solution for the general linear problem is \(y(t)=y_0 e^{A(t-t_0)}\).

\sphinxAtStartPar
All information about matrix exponential is from {[}4{]}.


\subsection{General problem}
\label{\detokenize{cap3:general-problem}}
\sphinxAtStartPar
Returning to the general case

\sphinxAtStartPar
\textbackslash{}begin\{cases\}
y’(t) + \textbackslash{}lambda y(t) = g(y(t), t), t \textbackslash{}in (t\_0, T) \textbackslash{}
y(t\_0) = y\_0
\textbackslash{}text\{,\}
\textbackslash{}end\{cases\}

\sphinxAtStartPar
there is the variation of constants formula:
\textbackslash{}begin\{equation*\}
y(t) = e\textasciicircum{}\{\sphinxhyphen{}t \textbackslash{}lambda\}y\_0 + \textbackslash{}int\_\{t\_0\}\textasciicircum{}t e\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}lambda(t\sphinxhyphen{}\textbackslash{}tau)\} g(y(\textbackslash{}tau), \textbackslash{}tau) d\textbackslash{}tau.
\textbackslash{}end\{equation*\}
This well known implicit function, gives a solution of the problem.

\sphinxAtStartPar
If the integral part can be solved, there is a explicit solution, and if the problem satisfies the hypotesis of the Piccard problem, being Lipschitz in \(t\), this is the only solution.

\sphinxAtStartPar
This formula is the basis of all the exponential methods.


\subsection{\protect\(\phi\protect\) functions}
\label{\detokenize{cap3:phi-functions}}
\sphinxAtStartPar
Before introducing exponential methods, it is useful to present the \(\phi\) functions.

\sphinxAtStartPar
They are \(\mathbb{C} \rightarrow \mathbb{C}\) functions defined as:

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
\textbackslash{}phi\_0 (z) = e\textasciicircum{}z;
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
\textbackslash{}phi\_n (z) = \textbackslash{}int\_\{0\}\textasciicircum{}\{1\} e\textasciicircum{}\{(1\sphinxhyphen{}\textbackslash{}tau)z\} \textbackslash{}frac\{\textbackslash{}tau\textasciicircum{}\{n\sphinxhyphen{}1\}\}\{(n\sphinxhyphen{}1)!\} ,d\textbackslash{}tau, n \textbackslash{}geq 1.
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
By integration by parts,

\sphinxAtStartPar
\textbackslash{}begin\{gather*\}
\textbackslash{}phi\_\{n+1\} (z) = \textbackslash{}int\_\{0\}\textasciicircum{}\{1\} e\textasciicircum{}\{(1\sphinxhyphen{}\textbackslash{}tau)z\} \textbackslash{}frac\{\textbackslash{}tau\textasciicircum{}n\}\{n!\} ,d\textbackslash{}tau \textbackslash{}
= \sphinxhyphen{} \textbackslash{}frac\{e\textasciicircum{}\{(1\sphinxhyphen{}1)z\}\}\{z\} \textbackslash{}frac\{1\textasciicircum{}n\}\{n!\} + \textbackslash{}frac\{e\textasciicircum{}\{(1\sphinxhyphen{}0)z\}\}\{z\} \textbackslash{}frac\{0\textasciicircum{}n\}\{l!\} \sphinxhyphen{} \textbackslash{}int\_\{0\}\textasciicircum{}\{1\} \sphinxhyphen{}\textbackslash{}frac\{e\textasciicircum{}\{(1\sphinxhyphen{}\textbackslash{}tau)z\}\}\{z\} \textbackslash{}frac\{\textbackslash{}tau\textasciicircum{}\{n\sphinxhyphen{}1\}\}\{(n\sphinxhyphen{}1)!\} ,d\textbackslash{}tau \textbackslash{}
= \sphinxhyphen{} \textbackslash{}frac\{1\}\{n!z\} + \textbackslash{}frac\{1\}\{z\}\textbackslash{}int\_\{0\}\textasciicircum{}\{1\} e\textasciicircum{}\{(1\sphinxhyphen{}\textbackslash{}tau)z\} \textbackslash{}frac\{\textbackslash{}tau\textasciicircum{}\{n\sphinxhyphen{}1\}\}\{(n\sphinxhyphen{}1)!\} ,d\textbackslash{}tau.
\textbackslash{}end\{gather*\}

\sphinxAtStartPar
Since
\textbackslash{}begin\{align*\}
\textbackslash{}phi\_n(0) = \textbackslash{}int\_\{0\}\textasciicircum{}\{1\} e\textasciicircum{}0 \textbackslash{}frac\{\textbackslash{}tau\textasciicircum{}\{n\sphinxhyphen{}1\}\}\{(n\sphinxhyphen{}1)!\} ,d\textbackslash{}tau = \textbackslash{}int\_\{0\}\textasciicircum{}\{1\} \textbackslash{}frac\{\textbackslash{}tau\textasciicircum{}\{n\sphinxhyphen{}1\}\}\{(n\sphinxhyphen{}1)!\} ,d\textbackslash{}tau = \textbackslash{}frac\{1\textasciicircum{}n\}\{n!\} \sphinxhyphen{} 0 = \textbackslash{}frac\{1\}\{n!\},
\textbackslash{}end\{align*\}

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
\textbackslash{}phi\_\{n+1\}(z) = \textbackslash{}frac\{\textbackslash{}phi\_n(z) \sphinxhyphen{} \textbackslash{}phi\_n(0)\}\{z\}, \textbackslash{}textbf\{the recursive characterization\}.
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
By the properties of integral {[}5{]}, if \(h \in \mathbb{R}^*, t_k \in \mathbb{R}, t_k+h = t_{k+1},\)

\sphinxAtStartPar
\textbackslash{}begin\{align*\}
\textbackslash{}phi\_n (z) = \textbackslash{}int\_\{0\}\textasciicircum{}\{1\} e\textasciicircum{}\{(1\sphinxhyphen{}\textbackslash{}tau)z\} \textbackslash{}frac\{\textbackslash{}tau\textasciicircum{}\{n\sphinxhyphen{}1\}\}\{(n\sphinxhyphen{}1)!\} ,d\textbackslash{}tau \textbackslash{}
= \textbackslash{}frac\{1\}\{h\}\textbackslash{}int\_\{0\}\textasciicircum{}\{h\} e\textasciicircum{}\{\textbackslash{}frac\{(h\sphinxhyphen{}\textbackslash{}tau)z\}\{h\}\} \textbackslash{}frac\{\textbackslash{}tau\textasciicircum{}\{n\sphinxhyphen{}1\}\}\{h\textasciicircum{}\{n\sphinxhyphen{}1\}(n\sphinxhyphen{}1)!\} ,d\textbackslash{}tau \textbackslash{}
= \textbackslash{}frac\{1\}\{h\}\textbackslash{}int\_\{t\_k\}\textasciicircum{}\{t\_k + h\} e\textasciicircum{}\{\textbackslash{}frac\{(h\sphinxhyphen{}\textbackslash{}tau+t\_k)z\}\{h\}\} \textbackslash{}frac\{(\textbackslash{}tau \sphinxhyphen{} t\_k)\textasciicircum{}\{n\sphinxhyphen{}1\}\}\{h\textasciicircum{}\{n\sphinxhyphen{}1\}(n\sphinxhyphen{}1)!\} ,d\textbackslash{}tau,
\textbackslash{}end\{align*\}

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
\textbackslash{}phi\_n (z) = \textbackslash{}frac\{1\}\{h\textasciicircum{}l\}\textbackslash{}int\_\{t\_k\}\textasciicircum{}\{t\_\{k+1\}\} e\textasciicircum{}\{\textbackslash{}frac\{1\}\{h\}(t\_\{k+1\}\sphinxhyphen{}\textbackslash{}tau)z\} \textbackslash{}frac\{(\textbackslash{}tau \sphinxhyphen{} t\_k)\textasciicircum{}\{n\sphinxhyphen{}1\}\}\{(n\sphinxhyphen{}1)!\} ,d\textbackslash{}tau.
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
Information from {[}1{]}.

\sphinxstepscope


\section{Chapter 2: Exponential methods}
\label{\detokenize{cap4:chapter-2-exponential-methods}}\label{\detokenize{cap4::doc}}
\sphinxAtStartPar
In this chapter, exponential methods are introduced, with further analysis of some of them, being tested and compared to more classical equivalents.

\sphinxAtStartPar
All the functions coded are in the following environment.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
from math import *
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
from scipy.linalg import expm
from scipy import linalg

stab\PYGZus{}lim = 1000.0

def classic\PYGZus{}euler(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    for i in range(1, n):
        x[:,i] = x[:,i\PYGZhy{}1] + h*(np.matmul(\PYGZhy{}A,x[:,i\PYGZhy{}1]) + g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
        if np.any(x[:,i].real \PYGZgt{} stab\PYGZus{}lim):
            x[:,i] = np.nan
    return x

def exponential\PYGZus{}euler(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0] = x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    for i in range(1, n):
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
    return x

def calculate\PYGZus{}hphi1(h, A):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, np.matrix) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    hphi1 = np.matmul(1\PYGZhy{}expm(\PYGZhy{}h*A), linalg.inv(A))
    return hphi1

def calculate\PYGZus{}hphi2(h, A, hphi1):
    \PYGZsh{}IT IS NOT H2PHI2
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, np.matrix, np.matrix) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    hphi2 = np.matmul(1\PYGZhy{}hphi1/h, linalg.inv(A))
    return hphi2

def etd2(t0, tf, n, x0, A, g, derivate\PYGZus{}of\PYGZus{}g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0] = x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    hphi2 = calculate\PYGZus{}hphi2(h, A, hphi1)
    for i in range(1, n):
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t)) + h*np.matmul(hphi2,derivate\PYGZus{}of\PYGZus{}g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
    return x

def etd2rk\PYGZus{}cox\PYGZus{}and\PYGZus{}matthews(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    hphi2 = calculate\PYGZus{}hphi2(h, A, hphi1)
    for i in range(1, n):
        a = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t))
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t)) + np.matmul(hphi2,g(a, t0 + i*h)\PYGZhy{}g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
    return x

def etd2rk\PYGZus{}cox\PYGZus{}and\PYGZus{}matthews\PYGZus{}midpoint\PYGZus{}rule(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    exponential\PYGZus{}matrix\PYGZus{}2 = expm(\PYGZhy{}h/2*A)
    h\PYGZus{}2phi1\PYGZus{}2 = calculate\PYGZus{}hphi1(h/2, A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    hphi2 = calculate\PYGZus{}hphi2(h, A, hphi1)
    for i in range(1, n):
        b = np.matmul(exponential\PYGZus{}matrix\PYGZus{}2, x[:,i\PYGZhy{}1]) + np.matmul(h\PYGZus{}2phi1\PYGZus{}2,g(x[:,i\PYGZhy{}1],t))
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t)) + 2*np.matmul(hphi2,g(b, t + h/2)\PYGZhy{}g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
    return x

def etd2rk\PYGZus{}trapezoidal\PYGZus{}rule(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    for i in range(1, n):
        a = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t))
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + .5 * h * (np.matmul(exponential\PYGZus{}matrix, g(x[:,i\PYGZhy{}1],t)) + g(a, t0 + i*h))
        t = t0 + i*h
    return x

def etd2rk\PYGZus{}midpoint\PYGZus{}rule(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function, np.matrix) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    exponential\PYGZus{}matrix\PYGZus{}2 = expm(\PYGZhy{}h/2*A)
    h\PYGZus{}2phi1\PYGZus{}2 = calculate\PYGZus{}hphi1(h/2, A)
    for i in range(1, n):
        b = np.matmul(exponential\PYGZus{}matrix\PYGZus{}2, x[:,i\PYGZhy{}1]) + np.matmul(h\PYGZus{}2phi1\PYGZus{}2,g(x[:,i\PYGZhy{}1],t))
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + h * np.matmul(exponential\PYGZus{}matrix\PYGZus{}2, g(b, t+h/2))
        t = t0 + i*h
    return x

def vectorize\PYGZus{}sol(t0, t1, n, sol):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    (float, float, int, function) \PYGZhy{}\PYGZgt{} np.vector
    n is the number of steps
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    x = np.zeros(n, dtype=np.complex\PYGZus{})
    h = (t1\PYGZhy{}t0)/n
    for i in range(n):
        x[i] = sol(t0+i*h)
    return x

def error\PYGZus{}2(x\PYGZus{}approx, x\PYGZus{}exact):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.vector, np.vector) \PYGZhy{}\PYGZgt{} float \PYGZsq{}\PYGZsq{}\PYGZsq{}
    \PYGZsh{}make sure that x\PYGZus{}approx and x\PYGZus{}exact have the same lenght
    v = (x\PYGZus{}approx \PYGZhy{} x\PYGZus{}exact)*(x\PYGZus{}approx \PYGZhy{} x\PYGZus{}exact).conjugate()
    \PYGZsh{}\PYGZca{}certainly pure real
    return np.sqrt(float(np.sum(v)/x\PYGZus{}approx.size)) \PYGZsh{}normalized

def error\PYGZus{}sup(x\PYGZus{}approx, x\PYGZus{}exact):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.vector, np.vector) \PYGZhy{}\PYGZgt{} float \PYGZsq{}\PYGZsq{}\PYGZsq{}
    \PYGZsh{}make sure that x\PYGZus{}approx and x\PYGZus{}exact have the same lenght
    v = abs(x\PYGZus{}approx \PYGZhy{} x\PYGZus{}exact)
    return np.amax(v)

def g( x, t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.array, float) \PYGZhy{}\PYGZgt{} float
        (x, t) \PYGZhy{}\PYGZgt{} g(x, t)
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    g = np.array([np.sin(t)])
    return g

def g\PYGZus{}linear\PYGZus{}deprec( x, t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
        (x, t) \PYGZhy{}\PYGZgt{} g(x, t)
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    g = 0
    return g

def g\PYGZus{}linear( x, t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.array, float) \PYGZhy{}\PYGZgt{} np.array
        (x, t) \PYGZhy{}\PYGZgt{} g(x, t)
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    g = np.zeros(x.size)
    return g

def g\PYGZus{}cm1 (x, t):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.array, float) \PYGZhy{}\PYGZgt{} np.array
        (x, t) \PYGZhy{}\PYGZgt{} g(x, t)
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    lamb = .5
    c = 100
    r\PYGZus{}2 = x[0]**2 + x[1]**2
    g = np.array([(lamb*x[1]\PYGZhy{}c*x[0])*r\PYGZus{}2, \PYGZhy{}(lamb*x[0]+c*x[1])*r\PYGZus{}2])
    return g

def sol( t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    lmba = 100
    sol = np.exp(\PYGZhy{}lmba*t)+(np.exp(\PYGZhy{}lmba*t)+lmba*np.sin(t)\PYGZhy{}np.cos(t))/(1+lmba*lmba)
    return sol

def sol\PYGZus{}100\PYGZus{}linear( t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    sol = exp(\PYGZhy{}100*t) \PYGZsh{}u0=1
    return sol

def sol\PYGZus{}1j\PYGZus{}linear( t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    return np.exp(1j*t)

def sol\PYGZus{}non\PYGZus{}linear\PYGZus{}sin( t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    sol = 2\PYGZhy{}cos(t) \PYGZsh{}u0=1
    return sol

def errors\PYGZus{}array(n0, nf, method, t0, tf, x0, lmba, g, sol, vectorize\PYGZus{}sol, error):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  This function will RETURN 2 arrays.
  The first one has the errors of the approximations given by the method with
  number of steps n = n0, n0+1, n0+2, ..., nf\PYGZhy{}1.
  The second is [n0, n0+1, n0+2, ..., nf\PYGZhy{}1]

  RECEIVES:
  n0 is the first number of steps. (int)
  nf is the last one plus 1. (int)
  method have arguments (t0, tf, n, x0, lmba, g) and return a
  np.vector of length n (0, 1, 2, ..., n\PYGZhy{}1), n is the number of steps. (function)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  x0 is the initial value of the Cauchy problem. (float)
  lmbda is the coefficient os the linear part of the ploblem. (float)
  g is a function (float, float) \PYGZhy{}\PYGZgt{} (float). (function)
  sol is a function (float) \PYGZhy{}\PYGZgt{} (float). (function)
  vectorize\PYGZus{}sol is a function that \PYGZdq{}transforms sol in a vector\PYGZdq{} (function)
  (float, float, int, function) \PYGZhy{}\PYGZgt{} (np.array)
  (t0, tf, n, sol) \PYGZhy{}\PYGZgt{} np.array([sol[t0], sol[t0+h], sol[t0+2h], ..., sol[tf\PYGZhy{}1]])
  error is a function (np.array, np.array) \PYGZhy{}\PYGZgt{} (float) (function)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  v = np.zeros(nf\PYGZhy{}n0)
  domain = np.zeros(nf\PYGZhy{}n0)
  for n in range(n0, nf):
    domain[n\PYGZhy{}n0] = n
    m = method(t0, tf, n, x0, lmba, g)
    exact = vectorize\PYGZus{}sol(t0, tf, n, sol)
    if np.max(np.abs(m))\PYGZgt{}1000:
        v[n\PYGZhy{}n0]=np.nan
    else:
        v[n\PYGZhy{}n0] = error(m, exact)
  return v, domain

def graphic\PYGZus{}2D(domain, matrix, names, labelx, labely, title, key1, key2):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  domain is a list of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines. (list)
  matrix is a list of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines \PYGZhy{} same length that domain. (list)
  names is a list of the labels for the graphs, must have the same length that
  the number of lines in matrix. (list of Strings)
  labelx is the name of the x coordinate. (String)
  labely is the name of the y coordinate. (String)
  title is the title of the graph. (String)
  key1 is a boolean that indicates if the last graph must be black. (bool)
  key2 is a boolean that indicates if it should use the log scale. (bool)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  fig, ax = plt.subplots()

  colors = [\PYGZsq{}blue\PYGZsq{}, \PYGZsq{}green\PYGZsq{}, \PYGZsq{}red\PYGZsq{}, \PYGZsq{}cyan\PYGZsq{}, \PYGZsq{}magenta\PYGZsq{}, \PYGZsq{}yellow\PYGZsq{}]
  for i in range(len(names)\PYGZhy{}1):
    ax.plot(domain[i], matrix[i], color=colors[i], label=names[i])
  if key1:
    ax.plot(domain[len(names)\PYGZhy{}1], matrix[len(names)\PYGZhy{}1], color=\PYGZsq{}black\PYGZsq{}, label=names[len(names)\PYGZhy{}1])
  else:
    ax.plot(domain[len(names)\PYGZhy{}1], matrix[len(names)\PYGZhy{}1], color=colors[len(names)\PYGZhy{}1], label=names[len(names)\PYGZhy{}1])
  if key2:
    plt.yscale(\PYGZsq{}log\PYGZsq{})
  ax.legend()
  ax.set\PYGZus{}xlabel(labelx)
  ax.set\PYGZus{}ylabel(labely)
  ax.set\PYGZus{}title(title)
  return fig, ax

def graphic\PYGZus{}3D(domain, matrix1, matrix2, names, labelx, labely, labelz, title, key1, key2):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  domain is a list of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines. (list)
  matrix1 and matrix2 are lists of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines \PYGZhy{} same length that domain. (list)
  names is a list of the labels for the graphs, must have the same length that
  the number of lines in matrix. (list of Strings)
  labelx is the name of the x coordinate. (String)
  labely is the name of the y coordinate. (String)
  labelz is the name of the z coordinate. (String)
  title is the title of the graph. (String)
  key1 is a boolean that indicates if the last graph must be black. (bool)
  key2 is a boolean that indicates if it should use the log scale. (bool)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  fig = plt.figure()
  ax = plt.figure().add\PYGZus{}subplot(projection=\PYGZsq{}3d\PYGZsq{})

  colors = [\PYGZsq{}blue\PYGZsq{}, \PYGZsq{}green\PYGZsq{}, \PYGZsq{}red\PYGZsq{}, \PYGZsq{}cyan\PYGZsq{}, \PYGZsq{}magenta\PYGZsq{}, \PYGZsq{}yellow\PYGZsq{}]
  for i in range(len(names)\PYGZhy{}1):
    ax.plot(domain[i], matrix1[i], matrix2[i], color=colors[i], label=names[i])
  if key1:
    ax.plot(domain[len(names)\PYGZhy{}1], matrix1[len(names)\PYGZhy{}1], matrix2[len(names)\PYGZhy{}1], color=\PYGZsq{}black\PYGZsq{}, label=names[len(names)\PYGZhy{}1])
  else:
    ax.plot(domain[len(names)\PYGZhy{}1], matrix1[len(names)\PYGZhy{}1], matrix2[len(names)\PYGZhy{}1], color=colors[len(names)\PYGZhy{}1], label=names[len(names)\PYGZhy{}1])
  if key2:
    plt.yscale(\PYGZsq{}log\PYGZsq{})
  ax.legend()
  ax.set\PYGZus{}xlabel(labelx)
  ax.set\PYGZus{}ylabel(labely)
  ax.set\PYGZus{}zlabel(labelz)
  ax.set\PYGZus{}title(title)
  return fig, ax

def errors\PYGZus{}2x(n0, k, method, t0, tf, x0, lmba, g, sol, vectorize\PYGZus{}sol, error):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  This function will RETURN a np.array with the errors of the approximations given
  by the method with number of steps n = n0, 2*n0, 2**2*n0, ..., 2**(k\PYGZhy{}1)*n0.

  RECEIVES:
  n0 is the first number of steps. (int)
  k is the number of errors in the final array. (int)
  method have arguments (t0, tf, n, x0, lmba, g) and return a
  np.vector of length n (0, 1, 2, ..., n\PYGZhy{}1), n is the number of steps. (function)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  x0 is the initial value of the Cauchy problem. (float)
  lmbda is the coefficient os the linear part of the ploblem. (float)
  g is a function (float, float) \PYGZhy{}\PYGZgt{} (float). (function)
  sol is a function (float) \PYGZhy{}\PYGZgt{} (float). (function)
  vectorize\PYGZus{}sol is a function that \PYGZdq{}transforms sol in a vector\PYGZdq{} (function)
  (float, float, int, function) \PYGZhy{}\PYGZgt{} (np.array)
  (t0, tf, n, sol) \PYGZhy{}\PYGZgt{} np.array([sol[t0], sol[t0+h], sol[t0+2h], ..., sol[tf\PYGZhy{}1]])
  error is a function (np.array, np.array) \PYGZhy{}\PYGZgt{} (float) (function)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  v = np.zeros(k)
  for i in range(k):
    m = method(t0, tf, n0*2**i, x0, lmba, g)
    exact = vectorize\PYGZus{}sol(t0, tf, n0*2**i, sol)
    v[i] = error(m, exact)
  return v

def convergence\PYGZus{}table(errors\PYGZus{}2x, n0, k, t0, tf):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  RECEIVES:
  errors\PYGZus{}2x is a array with the errors of the approximations given
  by a method with number of steps n = n0, 2*n0, 2**2*n0, ..., 2**(k\PYGZhy{}1)*n0. (np.array)
  n0 is the first number of steps. (int)
  k is the number of errors in the final array. (int)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  n = n0
  print(n, (tf\PYGZhy{}t0)/n, errors\PYGZus{}2x[0], \PYGZdq{}\PYGZhy{}\PYGZdq{}, sep=\PYGZdq{} \PYGZam{} \PYGZdq{}, end=\PYGZdq{} \PYGZbs{}\PYGZbs{}\PYGZbs{}\PYGZbs{} \PYGZbs{}n\PYGZdq{})
  for k in range(1, 4):
      n = n0 * 2 ** k
      h = (tf\PYGZhy{}t0)/n
      q = errors\PYGZus{}2x[k\PYGZhy{}1]/errors\PYGZus{}2x[k] \PYGZsh{}q=erro(h)/erro(h)
      r = ((tf\PYGZhy{}t0)/(n/2))/((tf\PYGZhy{}t0)/n)
      print(n, h, errors\PYGZus{}2x[k], log(q,2)/log(r,2), sep=\PYGZdq{} \PYGZam{} \PYGZdq{}, end=\PYGZdq{} \PYGZbs{}\PYGZbs{}\PYGZbs{}\PYGZbs{} \PYGZbs{}n\PYGZdq{})
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}

\subsection{Exponential Euler method}
\label{\detokenize{cap4:exponential-euler-method}}
\sphinxAtStartPar
For
\textbackslash{}begin\{cases\}
y’(t) + \textbackslash{}lambda y(t) = g(y(t), t), t \textbackslash{}in (t\_0, T) \textbackslash{}
y(0) = y\_0
\textbackslash{}end\{cases\}

\sphinxAtStartPar
the domain is evenly discretized:

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
N \textbackslash{}in \textbackslash{}mathbb\{N\}; h = \textbackslash{}frac\{T\sphinxhyphen{}t\_0\}\{N\}; \textbackslash{}text\{Domain: \}\{t\_k = t\_0 + k h : k = 0, 1, …\}.
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
The discretization of the ODE takes the exact solution of the Cauchy problem, given by the variation of constants formula
\textbackslash{}begin\{equation*\}
y(t) = e\textasciicircum{}\{\sphinxhyphen{}(t\sphinxhyphen{}t\_0) \textbackslash{}lambda\}y\_0 + \textbackslash{}int\_\{t\_0\}\textasciicircum{}t {[}e\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}lambda(t\sphinxhyphen{}\textbackslash{}tau)\} g(y(\textbackslash{}tau), \textbackslash{}tau){]} d\textbackslash{}tau
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
and, by Taylor expansion on \(g\):

\sphinxAtStartPar
\(\tau \in (t_k, t_{k+1})\)
\textbackslash{}begin\{equation*\}
g(y(\textbackslash{}tau), \textbackslash{}tau) = g(y(t\_k), t\_k) + (\textbackslash{}tau \sphinxhyphen{} t\_k) \textbackslash{}frac\{dg\}\{dt\} (y(\textbackslash{}theta\_k), \textbackslash{}theta\_k)
\textbackslash{}end\{equation*\}
for a \(\theta_k \in (t_k, t_{k+1}),\)

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
y(t\_\{k+1\}) = e\textasciicircum{}\{\sphinxhyphen{}(t\_\{k+1\}\sphinxhyphen{}t\_k) \textbackslash{}lambda\}y(t\_k) + \textbackslash{}int\_\{t\_k\}\textasciicircum{}\{t\_\{k+1\}\} {[}e\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}lambda(t\_\{k+1\}\sphinxhyphen{}\textbackslash{}tau)\} g(y(\textbackslash{}tau), \textbackslash{}tau){]} d\textbackslash{}tau
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
= e\textasciicircum{}\{\sphinxhyphen{}h \textbackslash{}lambda\}y(t\_k) + \textbackslash{}int\_\{t\_k\}\textasciicircum{}\{t\_\{k+1\}\} \textbackslash{}left{[}e\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}lambda(t\_\{k+1\}\sphinxhyphen{}\textbackslash{}tau)\} \textbackslash{}left( g(y(t\_k), t\_k) + (\textbackslash{}tau \sphinxhyphen{} t\_k) \textbackslash{}frac\{dg\}\{dt\} (y(\textbackslash{}theta\_k), \textbackslash{}theta\_k)\textbackslash{}right)\textbackslash{}right{]} d\textbackslash{}tau
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
= e\textasciicircum{}\{\sphinxhyphen{}h \textbackslash{}lambda\}y(t\_k) + g(y(t\_k), t\_k) \textbackslash{}int\_\{t\_k\}\textasciicircum{}\{t\_\{k+1\}\} e\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}lambda(t\_\{k+1\}\sphinxhyphen{}\textbackslash{}tau)\} d\textbackslash{}tau + \textbackslash{}frac\{dg\}\{dt\} (y(\textbackslash{}theta\_k), \textbackslash{}theta\_k) \textbackslash{}int\_\{t\_k\}\textasciicircum{}\{t\_\{k+1\}\} (\textbackslash{}tau \sphinxhyphen{} t\_k) e\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}lambda(t\_\{k+1\}\sphinxhyphen{}\textbackslash{}tau)\} d\textbackslash{}tau.
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
Since

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
\textbackslash{}int\_\{t\_k\}\textasciicircum{}\{t\_\{k+1\}\} e\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}lambda(t\_\{k+1\}\sphinxhyphen{}\textbackslash{}tau)\} d\textbackslash{}tau = h\textbackslash{}phi\_1(\sphinxhyphen{}\textbackslash{}lambda h)= \textbackslash{}frac\{1\sphinxhyphen{}e\textasciicircum{}\{\sphinxhyphen{}h \textbackslash{}lambda\}\}\{\textbackslash{}lambda\}
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
and, by the Taylor expansion of \(e^{-\lambda h}\) in the point zero

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
e\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}lambda h\} = 1 \sphinxhyphen{} \textbackslash{}lambda h + \textbackslash{}frac\{1\}\{2\}\textbackslash{}lambda\textasciicircum{}2h\textasciicircum{}2 \sphinxhyphen{} \textbackslash{}frac\{1\}\{3!\}\textbackslash{}lambda\textasciicircum{}3h\textasciicircum{}3 + \textbackslash{}dotsi + \textbackslash{}frac\{1\}\{n!\} (\sphinxhyphen{}\textbackslash{}lambda h)\textasciicircum{}n + \textbackslash{}dotsi, n \textbackslash{}in \textbackslash{}mathbb\{N\}
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
\textbackslash{}begin\{gather*\}
\textbackslash{}int\_\{t\_k\}\textasciicircum{}\{t\_\{k+1\}\} (\textbackslash{}tau \sphinxhyphen{} t\_k) e\textasciicircum{}\{\sphinxhyphen{}\textbackslash{}lambda(t\_\{k+1\}\sphinxhyphen{}\textbackslash{}tau)\} d\textbackslash{}tau =
h\textasciicircum{}2 \textbackslash{}phi\_2 (\sphinxhyphen{}\textbackslash{}lambda h) =
h \textbackslash{}frac\{\textbackslash{}phi\_1(0) \sphinxhyphen{} \textbackslash{}phi\_1(\sphinxhyphen{}\textbackslash{}lambda h)\}\{\textbackslash{}lambda\} =
\textbackslash{}frac\{h\}\{\textbackslash{}lambda\} \sphinxhyphen{} \textbackslash{}frac\{1\sphinxhyphen{}e\textasciicircum{}\{\sphinxhyphen{}h \textbackslash{}lambda\}\}\{\textbackslash{}lambda\textasciicircum{}2\} = \textbackslash{}
\textbackslash{}frac\{h\}\{\textbackslash{}lambda\} \sphinxhyphen{} \textbackslash{}frac\{1\sphinxhyphen{}(1 \sphinxhyphen{} \textbackslash{}lambda h + \textbackslash{}frac\{1\}\{2\}\textbackslash{}lambda\textasciicircum{}2h\textasciicircum{}2 \sphinxhyphen{} \textbackslash{}frac\{1\}\{3!\}\textbackslash{}lambda\textasciicircum{}3h\textasciicircum{}3 + \textbackslash{}dotsi + \textbackslash{}frac\{1\}\{n!\} (\sphinxhyphen{}\textbackslash{}lambda h)\textasciicircum{}n + \textbackslash{}dotsi)\}\{\textbackslash{}lambda\textasciicircum{}2\} = \textbackslash{}
\textbackslash{}frac\{h\textasciicircum{}2\}\{2\} \sphinxhyphen{} \textbackslash{}frac\{h\textasciicircum{}3\}\{3!\} \textbackslash{}lambda + \textbackslash{}dotsi + \textbackslash{}frac\{h\textasciicircum{}n\}\{n!\} (\sphinxhyphen{}\textbackslash{}lambda)\textasciicircum{}\{n\sphinxhyphen{}2\} + \textbackslash{}dotsi  =  O(h\textasciicircum{}2),
\textbackslash{}end\{gather*\}

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
y(t\_\{k+1\}) = e\textasciicircum{}\{\sphinxhyphen{}h \textbackslash{}lambda\}y(t\_k) + g(y(t\_k), t\_k) \textbackslash{}frac\{1\sphinxhyphen{}e\textasciicircum{}\{\sphinxhyphen{}h \textbackslash{}lambda\}\}\{\textbackslash{}lambda\} + \textbackslash{}frac\{dg\}\{dt\} (y(\textbackslash{}theta\_k), \textbackslash{}theta\_k) O(h\textasciicircum{}2),
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
\textbackslash{}begin\{equation*\}
y(t\_\{k+1\}) = e\textasciicircum{}\{\sphinxhyphen{}h \textbackslash{}lambda\}y(t\_k) + g(y(t\_k), t\_k) \textbackslash{}frac\{1\sphinxhyphen{}e\textasciicircum{}\{\sphinxhyphen{}h \textbackslash{}lambda\}\}\{\textbackslash{}lambda\} + O(h\textasciicircum{}2).
\textbackslash{}end\{equation*\}

\sphinxAtStartPar
That inspires the \(\textbf{Exponential Euler method}\) :
\$\(
y_0 = y(t_0)\\
\textbf{for } k = 0, 1, 2, ..., N-1 :\\
    y_{k+1} = e^{-h \lambda}y_k + g(y_k, t_k) \frac{1-e^{-h \lambda}}{\lambda}\\
    t_{k+1} = t_k + h
\)\(
with \)y\_k \textbackslash{}thickapprox y(t\_k)\$.

\sphinxstepscope
\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
HOCHBRUCK, M.; OSTERMANN, A. Exponential integrators. Acta Numer, Cambridge Univ Press, v. 19, p. 209–286, 2010.

\item {} 
\sphinxAtStartPar
BURDEN, Richard L.; FAIRES, J. Douglas. Numerical Analysis. 9.ed. Boston:Brooks/Cole, 2010. p.348\sphinxhyphen{}353.

\item {} 
\sphinxAtStartPar
ROMA, Alexandre. Lecture notes. Introdução à Resolução Numérica do Problema de Cauchy (Introduction to numerical resolution of Cauchy problem), MAP5002. Jan. and Feb. 2023. IME\sphinxhyphen{}USP University of São Paulo.

\item {} 
\sphinxAtStartPar
TAL, Fábio A. Lecture notes. Técnicas em Teoria do Controle (techniques in control theory), MAP2321. Aug. to Dez. 2022. IME\sphinxhyphen{}USP University from São Paulo.

\item {} 
\sphinxAtStartPar
Apostol, T.M. Calculus v. 1. Blaisdell book in pure and applied mathematics. \sphinxurl{https://books.google.com.br/books?id=sR\_vAAAAMAAJ}. 1961. Blaisdell Publishing Company.

\end{enumerate}

\sphinxstepscope


\chapter{Description and evaluation of institutional support received in the period}
\label{\detokenize{Description_and_evaluation_of_institutional_support_received_in_the_period:description-and-evaluation-of-institutional-support-received-in-the-period}}\label{\detokenize{Description_and_evaluation_of_institutional_support_received_in_the_period::doc}}
\sphinxAtStartPar
To be done.

\sphinxstepscope


\chapter{Participation in scientific event, list of publications and list of papers prepared or submitted}
\label{\detokenize{Participation_in_scientific_event,_list_of_publications_and_list_of_papers_prepared_or_submitted:participation-in-scientific-event-list-of-publications-and-list-of-papers-prepared-or-submitted}}\label{\detokenize{Participation_in_scientific_event,_list_of_publications_and_list_of_papers_prepared_or_submitted::doc}}
\sphinxAtStartPar
To be done.







\renewcommand{\indexname}{Index}
\printindex
\end{document}