%% Generated by Sphinx.
\def\sphinxdocclass{jupyterBook}
\documentclass[letterpaper,10pt,english]{jupyterBook}
\ifdefined\pdfpxdimen
   \let\sphinxpxdimen\pdfpxdimen\else\newdimen\sphinxpxdimen
\fi \sphinxpxdimen=.75bp\relax
\ifdefined\pdfimageresolution
    \pdfimageresolution= \numexpr \dimexpr1in\relax/\sphinxpxdimen\relax
\fi
%% let collapsible pdf bookmarks panel have high depth per default
\PassOptionsToPackage{bookmarksdepth=5}{hyperref}
%% turn off hyperref patch of \index as sphinx.xdy xindy module takes care of
%% suitable \hyperpage mark-up, working around hyperref-xindy incompatibility
\PassOptionsToPackage{hyperindex=false}{hyperref}
%% memoir class requires extra handling
\makeatletter\@ifclassloaded{memoir}
{\ifdefined\memhyperindexfalse\memhyperindexfalse\fi}{}\makeatother

\PassOptionsToPackage{warn}{textcomp}

\catcode`^^^^00a0\active\protected\def^^^^00a0{\leavevmode\nobreak\ }
\usepackage{cmap}
\usepackage{fontspec}
\defaultfontfeatures[\rmfamily,\sffamily,\ttfamily]{}
\usepackage{amsmath,amssymb,amstext}
\usepackage{polyglossia}
\setmainlanguage{english}



\setmainfont{FreeSerif}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Italic,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldItalic
]
\setsansfont{FreeSans}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]
\setmonofont{FreeMono}[
  Extension      = .otf,
  UprightFont    = *,
  ItalicFont     = *Oblique,
  BoldFont       = *Bold,
  BoldItalicFont = *BoldOblique,
]



\usepackage[Bjarne]{fncychap}
\usepackage[,numfigreset=1,mathnumfig]{sphinx}

\fvset{fontsize=\small}
\usepackage{geometry}


% Include hyperref last.
\usepackage{hyperref}
% Fix anchor placement for figures with captions.
\usepackage{hypcap}% it must be loaded after hyperref.
% Set up styles of URL: it should be placed after hyperref.
\urlstyle{same}


\usepackage{sphinxmessages}



        % Start of preamble defined in sphinx-jupyterbook-latex %
         \usepackage[Latin,Greek]{ucharclasses}
        \usepackage{unicode-math}
        % fixing title of the toc
        \addto\captionsenglish{\renewcommand{\contentsname}{Contents}}
        \hypersetup{
            pdfencoding=auto,
            psdextra
        }
        % End of preamble defined in sphinx-jupyterbook-latex %
        

\title{Numerical exponential integrators for dynamical systems}
\date{Jun 27, 2023}
\release{}
\author{Isabela Miki Suzuki}
\newcommand{\sphinxlogo}{\vbox{}}
\renewcommand{\releasename}{}
\makeindex
\begin{document}

\pagestyle{empty}
\sphinxmaketitle
\pagestyle{plain}
\sphinxtableofcontents
\pagestyle{normal}
\phantomsection\label{\detokenize{intro::doc}}

\begin{itemize}
\item {} 
\sphinxAtStartPar
Title:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Numerical exponential integrators for dynamical systems}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Researcher in charge:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{André Salles Carvalho, Prof. Dr.}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Beneficiary:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Isabela Miki Suzuki}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Host institution:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Instituto de Matemática e Estatística at the Universidade de São Paulo}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Research team:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{Isabela Miki Suzuki}

\sphinxAtStartPar
\sphinxstylestrong{Pedro S. Peixoto, Prof. Dr.}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Number of the research project:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{2021/06678\sphinxhyphen{}5}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Duration:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{1 August 2021 to 31 July 2023}
\begin{itemize}
\item {} 
\sphinxAtStartPar
Period covered by this research report:

\end{itemize}

\sphinxAtStartPar
\sphinxstylestrong{1 August 2022 to 26 June 2023}
\begin{itemize}
\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Summary_of_the_proposed_project::doc}]{\sphinxcrossref{Summary of the proposed project}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{Project_execution::doc}]{\sphinxcrossref{Project execution}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{gestao_dados::doc}]{\sphinxcrossref{Description of how the data management plan is being followed and any changes}}}

\item {} 
\sphinxAtStartPar
{\hyperref[\detokenize{appendix::doc}]{\sphinxcrossref{Appendix}}}

\end{itemize}

\sphinxstepscope


\chapter{Summary of the proposed project}
\label{\detokenize{Summary_of_the_proposed_project:summary-of-the-proposed-project}}\label{\detokenize{Summary_of_the_proposed_project::doc}}
\sphinxAtStartPar
This is a scientific initiation project that proposes the deep study of some of the main methods
of exponential integration for problems in dynamic systems, with emphasis on the paper {[}1{]}.
Here, the undergraduate will study the construction, analysis, implementation and application
of them and at the end, it is expected that she is familiar with modern techniques of numerical
methods.

\sphinxAtStartPar
\sphinxstylestrong{Keywords:} exponential integrator, numerical methods, dynamical systems.

\sphinxstepscope


\chapter{Project execution}
\label{\detokenize{Project_execution:project-execution}}\label{\detokenize{Project_execution::doc}}
\sphinxstepscope


\section{Motivation \sphinxhyphen{} Stiffness}
\label{\detokenize{cap1:motivation-stiffness}}\label{\detokenize{cap1::doc}}
\sphinxAtStartPar
The reason for studying exponential methods is that those are good with \(\textbf{stiff differential equations}\) in terms of precision and how small the time step is required to be to achieve good accuracy.


\subsection{Cauchy problem}
\label{\detokenize{cap1:cauchy-problem}}
\sphinxAtStartPar
A \(\textbf{Cauchy problem}\) is a ordinary differential equation (ODE) with initial conditions. Being its standard scalar form:

\sphinxAtStartPar
\(\begin{cases}
    y'(t) = f(y(t), t), t \in (t_0, T) \\
    y(t_0) = y_0 \in \mathbb{K} \text{,}
\end{cases}\)

\sphinxAtStartPar
with \(\mathbb{K}\) a field, \(f\) function with image in \(\mathbb{K}\) and \(t_0, T \in \mathbb{R}\).

\sphinxAtStartPar
Sometimes, it is convenient to separate the linear part of \(f\) as indicated below:
\begin{equation*}
\begin{split}
    f(y(t), t) = g(y(t), t) - \lambda y(t) \text{,}
\end{split}
\end{equation*}
\sphinxAtStartPar
with \(\lambda \in \mathbb{K}\) or \(\mathscr{M}_{N \times N}(\mathbb{K})\).

\sphinxAtStartPar
So the system is:

\sphinxAtStartPar
\(\begin{cases}
    y'(t) + \lambda y(t) = g(y(t), t), t \in (t_0, T) \\
    y(0) = y_0 
    \text{.}
\end{cases}\)

\sphinxAtStartPar
In this project, the stiff ones were those addressed.

\sphinxAtStartPar
Notation as in {[}1{]}.


\subsection{Stiffness}
\label{\detokenize{cap1:stiffness}}
\sphinxAtStartPar
The error of the approximation given by a method trying to estimate the solution of a Cauchy problem is always given by a term multiplied by a higher derivative of the exact solution, because of the Taylor expansion with Lagrange form of the remainder. In that way, if that is enough information about this derivative, the error can be estimated.

\sphinxAtStartPar
If the norm of the derivative increases with the time, but the exact solution doesn’t, that is possible that the error dominates the approximation and the precision is lost. Those problems are called \(\textbf{stiff equations}\).

\sphinxAtStartPar
Between them, there are the \(\textbf{stiff differential equations}\), that have exact solution given by the sum of a \(\textit{transient solution}\) with a \(\textit{steady state solution}\).

\sphinxAtStartPar
The \(\textbf{transient solution}\) is of the form:
\begin{equation*}
\begin{split}
    e^{-ct} \text{, with c >>1, }
\end{split}
\end{equation*}
\sphinxAtStartPar
which is known to go to zero really fast as t increases. But its \(n\)th derivative
\begin{equation*}
\begin{split}
    \mp c^{n}e^{-ct}
\end{split}
\end{equation*}
\sphinxAtStartPar
doesn’t go as quickly and may increase in magnitude.

\sphinxAtStartPar
The \(\textbf{steady state solution}\), however, as its name implies, have small changes as time passes, with higher derivative being almost constant zero.

\sphinxAtStartPar
In a system of ODE’s, these characteristics are most common in problems in which the solution of the initial value problem is of the form
\begin{equation*}
\begin{split}
    e^{A}
\end{split}
\end{equation*}
\sphinxAtStartPar
being \(A\) a matrix such that \(\lambda_{min}\) and \(\lambda_{max}\) are the eigenvalue with minimum and maximum value in modulus and \(\lambda_{min} << \lambda_{max}\). On the bigger magnitude eigenvalue direction, the behaviour is very similar to the transient solution, having drastic changes over time and on the smaller one, comparing to that, changes almost nothing as times passes, like the steady state solution.

\sphinxAtStartPar
Work around these problems and being able to accurately approximate these so contrasting parts of the solutions requires more robust methods than the more classic and common one\sphinxhyphen{}step methods addressed at the beginning of the study of numerical methods for Cauchy problems. For the systems, it is also required that that is a precise way to calculate the exponential of a matrix.

\sphinxAtStartPar
In this project, we studied the \(\textbf{exponential methods}\), their capabilities to deal with these problems and the comparision with other simpler methods.

\sphinxAtStartPar
Definition from {[}2{]}.

\sphinxstepscope


\section{Classical methods}
\label{\detokenize{cap2:classical-methods}}\label{\detokenize{cap2::doc}}
\sphinxAtStartPar
In order to show that the exponential methods improve in dealing with Stiff problems, that is necessary to know how the previows methods deal with them, so a review on the theory of the classical methods is made in this chapter. In particular there will be focus on the one step methods. All the information is from {[}3{]}.


\subsection{One step methods for ODE}
\label{\detokenize{cap2:one-step-methods-for-ode}}
\sphinxAtStartPar
In order to find a approximation for the solution of the problem
\(\begin{cases}
y'(t) = f(t, y(t)), t \in [t_0,T] \\
y(t_0)=y_0 \text{,}
\end{cases}\)

\sphinxAtStartPar
they are of the form:
\begin{equation*}
\begin{split}
y_{k+1} = y_{k} + h \phi (t_{k},y_{k},t_{k+1},y_{k+1},h) \text{,}
\end{split}
\end{equation*}
\sphinxAtStartPar
with
\begin{equation*}
\begin{split}k = 0, 1, ..., n-1;\end{split}
\end{equation*}\begin{equation*}
\begin{split}
N \in \mathbb{N}; h = \frac{T-t_0}{N}; \\
\{t_i = t_0 + ih : i = 0, 1, ..., N\}; \\ 
y_n \thickapprox y(t_n) .
\end{split}
\end{equation*}
\sphinxAtStartPar
To analyse the method, there is a model problem
\begin{equation*}
\begin{split}
\begin{cases}
    y'(t) = - \lambda y(t) \text{ ; } t \in[t_0,T]\\ 
    y(t_0)=y_0,\\
\end{cases}
\end{split}
\end{equation*}
\sphinxAtStartPar
whose solution is \(y(t) = y_0 e^{-\lambda (t-t_0)}\)
with \(\lambda > 0.\)

\sphinxAtStartPar
If that is possible to manipulate the method so that, for this problem, can be written as \(y_{k+1} = \zeta(\lambda,h) y_k,\)

\sphinxAtStartPar
then \(\zeta(\lambda,h)\) is called \(\textbf{amplification factor}\) of the method.

\sphinxAtStartPar
By induction, it gives
\begin{equation*}
\begin{split}
y_{k+1} = \zeta(\lambda, h)^{k+1} y_0.
\end{split}
\end{equation*}
\sphinxAtStartPar
It is well known that this expression only converges as k goes to infinity if \( |\zeta(\lambda, h)| < 1\)

\sphinxAtStartPar
and then converges to zero.

\sphinxAtStartPar
When it occurs, i.e.,
\begin{equation*}
\begin{split}
    k \rightarrow \infty \Rightarrow y_k \rightarrow 0
\end{split}
\end{equation*}
\sphinxAtStartPar
such as the exact solution
\begin{equation*}
\begin{split}
    y(t) = y_0 e^{-\lambda (t-t_0)},
\end{split}
\end{equation*}
\sphinxAtStartPar
it is said that there is \(\textbf{stability}\).

\sphinxAtStartPar
The interval with the values of \(\lambda h\) such as
\begin{equation*}
\begin{split}
|\zeta(\lambda, h)|<1,
\end{split}
\end{equation*}
\sphinxAtStartPar
is called \(\textbf{interval of stability}\).

\sphinxAtStartPar
And if the interval of stability contains all the points \(z\) such that
\begin{equation*}
\begin{split}
    Re(z) < 0,
\end{split}
\end{equation*}
\sphinxAtStartPar
the method is said \(\textbf{A-stable}\).

\sphinxAtStartPar
The reason for taking this specific problem is that it models the behaviour of the difference between the approximation and the solution on a small neighbourhood of any Cauchy problem:

\sphinxAtStartPar
Taking
\begin{equation*}
\begin{split}
\begin{cases}
    y'(t) = f(y(t), t), t \in (t_0, T) \\
    y(t_0) = y_0 \in \mathbb{K}
\end{cases}
\end{split}
\end{equation*}
\sphinxAtStartPar
and a approximation \(z\) of the solution \(y\), doing
\$\(
\sigma(t) = z(t) - y(t) \Rightarrow
\)\$
\begin{equation*}
\begin{split}
\dot{\sigma}(t) = \dot{z}(t) - \dot{y}(t) = f(z(t), t) - f(y(t), t) \Rightarrow
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
\dot{\sigma}(t) + \dot{y}(t) = \dot{z}(t) = f(z(t), t) = f(y(t)+\sigma(t), t)
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
 = f(y(t), t) + \sigma(t)\frac{\partial f}{\partial y} + O(\sigma^2(t)),
\end{split}
\end{equation*}
\sphinxAtStartPar
so
\begin{equation*}
\begin{split}
\begin{cases}
    \dot{\sigma}(t) \approx \sigma(t) \frac{\partial f}{\partial y} (y(t), t) \\
    \sigma(t_k) = \sigma_k.
\end{cases}
\end{split}
\end{equation*}
\sphinxAtStartPar
Other important definitions are:

\sphinxAtStartPar
\(\textbf{Local truncation error:}\) Is the difference between the exact expression and its numerical approximation in a certain point and with a certain domain discretization. If the domain is equally spaced by \(h\) is often denoted by \(\tau(h,t_0)\) being \(t_0\) the point.

\sphinxAtStartPar
\(\textbf{Order of the local truncation error:}\) the local truncation error (which depends on the \(h\) spacing of the discretized domain) \(\tau(h)\) has order \(n \in \mathbb{N}\) if \(\tau(h) = O(h^n) \), i.e., if there is constant \(M \in \mathbb{R}\) and \(h_0 \in \mathbb{R}\) such that \(\tau(h) \leq M h^n\), \(\forall h \leq h_0\).

\sphinxAtStartPar
\(\textbf{Global error:}\) Is the difference between the approximation given by the method for the solution of the problem on a certain point and the exact one (unlike the local truncation error, here we take the solution we got, not the expression used to find the approximation).

\sphinxAtStartPar
\(\textbf{Consistency:}\) The method is said consistent if \(\lim _{h \to 0} \frac{1}{h}\tau(h,x_0) = 0\).

\sphinxAtStartPar
\(\textbf{Obs.:}\) For consistency, we usually only analyse for the linear part of the Cauchy problem, since this is the part that most influences in the consistency.

\sphinxAtStartPar
\(\textbf{Order of consistency:}\) is the smallest order (varying the points at which the local error is calculated) of the local truncation error.

\sphinxAtStartPar
\(\textbf{Convergence:}\) A numerical method is convergent if, and only if, for any well\sphinxhyphen{}posed Cauchy problem and for every \(t \in (t_0, T)\),
\begin{equation*}
\begin{split}\lim_{h \to 0} e_k = 0\end{split}
\end{equation*}
\sphinxAtStartPar
with \(t - t_0 = kh\) fixed and \(e_k\) denoting the global error on \(t_k\) (following the past notation).

\sphinxAtStartPar
\(\textbf{Theorem:}\) A one\sphinxhyphen{}step explicit method given by
\begin{equation*}
\begin{split}
y_0 = y(t_0) \\
y_{k+1} = y_{k} + h \phi (t_{k},y_{k},h)
\end{split}
\end{equation*}
\sphinxAtStartPar
such that \(\phi\) is Lipschitzian in y, continuous in their arguments, and consistent for any well\sphinxhyphen{}posed Cauchy problem is convergent. Besides that, the convergence order is greater or equal to the consistency order.

\sphinxAtStartPar
\(\textit{Prove:}\) {[}3{]} pág 29\sphinxhyphen{}31.


\subsection{Examples}
\label{\detokenize{cap2:examples}}
\sphinxAtStartPar
Euler method:
\begin{equation*}
\begin{split}
    \phi (t_{k},y_{k},h) = f(t_{k},y_{k})
\end{split}
\end{equation*}
\sphinxAtStartPar
Modified Euler method:
\begin{equation*}
\begin{split}
    \phi (t_{k},y_{k},h) = \frac{1}{2} \left[ f(t_{k},y_{k}) + f(t_{k+1},y_{k} + h f(t_{k},y_{k})) \right]
\end{split}
\end{equation*}
\sphinxAtStartPar
Midpoint method:
\begin{equation*}
\begin{split}
    \phi (t_{k},y_{k},h) = f(t_{k} + \frac{h}{2},y_{k} + \frac{h}{2} f(t_{k},y_{k}))
\end{split}
\end{equation*}
\sphinxAtStartPar
Classic Runge\sphinxhyphen{}Kutta (RK 4\sphinxhyphen{}4):
\begin{equation*}
\begin{split}
    \phi (t_{k},y_{k},h) = \frac{1}{6} \left( \kappa_1 + 2 \kappa_2 + 2 \kappa_3 + \kappa_4 \right), \text{with }\\
    \kappa_1 = f(t_{k},y_{k})\\
    \kappa_2 = f(t_{k} + \frac{h}{2},y_{k} + \frac{h}{2} \kappa_1)\\
    \kappa_3 = f(t_{k} + \frac{h}{2},y_{k} + \frac{h}{2} \kappa_2)\\
    \kappa_4 = f(t_{k} + h, y_{k} + h \kappa_3)
\end{split}
\end{equation*}

\subsection{Euler method}
\label{\detokenize{cap2:euler-method}}
\sphinxAtStartPar
Further detailing this explicit one\sphinxhyphen{}step method of
\begin{equation*}
\begin{split}
    \phi (t_{k},y_{k},h) = f(t_{k},y_{k}),
\end{split}
\end{equation*}
\sphinxAtStartPar
an analysis on stability, convergence and order of convergence is done.


\subsubsection{Stability}
\label{\detokenize{cap2:stability}}
\sphinxAtStartPar
For the problem
\(\begin{cases}
    y'(t) = - \lambda y(t) \text{ ; } t \in [t_0 , T] \\
    y(t_0)=y_0,
\end{cases}\)

\sphinxAtStartPar
with known solution
\begin{equation*}
\begin{split} y(t) = y_0e^{-\lambda (t-t_0)},\end{split}
\end{equation*}
\sphinxAtStartPar
the method turn into:
\begin{equation*}
\begin{split}
y_0 = y(t_0)\\
\textbf{for } k = 0, 1, 2, ..., N-1 :\\
    y_{k+1} = y_k + h \lambda y_k \\
    t_{k+1} = t_k + h.
\end{split}
\end{equation*}
\sphinxAtStartPar
Then the amplification factor is:
\$\(
(1 - h \lambda).
\)\$

\sphinxAtStartPar
If
\begin{equation*}
\begin{split}
|1 - h \lambda| > 1, \text{for fixed } N,
\end{split}
\end{equation*}
\sphinxAtStartPar
it will be a divergent series
\begin{equation*}
\begin{split}
(k \rightarrow \infty \Rightarrow y_k \rightarrow \infty),
\end{split}
\end{equation*}
\sphinxAtStartPar
so, since the computer has a limitant number that can represent, even if the number of steps is such that \(h\) is not small enought, it might have sufficient steps to reach the maximum number represented by the machine.

\sphinxAtStartPar
However, if
\begin{equation*}
\begin{split}
    |1 - h \lambda| < 1 \text{ and } N \text{ is fixed,}
\end{split}
\end{equation*}
\sphinxAtStartPar
it converges to zero
\begin{equation*}
\begin{split}
    (k \rightarrow \infty \Rightarrow y_k \rightarrow 0 ).
\end{split}
\end{equation*}
\sphinxAtStartPar
Besides that,
\begin{equation*}
\begin{split}
|1 - h \lambda| < 1
\end{split}
\end{equation*}
\sphinxAtStartPar
is the same as
\begin{equation*}
\begin{split}
0 < h \lambda < 2.
\end{split}
\end{equation*}
\sphinxAtStartPar
So the interval of stability is \((0,2)\).

\sphinxAtStartPar
That’s why the method suddenly converged, it was when \(h\) got small enought to \(h \lambda\) be in the interval of stability, i.e.,
\begin{equation*}
\begin{split}
    h < 2/\lambda.
\end{split}
\end{equation*}
\sphinxAtStartPar
It is worth mentioning here that if
\begin{equation*}
\begin{split}
-1 < 1 - h \lambda < 0,
\end{split}
\end{equation*}
\sphinxAtStartPar
the error will converge oscillating since it takes positive values with even exponents and negative with odd ones.


\subsubsection{Convergence}
\label{\detokenize{cap2:convergence}}
\sphinxAtStartPar
Since
\begin{equation*}
\begin{split}
\lim_{m \to +\infty} \left(1 + \frac{p}{m} \right)^m = e^p,
\end{split}
\end{equation*}
\sphinxAtStartPar
and h = \(\frac{T-t_0}{N}\), for \(y_N\) we have
\begin{equation*}
\begin{split}
\lim_{N \to +\infty} y_N = \lim_{N \to +\infty} \left(1 - h \lambda \right)^N y_0 = \lim_{N \to +\infty} \left(1 - \frac{(T-t_0) \lambda}{N} \right)^N y_0.
\end{split}
\end{equation*}
\sphinxAtStartPar
It is reasonable to take \(p = -(T-t_0) \lambda\) and conclude that the last point estimated by the method will converge to
\begin{equation*}
\begin{split}
y_0e^{-\lambda (T-t_0)}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Which is precisely \(y(T)\) and proves the convergence.


\subsubsection{Order of convergence}
\label{\detokenize{cap2:order-of-convergence}}
\sphinxAtStartPar
Being \(\tau(h, t_k)\) the local truncation error.

\sphinxAtStartPar
From
\begin{equation*}
\begin{split}
    y(t_{k+1}) = y(t_k) + h f(y(t_k),t_k) + O(h^2),
\end{split}
\end{equation*}
\sphinxAtStartPar
we have
\begin{equation*}
\begin{split}
    h \tau(h, t_k) \doteq \frac{y(t_{k+1}) - y(t_k)}{h} - f(t_k, y(t_k)) = O(h^2),
\end{split}
\end{equation*}
\sphinxAtStartPar
so
\begin{equation*}
\begin{split}
    \tau(h, t_k) = O(h).
\end{split}
\end{equation*}
\sphinxAtStartPar
Since for one step methods the order of convergence is the order of the local truncation error, the order is of \(O(h)\), order 1.

\sphinxstepscope


\section{Important concepts for the study of exponential methods}
\label{\detokenize{cap3:important-concepts-for-the-study-of-exponential-methods}}\label{\detokenize{cap3::doc}}
\sphinxAtStartPar
In this chapter, a review on the theory of matrix exponential and in \(\phi\) functions is done because of its need when applying exponential methods in systems of ODE with initial value. Besides that, the format of the treated problem is shown.


\subsection{Matrix exponential}
\label{\detokenize{cap3:matrix-exponential}}
\sphinxAtStartPar
Based on the Maclaurin series of the exponential function
\begin{equation*}
\begin{split}
    e^x = \sum_{i=0}^{\infty} \frac{x^i}{i!},
\end{split}
\end{equation*}
\sphinxAtStartPar
the \(\textbf{exponential of a square complex matrix }A\) is defined as
\begin{equation*}
\begin{split}
    e^A \doteq \sum_{i=0}^{\infty} \frac{A^i}{i!}.
\end{split}
\end{equation*}
\sphinxAtStartPar
This is well defined because it has been proven that the sequence \({p_k}\) with, \(\forall k \in \mathbb{N}\):
\begin{equation*}
\begin{split}
    p_k = \sum_{i=0}^{k} \frac{A^i}{i!}, \forall A \text{ as decribed above,}
\end{split}
\end{equation*}
\sphinxAtStartPar
is a Cauchy sequence, and therefore converge to a limit matrix which was denoted \(e^A\), since the set of the square complex matrix with fixed lenght with the norm
\begin{equation*}
\begin{split}
||A|| = \max_{||x||=1} ||Ax||
\end{split}
\end{equation*}
\sphinxAtStartPar
is a Banach space.


\subsubsection{Exponential of a zeros matrix}
\label{\detokenize{cap3:exponential-of-a-zeros-matrix}}
\sphinxAtStartPar
If \(A =   
\left[ {\begin{array}{ccccc}
    0 & 0 & 0 & \dotsm & 0\\
    0 & 0 & 0 & \dotsm & 0\\
    0 & 0 & 0 & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & 0\\
\end{array} } \right] \),
\begin{equation*}
\begin{split}
    e^A \doteq \sum_{i=0}^{\infty} \frac{A^i}{i!} = I + A + \frac{A^2}{2} + \dotsm = I + 0 + 0 + \dotsm = I.
\end{split}
\end{equation*}

\subsubsection{Exponential of a diagonal matrix}
\label{\detokenize{cap3:exponential-of-a-diagonal-matrix}}
\sphinxAtStartPar
If \(A =   
\left[ {\begin{array}{ccccc}
    \lambda_1 & 0 & 0 & \dotsm & 0\\
    0 & \lambda_2 & 0 & \dotsm & 0\\
    0 & 0 & \lambda_3 & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_{N}\\
\end{array} } \right] 
  = diag(\lambda_1, \lambda_2, \lambda_3, \dotsm, \lambda_N)\),

\sphinxAtStartPar
it is easy to note that
\begin{equation*}
\begin{split}
    A^2 = diag \left(\lambda_1^2, \lambda_2^2, \lambda_3^2, \dotsc, \lambda_N^2 \right)
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    A^3 = diag \left(\lambda_1^3, \lambda_2^3, \lambda_3^3, \dotsc, \lambda_N^3 \right)
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
\vdots
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    A^j = diag \left(\lambda_1^j, \lambda_2^j, \lambda_3^j, \dotsc, \lambda_N^j \right) , \forall j \in \mathbb{N}
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
\vdots
\end{split}
\end{equation*}
\sphinxAtStartPar
so
\begin{equation*}
\begin{split}
    e^A \doteq \sum_{i=0}^{\infty} \frac{A^i}{i!} = diag\left(\sum_{i=0}^{\infty} \frac{\lambda_1^i}{i!}, \sum_{i=0}^{\infty} \frac{\lambda_2^i}{i!}, \sum_{i=0}^{\infty} \frac{\lambda_3^i}{i!}, \dotsc, \sum_{i=0}^{\infty} \frac{\lambda_N^i}{i!}\right)
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = diag \left( e^{\lambda_1}, e^{\lambda_2}, e^{\lambda_3}, \dotsc, e^{\lambda_N} \right).
\end{split}
\end{equation*}
\sphinxAtStartPar
In the same way, if B is a diagonal by blocks matrix:
\begin{equation*}
\begin{split}
B =   
\left[ {\begin{array}{ccccc}
    B_1 & 0 & 0 & \dotsm & 0\\
    0 & B_2 & 0 & \dotsm & 0\\
    0 & 0 & B_3 & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & B_{N}\\
\end{array} } \right] 
  = diag(B_1, B_2, B_3, \dotsm, B_N),
\end{split}
\end{equation*}
\sphinxAtStartPar
then
\begin{equation*}
\begin{split}
e^B = diag(e^{B_1}, e^{B_2}, e^{B_3}, \dotsm, e^{B_N}).
\end{split}
\end{equation*}

\subsubsection{Exponential of a matrix of ones above the diagonal}
\label{\detokenize{cap3:exponential-of-a-matrix-of-ones-above-the-diagonal}}
\sphinxAtStartPar
If \(A = A_{N \times N} =   
\left[ {\begin{array}{ccccccc}
    0 & 1 &  &  &  &  & \\
     & 0 & 1 &  &  &  &\\
     &  & 0 & 1 &  &  &\\
     &  &  & 0 & 1 &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & 1 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] \),

\sphinxAtStartPar
one can calculate
\begin{equation*}
\begin{split}
A^2 = A \cdot A =  
\left[ {\begin{array}{ccccccc}
    0 & 1 &  &  &  &  & \\
     & 0 & 1 &  &  &  &\\
     &  & 0 & 1 &  &  &\\
     &  &  & 0 & 1 &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & 1 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right]  \cdot 
\left[ {\begin{array}{ccccccc}
    0 & 1 &  &  &  &  & \\
     & 0 & 1 &  &  &  &\\
     &  & 0 & 1 &  &  &\\
     &  &  & 0 & 1 &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & 1 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
  =   \left[ {\begin{array}{ccccccc}
    0 & 0 & 1 &  &  &  & \\
     & 0 & 0 & 1 &  &  &\\
     &  & 0 & 0 & 1 &  &\\
     &  &  & 0 & 0 & \ddots &\\
     &  &  &  & 0 & \ddots & 1 \\
     &  &  &  &  & \ddots & 0 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right], 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    A^3 = A \cdot A^2 = \left[ {\begin{array}{ccccccc}
    0 & 1 &  &  &  &  & \\
     & 0 & 1 &  &  &  &\\
     &  & 0 & 1 &  &  &\\
     &  &  & 0 & 1 &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & 1 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccccc}
    0 & 0 & 1 &  &  &  & \\
     & 0 & 0 & 1 &  &  &\\
     &  & 0 & 0 & 1 &  &\\
     &  &  & 0 & 0 & \ddots &\\
     &  &  &  & 0 & \ddots & 1 \\
     &  &  &  &  & \ddots & 0 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
= \left[ {\begin{array}{ccccccc}
    0 & 0 & 0 & 1 &  &  & \\
     & 0 & 0 & 0 & 1 &  &\\
     &  & 0 & 0 & 0 & \ddots &\\
     &  &  & 0 & 0 & \ddots & 1\\
     &  &  &  & 0 & \ddots & 0 \\
     &  &  &  &  & \ddots & 0 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right],
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    \vdots
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    A^{N-2} = \left[ {\begin{array}{ccccccc}
     &  &  &  & 0 & 1 & 0\\
     &  &  &  &  & 0 & 1 \\
     &  &  &  &  &  & 0 \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
\end{array} } \right],
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    A^{N-1} = \left[ {\begin{array}{ccccccc}
     &  &  &  &  &  & 1\\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
\end{array} } \right],
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    A^{N} = 0.
\end{split}
\end{equation*}
\sphinxAtStartPar
And then, with \(t \in \mathbb{R}\)
\begin{equation*}
\begin{split}
    e^{tA} \doteq \sum_{i=0}^{\infty} \frac{tA^i}{i!}
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = Id + tA + \frac{t^2 A^2}{2} + \frac{t^3 A^3}{6} + \dotsc + \frac{t^{N-2} A^{N-2}}{(N-2)!} + \frac{t^{N-1} A^{N-1}}{(N-1)!} + 0 + 0 + \dotsc + 0
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = \left[ {\begin{array}{ccccccc}
    1 &  &  &  &  &  & \\
     & 1 &  &  &  &  &\\
     &  & 1 &  &  &  &\\
     &  &  & 1 &  &  &\\
     &  &  &  & 1 &  &\\
     &  &  &  &  & \ddots &\\
     &  &  &  &  &  & 1 \\
\end{array} } \right] + \left[ {\begin{array}{ccccccc}
    0 & t &  &  &  &  & \\
     & 0 & t &  &  &  &\\
     &  & 0 & t &  &  &\\
     &  &  & 0 & t &  &\\
     &  &  &  & 0 & \ddots &  \\
     &  &  &  &  & \ddots & t \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] + 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
+ \left[ {\begin{array}{ccccccc}
    0 & 0 & \frac{t^2}{2} &  &  &  & \\
     & 0 & 0 & \frac{t^2}{2} &  &  &\\
     &  & 0 & 0 & \frac{t^2}{2} &  &\\
     &  &  & 0 & 0 & \ddots &\\
     &  &  &  & 0 & \ddots & \frac{t^2}{2} \\
     &  &  &  &  & \ddots & 0 \\
     &  &  &  &  &  & 0 \\
\end{array} } \right] + \dotsc + \left[ {\begin{array}{ccccccc}
     &  &  &  &  &  & \frac{t^{N-1}}{(N-1)!}\\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
     &  &  &  &  &  &  \\
\end{array} } \right]
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = \left[ {\begin{array}{ccccccc}
    1 & t & \frac{t^2}{2} & \frac{t^3}{3!} & \frac{t^4}{4!} & \dotsc & \frac{t^{N-1}}{(N-1)!}\\
     & 1 & t & \frac{t^2}{2} & \frac{t^3}{3!} & \ddots & \vdots \\
     &  & 1 & t & \frac{t^2}{2} & \ddots & \frac{t^4}{4!}\\
     &  &  & 1 & t & \ddots & \frac{t^3}{3!}\\
     &  &  &  & 1 & \ddots & \frac{t^2}{2} \\
     &  &  &  &  & \ddots & t \\
     &  &  &  &  &  & 1 \\
\end{array} } \right].
\end{split}
\end{equation*}

\subsubsection{Exponential of a Jordan block}
\label{\detokenize{cap3:exponential-of-a-jordan-block}}
\sphinxAtStartPar
\(\textbf{Proposition:}\) \(A_1, A_2 \in \mathscr{M}_{N \times N}(\mathbb{C})\). If \(A_1 \cdot A_2 = A_2 \cdot A_1\), then \(e^{A_1+A_2} = e^{A_1} \cdot e^{A_2}\).

\sphinxAtStartPar
A Jordan block is of the form:
\$\(
J = \left[ {\begin{array}{ccccc}
    \lambda_i & 1 & 0 & \dotsm & 0\\
    0 & \lambda_i & 1 & \dotsm & 0\\
    0 & 0 & \lambda_i & \ddots & 0\\
    \vdots & \vdots & \vdots & \ddots & 1\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right] 
\)\$
\begin{equation*}
\begin{split}
= \left[ {\begin{array}{ccccc}
    \lambda_i & 0 & 0 & \dotsm & 0\\
    0 & \lambda_i & 0 & \dotsm & 0\\
    0 & 0 & \lambda_i & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right] + \left[ {\begin{array}{ccccc}
    0 & 1 &  &  & \\
     & 0 & 1 &  &\\
     &  & 0 & \ddots &\\
     &  &  & \ddots & 1\\
     &  &  &  & 0\\
\end{array} } \right] 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = D + N,
\end{split}
\end{equation*}
\sphinxAtStartPar
and
\$\(
\left[ {\begin{array}{ccccc}
    \lambda_i & 0 & 0 & \dotsm & 0\\
    0 & \lambda_i & 0 & \dotsm & 0\\
    0 & 0 & \lambda_i & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccc}
    0 & 1 &  &  & \\
     & 0 & 1 &  &\\
     &  & 0 & \ddots &\\
     &  &  & \ddots & 1\\
     &  &  &  & 0\\
\end{array} } \right] 
\)\$
\begin{equation*}
\begin{split}
= \left[ {\begin{array}{ccccc}
    0 & \lambda_i &  &  & \\
     & 0 & \lambda_i &  &\\
     &  & 0 & \ddots &\\
     &  &  & \ddots & \lambda_i\\
     &  &  &  & 0\\
\end{array} } \right] 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
= \left[ {\begin{array}{ccccc}
    0 & 1 &  &  & \\
     & 0 & 1 &  &\\
     &  & 0 & \ddots &\\
     &  &  & \ddots & 1\\
     &  &  &  & 0\\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccc}
    \lambda_i & 0 & 0 & \dotsm & 0\\
    0 & \lambda_i & 0 & \dotsm & 0\\
    0 & 0 & \lambda_i & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right],
\end{split}
\end{equation*}
\sphinxAtStartPar
so
\begin{equation*}
\begin{split}
    e^{tJ} = e^{tD+tN} = e^{tD} \cdot e^{tN}
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
= \left[ {\begin{array}{ccccc}
    e^{t \lambda_i} & 0 & 0 & \dotsm & 0\\
    0 & e^{t \lambda_i} & 0 & \dotsm & 0\\
    0 & 0 & e^{t \lambda_i} & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & e^{t \lambda_i}\\
\end{array} } \right] \cdot \left[ {\begin{array}{ccccc}
    1 & t & \frac{t^2}{2} & \dotsc & \frac{t^{N-1}}{(N-1)!}\\
     & 1 & t & \ddots & \vdots\\
     &  & 1 & \ddots & \frac{t^2}{2} \\
     &  &  & \ddots & t \\
     &  &  &  & 1 \\
\end{array} } \right]
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
= \left[ {\begin{array}{ccccc}
    e^{t \lambda_i} & e^{t \lambda_i}t & \frac{e^{t \lambda_i} t^2}{2} & \dotsc & \frac{e^{t \lambda_i} t^{N-1}}{(N-1)!}\\
     & e^{t \lambda_i} & e^{t \lambda_i} t & \ddots & \vdots\\
     &  & e^{t \lambda_i} & \ddots & \frac{e^{t \lambda_i} t^2}{2} \\
     &  &  & \ddots & e^{t \lambda_i} t \\
     &  &  &  & e^{t \lambda_i} \\
\end{array} } \right], t \in \mathbb{R}.
\end{split}
\end{equation*}

\subsubsection{Exponential of any matrix}
\label{\detokenize{cap3:exponential-of-any-matrix}}
\sphinxAtStartPar
\(\textbf{Proposition: } \forall A \in \mathscr{M}_{N \times N}(\mathbb{C}), \exists M \in \mathscr{M}_{N \times N}(\mathbb{C})\) invertible, such that \(A = MJM^{-1}\), with
\begin{equation*}
\begin{split} 
J = \left[ {\begin{array}{ccccc}
    J_1 & 0 & 0 & \dotsm & 0\\
    0 & J_2 & 0 & \dotsm & 0\\
    0 & 0 & J_3 & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & J_{N}\\
\end{array} } \right]
\end{split}
\end{equation*}
\sphinxAtStartPar
and each \(J_i\), \(i = 1, 2, 3, \dotsc, N\) being a Jordan block, i.e.,
\begin{equation*}
\begin{split}
J_i = \left[ {\begin{array}{ccccc}
    \lambda_i & 0 & 0 & \dotsm & 0\\
    0 & \lambda_i & 0 & \dotsm & 0\\
    0 & 0 & \lambda_i & \dotsm & 0\\
    \vdots & \vdots & \vdots & \ddots & \vdots\\
    0 & 0 & 0 & \dotsm & \lambda_i\\
\end{array} } \right]
\end{split}
\end{equation*}
\sphinxAtStartPar
for some \(\lambda_i \in \mathbb{C}\) .

\sphinxAtStartPar
Note that
\$\(
    (MJM^{-1})^k = MJM^{-1}MJM^{-1}MJM^{-1} \dotsc MJM^{-1} 
\)\$
\begin{equation*}
\begin{split}
    = MJIJIJM^{-1} \dotsc MJM^{-1} = MJJJ \dotsc JM^{-1} = MJ^kM^{-1}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Because of the formula of the series that defines the expansion, it implicates in \(e^{MJM^{-1}} = M e^J M^{-1}\).

\sphinxAtStartPar
And then, using the same notation from the last proposition,
\$\(
e^{tA} = e^{tMJM^{-1}} = e^{MtJM^{-1}} = Me^{tJ}M^{-1} 
\)\$
\begin{equation*}
\begin{split}
    = M \left[ { \begin{array}{ccccc}
        e^{tJ_1} & 0 & 0 & \dotsm & 0\\
        0 & e^{tJ_2} & 0 & \dotsm & 0\\
        0 & 0 & e^{tJ_3} & \dotsm & 0\\
        \vdots & \vdots & \vdots & \ddots & \vdots\\
        0 & 0 & 0 & \dotsm & e^{tJ_{N}}\\
    \end{array} } \right] M^{-1}, t \in \mathbb{R},
\end{split}
\end{equation*}
\sphinxAtStartPar
with each block as the section above indicates.


\subsection{Linear problem}
\label{\detokenize{cap3:linear-problem}}
\sphinxAtStartPar
The linear problem is, following with the used notation:
\begin{equation*}
\begin{split}
\begin{cases}
    y'(t) + \lambda y(t) = g(y(t), t), t \in (t_0, T) \\
    y(t_0) = y_0 
    \text{,}
\end{cases}
\end{split}
\end{equation*}
\sphinxAtStartPar
the one with \(g \equiv 0.\)

\sphinxAtStartPar
So, generaly, it is of the form:
\begin{equation*}
\begin{split}
\begin{cases}
    y'(t) = A y(t), t \in (t_0, T) \\
    y(t_0) = y_0 
    \text{,}
\end{cases}
\end{split}
\end{equation*}
\sphinxAtStartPar
with \(A \in \mathscr{M}_{N \times N}(\mathbb{C}), N \in \mathbb{N}\)  (remembering that a matrix \(1 \times 1\) is simply a number).

\sphinxAtStartPar
Because \(A y(t)\) is a \(C^1\) function in \(y\), continuous in \(t\) and \(t \in (t_0, T)\), a limited interval, by the existence and uniqueness theorem, there is a single solution of the problem.

\sphinxAtStartPar
Since
\begin{equation*}
\begin{split}
    \frac{d}{dt}y_0e^{A(t-t_0)} \doteq \lim_{h\to0} \frac{y_0e^{A(t-t_0+h)}-y_0e^{A(t-t_0)}}{h}
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = y_0e^{(t-t_0)A}\lim_{h\to0} \frac{e^{Ah}-I}{h} 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = y_0e^{(t-t_0)A}\lim_{h\to0} \frac{Ae^{Ah}}{1} 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = y_0e^{(t-t_0)A} \frac{Ae^{A0}}{1}
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = y_0e^{(t-t_0)A} A I = A y_0e^{(t-t_0)A} 
\end{split}
\end{equation*}
\sphinxAtStartPar
using L’Hôpital’s rule on the second equality and noting that \(A(t-t_0+h) = A(t-t_0)+Ah\) and \(A(t-t_0) \cdot Ah = (t-t_0)hAA = Ah \cdot A(t-t_0)\), so it was possible to apply the last proposition and make \(e^{A(t-t_0+h)} = e^{A(t-t_0)} \cdot e^{Ah}\),

\sphinxAtStartPar
taking
\begin{equation*}
\begin{split}
    y(t) = y_0e^{A(t-t_0)},
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    y'(t) = A y_0 e^{(t-t_0)A} = A y(t) \text{ and } y(t_0) = y_0 e^{(t_0-t_0)A} = y_0 I = y_0.
\end{split}
\end{equation*}
\sphinxAtStartPar
So, the solution for the general linear problem is \(y(t)=y_0 e^{A(t-t_0)}\).

\sphinxAtStartPar
All information about matrix exponential is from {[}4{]}.


\subsection{General problem}
\label{\detokenize{cap3:general-problem}}
\sphinxAtStartPar
Returning to the general case

\sphinxAtStartPar
\(\begin{cases}
    y'(t) + \lambda y(t) = g(y(t), t), t \in (t_0, T) \\
    y(t_0) = y_0 
    \text{,}
\end{cases}\)

\sphinxAtStartPar
there is the variation of constants formula:
\begin{equation*}
\begin{split}
    y(t) = e^{-t \lambda}y_0 + \int_{t_0}^t e^{-\lambda(t-\tau)} g(y(\tau), \tau) d\tau.
\end{split}
\end{equation*}
\sphinxAtStartPar
This well known implicit function, gives a solution of the problem.

\sphinxAtStartPar
If the integral part can be solved, there is a explicit solution, and if the problem satisfies the hypotesis of the Piccard problem, being Lipschitz in \(t\), this is the only solution.

\sphinxAtStartPar
This formula is the basis of all the exponential methods.


\subsection{\protect\(\phi\protect\) functions}
\label{\detokenize{cap3:phi-functions}}
\sphinxAtStartPar
Before introducing exponential methods, it is useful to present the \(\phi\) functions.

\sphinxAtStartPar
They are \(\mathbb{C} \rightarrow \mathbb{C}\) functions defined as:
\begin{equation*}
\begin{split}
  \phi_0 (z) = e^z;
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
  \phi_n (z) = \int_{0}^{1} e^{(1-\tau)z} \frac{\tau^{n-1}}{(n-1)!} \,d\tau, n \geq 1.
\end{split}
\end{equation*}
\sphinxAtStartPar
By integration by parts,
\begin{equation*}
\begin{split}
  \phi_{n+1} (z) = \int_{0}^{1} e^{(1-\tau)z} \frac{\tau^n}{n!} \,d\tau \\
  = - \frac{e^{(1-1)z}}{z} \frac{1^n}{n!} + \frac{e^{(1-0)z}}{z} \frac{0^n}{l!} - \int_{0}^{1} -\frac{e^{(1-\tau)z}}{z} \frac{\tau^{n-1}}{(n-1)!} \,d\tau \\
  = - \frac{1}{n!z} + \frac{1}{z}\int_{0}^{1} e^{(1-\tau)z} \frac{\tau^{n-1}}{(n-1)!} \,d\tau.
\end{split}
\end{equation*}
\sphinxAtStartPar
Since
\begin{equation*}
\begin{split}
  \phi_n(0) = \int_{0}^{1} e^0 \frac{\tau^{n-1}}{(n-1)!} \,d\tau = \int_{0}^{1} \frac{\tau^{n-1}}{(n-1)!} \,d\tau = \frac{1^n}{n!} - 0 = \frac{1}{n!},
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
  \phi_{n+1}(z) = \frac{\phi_n(z) - \phi_n(0)}{z}, \textbf{the recursive characterization}.
\end{split}
\end{equation*}
\sphinxAtStartPar
By the properties of integral {[}5{]}, if \(h \in \mathbb{R}^*, t_k \in \mathbb{R}, t_k+h = t_{k+1},\)
\begin{equation*}
\begin{split}
  \phi_n (z) = \int_{0}^{1} e^{(1-\tau)z} \frac{\tau^{n-1}}{(n-1)!} \,d\tau \\
  = \frac{1}{h}\int_{0}^{h} e^{\frac{(h-\tau)z}{h}} \frac{\tau^{n-1}}{h^{n-1}(n-1)!} \,d\tau \\
  = \frac{1}{h}\int_{t_k}^{t_k + h} e^{\frac{(h-\tau+t_k)z}{h}} \frac{(\tau - t_k)^{n-1}}{h^{n-1}(n-1)!} \,d\tau,
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
  \phi_n (z) = \frac{1}{h^l}\int_{t_k}^{t_{k+1}} e^{\frac{1}{h}(t_{k+1}-\tau)z} \frac{(\tau - t_k)^{n-1}}{(n-1)!} \,d\tau.
\end{split}
\end{equation*}
\sphinxAtStartPar
Information from {[}1{]}.

\sphinxstepscope


\section{Exponential methods}
\label{\detokenize{cap4:exponential-methods}}\label{\detokenize{cap4::doc}}
\sphinxAtStartPar
In this chapter, exponential methods are introduced, with further analysis of some of them, being tested and compared to more classical equivalents.


\subsection{Exponential Euler method}
\label{\detokenize{cap4:exponential-euler-method}}
\sphinxAtStartPar
For

\sphinxAtStartPar
\(\begin{cases}
    y'(t) + \lambda y(t) = g(y(t), t), t \in (t_0, T) \\
    y(0) = y_0
\end{cases}\)

\sphinxAtStartPar
the domain is evenly discretized:
\begin{equation*}
\begin{split}
    N \in \mathbb{N}; h = \frac{T-t_0}{N}; \text{Domain: }\{t_k = t_0 + k h : k = 0, 1, ...\}.
\end{split}
\end{equation*}
\sphinxAtStartPar
The discretization of the ODE takes the exact solution of the Cauchy problem, given by the variation of constants formula
\begin{equation*}
\begin{split}
    y(t) = e^{-(t-t_0) \lambda}y_0 + \int_{t_0}^t [e^{-\lambda(t-\tau)} g(y(\tau), \tau)] d\tau
\end{split}
\end{equation*}
\sphinxAtStartPar
and, by Taylor expansion on \(g\):

\sphinxAtStartPar
\(\tau \in (t_k, t_{k+1})\)
\begin{equation*}
\begin{split}
    g(y(\tau), \tau) = g(y(t_k), t_k) + (\tau - t_k) \frac{dg}{dt} (y(\theta_k), \theta_k)
\end{split}
\end{equation*}
\sphinxAtStartPar
for a \(\theta_k \in (t_k, t_{k+1}),\)
\begin{equation*}
\begin{split}
    y(t_{k+1}) = e^{-(t_{k+1}-t_k) \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} [e^{-\lambda(t_{k+1}-\tau)} g(y(\tau), \tau)] d\tau
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = e^{-h \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} \left[e^{-\lambda(t_{k+1}-\tau)} \left( g(y(t_k), t_k) + (\tau - t_k) \frac{dg}{dt} (y(\theta_k), \theta_k)\right)\right] d\tau
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d\tau + \frac{dg}{dt} (y(\theta_k), \theta_k) \int_{t_k}^{t_{k+1}} (\tau - t_k) e^{-\lambda(t_{k+1}-\tau)} d\tau.
\end{split}
\end{equation*}
\sphinxAtStartPar
Since
\begin{equation*}
\begin{split}
    \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d\tau = h\phi_1(-\lambda h)= \frac{1-e^{-h \lambda}}{\lambda}
\end{split}
\end{equation*}
\sphinxAtStartPar
and, by the Taylor expansion of \(e^{-\lambda h}\) in the point zero
\begin{equation*}
\begin{split}
    e^{-\lambda h} = 1 - \lambda h + \frac{1}{2}\lambda^2h^2 - \frac{1}{3!}\lambda^3h^3 + \dotsi + \frac{1}{n!} (-\lambda h)^n + \dotsi, n \in \mathbb{N}
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
     \int_{t_k}^{t_{k+1}} (\tau - t_k) e^{-\lambda(t_{k+1}-\tau)} d\tau =
     h^2 \phi_2 (-\lambda h) =
     h \frac{\phi_1(0) - \phi_1(-\lambda h)}{\lambda} =
     \frac{h}{\lambda} - \frac{1-e^{-h \lambda}}{\lambda^2} = \\
     \frac{h}{\lambda} - \frac{1-(1 - \lambda h + \frac{1}{2}\lambda^2h^2 - \frac{1}{3!}\lambda^3h^3 + \dotsi + \frac{1}{n!} (-\lambda h)^n + \dotsi)}{\lambda^2} = \\
     \frac{h^2}{2} - \frac{h^3}{3!} \lambda + \dotsi + \frac{h^n}{n!} (-\lambda)^{n-2} + \dotsi  =  O(h^2),
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
    y(t_{k+1}) = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h \lambda}}{\lambda} + \frac{dg}{dt} (y(\theta_k), \theta_k) O(h^2),
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
  y(t_{k+1}) = e^{-h \lambda}y(t_k) + g(y(t_k), t_k) \frac{1-e^{-h \lambda}}{\lambda} + O(h^2).
\end{split}
\end{equation*}
\sphinxAtStartPar
That inspires the \(\textbf{Exponential Euler method}\) :
\begin{equation*}
\begin{split}
y_0 = y(t_0)\\
\textbf{for } k = 0, 1, 2, ..., N-1 :\\
    y_{k+1} = e^{-h \lambda}y_k + g(y_k, t_k) \frac{1-e^{-h \lambda}}{\lambda}\\
    t_{k+1} = t_k + h
\end{split}
\end{equation*}
\sphinxAtStartPar
with \(y_k \thickapprox y(t_k)\).


\subsection{Exponential time differencing methods (ETD)}
\label{\detokenize{cap4:exponential-time-differencing-methods-etd}}
\sphinxAtStartPar
In the same conditions as above, it is taken a general Taylor expansion of \(g\):

\sphinxAtStartPar
\(\tau \in (t_k, t_{k+1}), n \in \mathbb{N}\)
\begin{equation*}
\begin{split}
    g(y(\tau), \tau) = g(y(t_k), t_k) + (\tau - t_k) \frac{dg}{dt} (y(t_k), t_k) + \frac{(\tau - t_k)^2}{2!} \frac{d^2g}{dt^2} (y(t_k), t_k) + \\
    \dotsi + \frac{(\tau - t_k)^{n-1}}{(n-1)!} \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k) + \frac{(\tau - t_k)^n}{n!} \frac{d^ng}{dt^n} (y(\theta_k), \theta_k)
\end{split}
\end{equation*}
\sphinxAtStartPar
for a \(\theta_k \in (t_k, t_{k+1})\)

\sphinxAtStartPar
In
\begin{equation*}
\begin{split}
    y(t_{k+1}) = e^{-h \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} g(y(\tau), \tau) d\tau
\end{split}
\end{equation*}
\sphinxAtStartPar
It will now become
\begin{equation*}
\begin{split}
y(t_{k+1}) = e^{-h \lambda}y(t_k) + \int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)}  g(y(t_k), t_k) +
(\tau - t_k) \frac{dg}{dt} (y(t_k), t_k) +
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
 \frac{(\tau - t_k)^2}{2!} \frac{d^2g}{dt^2} (y(t_k), t_k) + \dotsi + 
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
 + \frac{(\tau - t_k)^{n-1}}{(n-1)!} \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k) + \frac{(\tau - t_k)^n}{n!} \frac{d^ng}{dt^n} (y(\theta_k), \theta_k)  d\tau,
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
y(t_{k+1}) = e^{-h \lambda}y(t_k) +
g(y(t_k), t_k)\int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} d \tau +
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
+ \frac{dg}{dt}(y(t_k), t_k)\int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} (\tau - t_k)d\tau + \frac{d^2g}{dt^2} (y(t_k), t_k)\int_{t_k}^{t_{k+1}} e^{-\lambda(t_{k+1}-\tau)} \frac{(\tau - t_k)^2}{2!}d\tau +
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
+ \dotsi +
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
+ \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k)\int_{t_k}^{t_{k+1}}  e^{-\lambda(t_{k+1}-\tau)} \frac{(\tau - t_k)^{n-1}}{(n-1)!} d\tau + \frac{d^ng}{dt^n} (y(\theta_k), \theta_k) \int_{t_k}^{t_{k+1}}  e^{-\lambda(t_{k+1}-\tau)} \frac{(\tau - t_k)^n}{n!} d\tau,
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
y(t_{k+1}) = e^{-h \lambda}y(t_k) +
h\phi_1(-\lambda h) g(y(t_k), t_k) +
h^2\phi_2(-\lambda h) \frac{dg}{dt}(y(t_k), t_k) +
h^3\phi_3(-\lambda h)\frac{d^2g}{dt^2} (y(t_k), t_k)
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
+ \dotsi +
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
+ h^n\phi_n(-\lambda h) \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k)+
h^{n+1}\phi_{n+1}(-\lambda h) \frac{d^ng}{dt^n} (y(\theta_k), \theta_k).
\end{split}
\end{equation*}
\sphinxAtStartPar
From the discussion about the exponential Euler, that is known that
\begin{equation*}
\begin{split}
h^2\phi_2(-\lambda h) = \frac{h^2}{2} - \frac{h^3}{3!} \lambda + \dotsi + \frac{h^l}{l!} (-\lambda)^{l-2} + \dotsi = \frac{1}{(-\lambda)^2} \sum\limits_{i=2}^{\infty} \frac{(-\lambda h)^i}{i!}.
\end{split}
\end{equation*}
\sphinxAtStartPar
Since
\begin{equation*}
\begin{split}
  \phi_{n+1}(-\lambda h) = \frac{\phi_n(-\lambda h) - \phi_n(0)}{-\lambda h} \text{ and}\\
  \phi_n(0) = \frac{1}{n!},
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
  h^3 \phi_3(-\lambda h) = h^2 \frac{\phi_2(0) - \phi_2(-\lambda h)}{\lambda} = \frac{\frac{h^2}{2} - (\frac{h^2}{2} - \frac{h^3}{3!} \lambda + \dotsi + \frac{h^l}{l!} (-\lambda)^{l-2} + O(h^{l+1}))}{\lambda} = \frac{1}{(-\lambda)^3} \sum\limits_{i=3}^{\infty} \frac{(-\lambda h)^i}{i!}.
\end{split}
\end{equation*}
\sphinxAtStartPar
And if
\begin{equation*}
\begin{split}
h^l \phi_l(-\lambda h) = \frac{1}{(-\lambda)^l} \sum\limits_{i=l}^{\infty} \frac{(-\lambda h)^i}{i!}, \text{for a } l \in \mathbb{N},
\end{split}
\end{equation*}\begin{equation*}
\begin{split}
  h^{l+1}\phi_{l+1}(-\lambda h) = h^{l+1} \frac{\phi_l(-\lambda h) - \phi_l(0)}{-\lambda h} = \frac{h^l \phi_l(0) - h^l \phi_l(-\lambda h)}{\lambda} = \frac{h^l}{l! \lambda} - \frac{1}{\lambda} \frac{1}{(-\lambda)^l} \sum\limits_{i=l}^{\infty} \frac{(-\lambda h)^i}{i!} = \frac{1}{(-\lambda)^{l+1}} \sum\limits_{i=l+1}^{\infty} \frac{(-\lambda h)^i}{i!}.
\end{split}
\end{equation*}
\sphinxAtStartPar
So, by induction,
\begin{equation*}
\begin{split}
h^n \phi_n(-\lambda h) = \frac{1}{(-\lambda)^n} \sum\limits_{i=n}^{\infty} \frac{(-\lambda h)^i}{i!} = O(h^n), \forall n \geq 2.
\end{split}
\end{equation*}
\sphinxAtStartPar
Then,
\begin{equation*}
\begin{split}
y(t_{k+1}) = e^{-h \lambda}y(t_k) +
h\phi_1(-\lambda h) g(y(t_k), t_k) +
h^2\phi_2(-\lambda h) \frac{dg}{dt}(y(t_k), t_k) +
h^3\phi_3(-\lambda h)\frac{d^2g}{dt^2} (y(t_k), t_k) +
\dotsi + \\
h^n\phi_n(-\lambda h) \frac{d^{n-1}g}{dt^{n-1}} (y(t_k), t_k)+
O(h^{n+1}).
\end{split}
\end{equation*}
\sphinxAtStartPar
It is possible to note that the exponential euler is essentially the exponential time differencing method of order 1.

\sphinxAtStartPar
In the same way as Taylor methods, the problem here is that at the expense of a higher order of convergence, ends up requiring the evaluation and implementation of multiple derivatives that may not even be easy to calculate. It can be avoided using Runge\sphinxhyphen{}Kutta methods.


\subsection{Exponential time differencing methods with Runge\sphinxhyphen{}Kutta time stepping}
\label{\detokenize{cap4:exponential-time-differencing-methods-with-runge-kutta-time-stepping}}
\sphinxAtStartPar
For the Runge\sphinxhyphen{}Kutte methods, that is used approximations of the derivatives that converges together with the whole expression as the time step decreases.

\sphinxstepscope


\section{References}
\label{\detokenize{References:references}}\label{\detokenize{References::doc}}\begin{enumerate}
\sphinxsetlistlabels{\arabic}{enumi}{enumii}{}{.}%
\item {} 
\sphinxAtStartPar
HOCHBRUCK, M.; OSTERMANN, A. Exponential integrators. Acta Numer, Cambridge Univ Press, v. 19, p. 209–286, 2010.

\item {} 
\sphinxAtStartPar
BURDEN, Richard L.; FAIRES, J. Douglas. Numerical Analysis. 9.ed. Boston:Brooks/Cole, 2010. p.348\sphinxhyphen{}353.

\item {} 
\sphinxAtStartPar
ROMA, Alexandre. Lecture notes. Introdução à Resolução Numérica do Problema de Cauchy (Introduction to numerical resolution of Cauchy problem), MAP5002. Jan. and Feb. 2023. IME\sphinxhyphen{}USP University of São Paulo.

\item {} 
\sphinxAtStartPar
TAL, Fábio A. Lecture notes. Técnicas em Teoria do Controle (techniques in control theory), MAP2321. Aug. to Dez. 2022. IME\sphinxhyphen{}USP University from São Paulo.

\item {} 
\sphinxAtStartPar
Apostol, T.M. Calculus v. 1. Blaisdell book in pure and applied mathematics. \sphinxurl{https://books.google.com.br/books?id=sR\_vAAAAMAAJ}. 1961. Blaisdell Publishing Company.

\item {} 
\sphinxAtStartPar
S.M. Cox, P.C. Matthews, Exponential Time Differencing for Stiff Systems, Journal of Computational Physics, Volume 176, Issue 2, 2002, Pages 430\sphinxhyphen{}455, ISSN 0021\sphinxhyphen{}9991, \sphinxurl{https://doi.org/10.1006/jcph.2002.6995}.

\item {} 
\sphinxAtStartPar
Hochbruck, Marlis, and Alexander Ostermann. “Explicit Exponential Runge\sphinxhyphen{}Kutta Methods for Semilinear Parabolic Problems.” SIAM Journal on Numerical Analysis, vol. 43, no. 3, 2006, pp. 1069–90. JSTOR, \sphinxurl{http://www.jstor.org/stable/4101280}. Accessed 27 June 2023.

\end{enumerate}

\sphinxstepscope


\chapter{Description of how the data management plan is being followed and any changes}
\label{\detokenize{gestao_dados:description-of-how-the-data-management-plan-is-being-followed-and-any-changes}}\label{\detokenize{gestao_dados::doc}}
\sphinxAtStartPar
The project is following what was initially proposed, with the student doing an in\sphinxhyphen{}depth study of the main exponential methods, such as exponential Euler, exponential time differencing methods and exponential time differencing methods with Runge\sphinxhyphen{}Kutta time stepping.

\sphinxAtStartPar
Small changes were the most emphasized paper {[}1{]}, which shared a lot of attention with other papers like {[}6{]} and {[}7{]}.

\sphinxAtStartPar
Until the end of the period of validity, it is expected that the analyzes in methods of the Runge\sphinxhyphen{}Kutta type will be completed and that the methods studied will be applied to problems of dynamical systems.

\sphinxstepscope


\chapter{Appendix}
\label{\detokenize{appendix:appendix}}\label{\detokenize{appendix::doc}}
\sphinxAtStartPar
All the functions coded are in the following environment.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
from math import *
import numpy as np
from collections import deque
import matplotlib.pyplot as plt
from scipy.linalg import expm
from scipy import linalg

stab\PYGZus{}lim = 1000.0

def classic\PYGZus{}euler\PYGZus{}deprec(t0, tf, n, x0, lamb, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, float, float, function) \PYGZhy{}\PYGZgt{} np.vector\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros(n)
    x[0]=x0
    t = t0
    for i in range(1, n):
        x[i] = x[i\PYGZhy{}1] + h*(\PYGZhy{}lamb*x[i\PYGZhy{}1] + g(x[i\PYGZhy{}1],t))
        t = t0 + i*h
        if np.abs(x[i]) \PYGZgt{} stab\PYGZus{}lim:
            x[i] = 0.0
    return x

def exponential\PYGZus{}euler\PYGZus{}deprec(t0, tf, n, x0, lamb, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, float, function) \PYGZhy{}\PYGZgt{} np.vector\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros(n)
    x[0]=x0
    t = t0
    phi1 = (1\PYGZhy{}np.exp(\PYGZhy{}h*lamb))/lamb
    for i in range(1, n):
        x[i] = np.exp(\PYGZhy{}h*lamb)*x[i\PYGZhy{}1] + phi1*g(x[i\PYGZhy{}1],t)
        t = t0 + i*h
    return x

def classic\PYGZus{}euler(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    for i in range(1, n):
        x[:,i] = x[:,i\PYGZhy{}1] + h*(np.matmul(\PYGZhy{}A,x[:,i\PYGZhy{}1]) + g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
        if np.any(x[:,i].real \PYGZgt{} stab\PYGZus{}lim):
            x[:,i] = np.nan
    return x

def exponential\PYGZus{}euler(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0] = x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    for i in range(1, n):
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
    return x

def calculate\PYGZus{}hphi1(h, A):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, np.matrix) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    hphi1 = np.matmul(1\PYGZhy{}expm(\PYGZhy{}h*A), linalg.inv(A))
    return hphi1

def calculate\PYGZus{}hphi2(h, A, hphi1):
    \PYGZsh{}IT IS NOT H2PHI2
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, np.matrix, np.matrix) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    hphi2 = np.matmul(1\PYGZhy{}hphi1/h, linalg.inv(A))
    return hphi2

def etd2(t0, tf, n, x0, A, g, derivate\PYGZus{}of\PYGZus{}g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0] = x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    hphi2 = calculate\PYGZus{}hphi2(h, A, hphi1)
    for i in range(1, n):
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t)) + h*np.matmul(hphi2,derivate\PYGZus{}of\PYGZus{}g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
    return x

def etd2rk\PYGZus{}cox\PYGZus{}and\PYGZus{}matthews(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    hphi2 = calculate\PYGZus{}hphi2(h, A, hphi1)
    for i in range(1, n):
        a = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t))
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t)) + np.matmul(hphi2,g(a, t0 + i*h)\PYGZhy{}g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
    return x

def etd2rk\PYGZus{}cox\PYGZus{}and\PYGZus{}matthews\PYGZus{}midpoint\PYGZus{}rule(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    exponential\PYGZus{}matrix\PYGZus{}2 = expm(\PYGZhy{}h/2*A)
    h\PYGZus{}2phi1\PYGZus{}2 = calculate\PYGZus{}hphi1(h/2, A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    hphi2 = calculate\PYGZus{}hphi2(h, A, hphi1)
    for i in range(1, n):
        b = np.matmul(exponential\PYGZus{}matrix\PYGZus{}2, x[:,i\PYGZhy{}1]) + np.matmul(h\PYGZus{}2phi1\PYGZus{}2,g(x[:,i\PYGZhy{}1],t))
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t)) + 2*np.matmul(hphi2,g(b, t + h/2)\PYGZhy{}g(x[:,i\PYGZhy{}1],t))
        t = t0 + i*h
    return x

def etd2rk\PYGZus{}trapezoidal\PYGZus{}rule(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    hphi1 = calculate\PYGZus{}hphi1(h, A)
    for i in range(1, n):
        a = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + np.matmul(hphi1,g(x[:,i\PYGZhy{}1],t))
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + .5 * h * (np.matmul(exponential\PYGZus{}matrix, g(x[:,i\PYGZhy{}1],t)) + g(a, t0 + i*h))
        t = t0 + i*h
    return x

def etd2rk\PYGZus{}midpoint\PYGZus{}rule(t0, tf, n, x0, A, g):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}(float, float, int, np.array, np.matrix, function, np.matrix) \PYGZhy{}\PYGZgt{} np.matrix\PYGZsq{}\PYGZsq{}\PYGZsq{}
    h = (tf\PYGZhy{}t0)/n
    x = np.zeros((x0.size,n), dtype=np.complex\PYGZus{})
    x[:,0]=x0
    t = t0
    exponential\PYGZus{}matrix = expm(\PYGZhy{}h*A)
    exponential\PYGZus{}matrix\PYGZus{}2 = expm(\PYGZhy{}h/2*A)
    h\PYGZus{}2phi1\PYGZus{}2 = calculate\PYGZus{}hphi1(h/2, A)
    for i in range(1, n):
        b = np.matmul(exponential\PYGZus{}matrix\PYGZus{}2, x[:,i\PYGZhy{}1]) + np.matmul(h\PYGZus{}2phi1\PYGZus{}2,g(x[:,i\PYGZhy{}1],t))
        x[:,i] = np.matmul(exponential\PYGZus{}matrix, x[:,i\PYGZhy{}1]) + h * np.matmul(exponential\PYGZus{}matrix\PYGZus{}2, g(b, t+h/2))
        t = t0 + i*h
    return x

def vectorize\PYGZus{}sol(t0, t1, n, sol):
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    (float, float, int, function) \PYGZhy{}\PYGZgt{} np.vector
    n is the number of steps
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    x = np.zeros(n, dtype=np.complex\PYGZus{})
    h = (t1\PYGZhy{}t0)/n
    for i in range(n):
        x[i] = sol(t0+i*h)
    return x

def error\PYGZus{}2(x\PYGZus{}approx, x\PYGZus{}exact):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.vector, np.vector) \PYGZhy{}\PYGZgt{} float \PYGZsq{}\PYGZsq{}\PYGZsq{}
    \PYGZsh{}make sure that x\PYGZus{}approx and x\PYGZus{}exact have the same lenght
    v = (x\PYGZus{}approx \PYGZhy{} x\PYGZus{}exact)*(x\PYGZus{}approx \PYGZhy{} x\PYGZus{}exact).conjugate()
    \PYGZsh{}\PYGZca{}certainly pure real
    return np.sqrt(float(np.sum(v)/x\PYGZus{}approx.size)) \PYGZsh{}normalized

def error\PYGZus{}sup(x\PYGZus{}approx, x\PYGZus{}exact):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.vector, np.vector) \PYGZhy{}\PYGZgt{} float \PYGZsq{}\PYGZsq{}\PYGZsq{}
    \PYGZsh{}make sure that x\PYGZus{}approx and x\PYGZus{}exact have the same lenght
    v = abs(x\PYGZus{}approx \PYGZhy{} x\PYGZus{}exact)
    return np.amax(v)

def g( x, t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.array, float) \PYGZhy{}\PYGZgt{} float
        (x, t) \PYGZhy{}\PYGZgt{} g(x, t)
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    g = np.array([np.sin(t)])
    return g

def g\PYGZus{}linear\PYGZus{}deprec( x, t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
        (x, t) \PYGZhy{}\PYGZgt{} g(x, t)
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    g = 0
    return g

def g\PYGZus{}linear( x, t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.array, float) \PYGZhy{}\PYGZgt{} np.array
        (x, t) \PYGZhy{}\PYGZgt{} g(x, t)
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    g = np.zeros(x.size)
    return g

def g\PYGZus{}cm1 (x, t):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (np.array, float) \PYGZhy{}\PYGZgt{} np.array
        (x, t) \PYGZhy{}\PYGZgt{} g(x, t)
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    lamb = .5
    c = 100
    r\PYGZus{}2 = x[0]**2 + x[1]**2
    g = np.array([(lamb*x[1]\PYGZhy{}c*x[0])*r\PYGZus{}2, \PYGZhy{}(lamb*x[0]+c*x[1])*r\PYGZus{}2])
    return g

def sol( t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    lmba = 100
    sol = np.exp(\PYGZhy{}lmba*t)+(np.exp(\PYGZhy{}lmba*t)+lmba*np.sin(t)\PYGZhy{}np.cos(t))/(1+lmba*lmba)
    return sol

def sol\PYGZus{}100\PYGZus{}linear( t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    sol = exp(\PYGZhy{}100*t) \PYGZsh{}u0=1
    return sol

def sol\PYGZus{}1j\PYGZus{}linear( t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    return np.exp(1j*t)

def sol\PYGZus{}non\PYGZus{}linear\PYGZus{}sin( t ):
    \PYGZsq{}\PYGZsq{}\PYGZsq{} (float, float) \PYGZhy{}\PYGZgt{} float
    RECEIVES the initial value and a real (t).
    APPLIES the cauchy problem solution to this initial value at this point.
    RETURNS a real value.
    \PYGZsq{}\PYGZsq{}\PYGZsq{}
    sol = 2\PYGZhy{}cos(t) \PYGZsh{}u0=1
    return sol

def errors\PYGZus{}array(n0, nf, method, t0, tf, x0, lmba, g, sol, vectorize\PYGZus{}sol, error):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  This function will RETURN 2 arrays.
  The first one has the errors of the approximations given by the method with
  number of steps n = n0, n0+1, n0+2, ..., nf\PYGZhy{}1.
  The second is [n0, n0+1, n0+2, ..., nf\PYGZhy{}1]

  RECEIVES:
  n0 is the first number of steps. (int)
  nf is the last one plus 1. (int)
  method have arguments (t0, tf, n, x0, lmba, g) and return a
  np.vector of length n (0, 1, 2, ..., n\PYGZhy{}1), n is the number of steps. (function)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  x0 is the initial value of the Cauchy problem. (float)
  lmbda is the coefficient os the linear part of the ploblem. (float)
  g is a function (float, float) \PYGZhy{}\PYGZgt{} (float). (function)
  sol is a function (float) \PYGZhy{}\PYGZgt{} (float). (function)
  vectorize\PYGZus{}sol is a function that \PYGZdq{}transforms sol in a vector\PYGZdq{} (function)
  (float, float, int, function) \PYGZhy{}\PYGZgt{} (np.array)
  (t0, tf, n, sol) \PYGZhy{}\PYGZgt{} np.array([sol[t0], sol[t0+h], sol[t0+2h], ..., sol[tf\PYGZhy{}1]])
  error is a function (np.array, np.array) \PYGZhy{}\PYGZgt{} (float) (function)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  v = np.zeros(nf\PYGZhy{}n0)
  domain = np.zeros(nf\PYGZhy{}n0)
  for n in range(n0, nf):
    domain[n\PYGZhy{}n0] = n
    m = method(t0, tf, n, x0, lmba, g)
    exact = vectorize\PYGZus{}sol(t0, tf, n, sol)
    if np.max(np.abs(m))\PYGZgt{}1000:
        v[n\PYGZhy{}n0]=np.nan
    else:
        v[n\PYGZhy{}n0] = error(m, exact)
  return v, domain

def graphic\PYGZus{}2D(domain, matrix, names, labelx, labely, title, key1, key2):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  domain is a list of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines. (list)
  matrix is a list of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines \PYGZhy{} same length that domain. (list)
  names is a list of the labels for the graphs, must have the same length that
  the number of lines in matrix. (list of Strings)
  labelx is the name of the x coordinate. (String)
  labely is the name of the y coordinate. (String)
  title is the title of the graph. (String)
  key1 is a boolean that indicates if the last graph must be black. (bool)
  key2 is a boolean that indicates if it should use the log scale. (bool)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  fig, ax = plt.subplots()

  colors = [\PYGZsq{}blue\PYGZsq{}, \PYGZsq{}green\PYGZsq{}, \PYGZsq{}red\PYGZsq{}, \PYGZsq{}cyan\PYGZsq{}, \PYGZsq{}magenta\PYGZsq{}, \PYGZsq{}yellow\PYGZsq{}]
  for i in range(len(names)\PYGZhy{}1):
    ax.plot(domain[i], matrix[i], color=colors[i], label=names[i])
  if key1:
    ax.plot(domain[len(names)\PYGZhy{}1], matrix[len(names)\PYGZhy{}1], color=\PYGZsq{}black\PYGZsq{}, label=names[len(names)\PYGZhy{}1])
  else:
    ax.plot(domain[len(names)\PYGZhy{}1], matrix[len(names)\PYGZhy{}1], color=colors[len(names)\PYGZhy{}1], label=names[len(names)\PYGZhy{}1])
  if key2:
    plt.yscale(\PYGZsq{}log\PYGZsq{})
  ax.legend()
  ax.set\PYGZus{}xlabel(labelx)
  ax.set\PYGZus{}ylabel(labely)
  ax.set\PYGZus{}title(title)
  return fig, ax

def graphic\PYGZus{}3D(domain, matrix1, matrix2, names, labelx, labely, labelz, title, key1, key2):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  domain is a list of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines. (list)
  matrix1 and matrix2 are lists of np.arrays [[length n1], [legth n2], ..., [length nk]]
  k = 1, 2, ..., 5 lines \PYGZhy{} same length that domain. (list)
  names is a list of the labels for the graphs, must have the same length that
  the number of lines in matrix. (list of Strings)
  labelx is the name of the x coordinate. (String)
  labely is the name of the y coordinate. (String)
  labelz is the name of the z coordinate. (String)
  title is the title of the graph. (String)
  key1 is a boolean that indicates if the last graph must be black. (bool)
  key2 is a boolean that indicates if it should use the log scale. (bool)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  fig = plt.figure()
  ax = plt.figure().add\PYGZus{}subplot(projection=\PYGZsq{}3d\PYGZsq{})

  colors = [\PYGZsq{}blue\PYGZsq{}, \PYGZsq{}green\PYGZsq{}, \PYGZsq{}red\PYGZsq{}, \PYGZsq{}cyan\PYGZsq{}, \PYGZsq{}magenta\PYGZsq{}, \PYGZsq{}yellow\PYGZsq{}]
  for i in range(len(names)\PYGZhy{}1):
    ax.plot(domain[i], matrix1[i], matrix2[i], color=colors[i], label=names[i])
  if key1:
    ax.plot(domain[len(names)\PYGZhy{}1], matrix1[len(names)\PYGZhy{}1], matrix2[len(names)\PYGZhy{}1], color=\PYGZsq{}black\PYGZsq{}, label=names[len(names)\PYGZhy{}1])
  else:
    ax.plot(domain[len(names)\PYGZhy{}1], matrix1[len(names)\PYGZhy{}1], matrix2[len(names)\PYGZhy{}1], color=colors[len(names)\PYGZhy{}1], label=names[len(names)\PYGZhy{}1])
  if key2:
    plt.yscale(\PYGZsq{}log\PYGZsq{})
  ax.legend()
  ax.set\PYGZus{}xlabel(labelx)
  ax.set\PYGZus{}ylabel(labely)
  ax.set\PYGZus{}zlabel(labelz)
  ax.set\PYGZus{}title(title)
  return fig, ax

def errors\PYGZus{}2x(n0, k, method, t0, tf, x0, lmba, g, sol, vectorize\PYGZus{}sol, error):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  This function will RETURN a np.array with the errors of the approximations given
  by the method with number of steps n = n0, 2*n0, 2**2*n0, ..., 2**(k\PYGZhy{}1)*n0.

  RECEIVES:
  n0 is the first number of steps. (int)
  k is the number of errors in the final array. (int)
  method have arguments (t0, tf, n, x0, lmba, g) and return a
  np.vector of length n (0, 1, 2, ..., n\PYGZhy{}1), n is the number of steps. (function)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  x0 is the initial value of the Cauchy problem. (float)
  lmbda is the coefficient os the linear part of the ploblem. (float)
  g is a function (float, float) \PYGZhy{}\PYGZgt{} (float). (function)
  sol is a function (float) \PYGZhy{}\PYGZgt{} (float). (function)
  vectorize\PYGZus{}sol is a function that \PYGZdq{}transforms sol in a vector\PYGZdq{} (function)
  (float, float, int, function) \PYGZhy{}\PYGZgt{} (np.array)
  (t0, tf, n, sol) \PYGZhy{}\PYGZgt{} np.array([sol[t0], sol[t0+h], sol[t0+2h], ..., sol[tf\PYGZhy{}1]])
  error is a function (np.array, np.array) \PYGZhy{}\PYGZgt{} (float) (function)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  v = np.zeros(k)
  for i in range(k):
    m = method(t0, tf, n0*2**i, x0, lmba, g)
    exact = vectorize\PYGZus{}sol(t0, tf, n0*2**i, sol)
    v[i] = error(m, exact)
  return v

def convergence\PYGZus{}table(errors\PYGZus{}2x, n0, k, t0, tf):
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  RECEIVES:
  errors\PYGZus{}2x is a array with the errors of the approximations given
  by a method with number of steps n = n0, 2*n0, 2**2*n0, ..., 2**(k\PYGZhy{}1)*n0. (np.array)
  n0 is the first number of steps. (int)
  k is the number of errors in the final array. (int)
  t0 is the initial point of the approximation. (float)
  tf is the last one. (float)
  \PYGZsq{}\PYGZsq{}\PYGZsq{}
  n = n0
  print(n, (tf\PYGZhy{}t0)/n, errors\PYGZus{}2x[0], \PYGZdq{}\PYGZhy{}\PYGZdq{}, sep=\PYGZdq{} \PYGZam{} \PYGZdq{}, end=\PYGZdq{} \PYGZbs{}\PYGZbs{}\PYGZbs{}\PYGZbs{} \PYGZbs{}n\PYGZdq{})
  for i in range(1, k):
      n = n0 * 2 ** i
      h = (tf\PYGZhy{}t0)/n
      q = errors\PYGZus{}2x[i\PYGZhy{}1]/errors\PYGZus{}2x[i] \PYGZsh{}q=erro(h)/erro(h)
      r = ((tf\PYGZhy{}t0)/(n/2))/((tf\PYGZhy{}t0)/n)
      print(n, h, errors\PYGZus{}2x[i], log(q,2)/log(r,2), sep=\PYGZdq{} \PYGZam{} \PYGZdq{}, end=\PYGZdq{} \PYGZbs{}\PYGZbs{}\PYGZbs{}\PYGZbs{} \PYGZbs{}n\PYGZdq{})
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}

\end{sphinxuseclass}
\sphinxAtStartPar
The execution done are the following.

\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])
errors\PYGZus{}2x\PYGZus{}vector = errors\PYGZus{}2x(n0, k, classic\PYGZus{}euler, t0, tf, x0, A, g, sol, vectorize\PYGZus{}sol, error\PYGZus{}sup)
convergence\PYGZus{}table(errors\PYGZus{}2x\PYGZus{}vector, n0, k, t0, tf)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
128 \PYGZam{} 0.0078125 \PYGZam{} 0.2391072699739873 \PYGZam{} \PYGZhy{} \PYGZbs{}\PYGZbs{} 
256 \PYGZam{} 0.00390625 \PYGZam{} 0.08650412059872986 \PYGZam{} 1.466817233501749 \PYGZbs{}\PYGZbs{} 
512 \PYGZam{} 0.001953125 \PYGZam{} 0.039214210532948934 \PYGZam{} 1.1413923006132296 \PYGZbs{}\PYGZbs{} 
1024 \PYGZam{} 0.0009765625 \PYGZam{} 0.018739566082401515 \PYGZam{} 1.0652890085799935 \PYGZbs{}\PYGZbs{} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])
errors\PYGZus{}2x\PYGZus{}vector = errors\PYGZus{}2x(n0, k, exponential\PYGZus{}euler, t0, tf, x0, A, g, sol, vectorize\PYGZus{}sol, error\PYGZus{}sup)
convergence\PYGZus{}table(errors\PYGZus{}2x\PYGZus{}vector, n0, k, t0, tf)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
128 \PYGZam{} 0.0078125 \PYGZam{} 4.398075514689716e\PYGZhy{}05 \PYGZam{} \PYGZhy{} \PYGZbs{}\PYGZbs{} 
256 \PYGZam{} 0.00390625 \PYGZam{} 2.074422525626487e\PYGZhy{}05 \PYGZam{} 1.0841625981445133 \PYGZbs{}\PYGZbs{} 
512 \PYGZam{} 0.001953125 \PYGZam{} 1.0056221183126109e\PYGZhy{}05 \PYGZam{} 1.0446214904461004 \PYGZbs{}\PYGZbs{} 
1024 \PYGZam{} 0.0009765625 \PYGZam{} 4.948885884282876e\PYGZhy{}06 \PYGZam{} 1.0229126060177947 \PYGZbs{}\PYGZbs{} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors\PYGZus{}2x\PYGZus{}vector = errors\PYGZus{}2x(n0, k, etd2rk\PYGZus{}cox\PYGZus{}and\PYGZus{}matthews, t0, tf, x0, A, g, sol, vectorize\PYGZus{}sol, error\PYGZus{}sup)
convergence\PYGZus{}table(errors\PYGZus{}2x\PYGZus{}vector, n0, k, t0, tf)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
128 \PYGZam{} 0.0078125 \PYGZam{} 4.186569175362864e\PYGZhy{}08 \PYGZam{} \PYGZhy{} \PYGZbs{}\PYGZbs{} 
256 \PYGZam{} 0.00390625 \PYGZam{} 1.0575183428604418e\PYGZhy{}08 \PYGZam{} 1.985085775819591 \PYGZbs{}\PYGZbs{} 
512 \PYGZam{} 0.001953125 \PYGZam{} 2.652380943352073e\PYGZhy{}09 \PYGZam{} 1.9953227875115886 \PYGZbs{}\PYGZbs{} 
1024 \PYGZam{} 0.0009765625 \PYGZam{} 6.638462730912398e\PYGZhy{}10 \PYGZam{} 1.9983668943519293 \PYGZbs{}\PYGZbs{} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors\PYGZus{}2x\PYGZus{}vector = errors\PYGZus{}2x(n0, k, etd2rk\PYGZus{}cox\PYGZus{}and\PYGZus{}matthews\PYGZus{}midpoint\PYGZus{}rule, t0, tf, x0, A, g, sol, vectorize\PYGZus{}sol, error\PYGZus{}sup)
convergence\PYGZus{}table(errors\PYGZus{}2x\PYGZus{}vector, n0, k, t0, tf)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
128 \PYGZam{} 0.0078125 \PYGZam{} 2.9740964063024178e\PYGZhy{}08 \PYGZam{} \PYGZhy{} \PYGZbs{}\PYGZbs{} 
256 \PYGZam{} 0.00390625 \PYGZam{} 6.3603379351490075e\PYGZhy{}09 \PYGZam{} 2.225276088173374 \PYGZbs{}\PYGZbs{} 
512 \PYGZam{} 0.001953125 \PYGZam{} 1.4582129219398166e\PYGZhy{}09 \PYGZam{} 2.1249020291594443 \PYGZbs{}\PYGZbs{} 
1024 \PYGZam{} 0.0009765625 \PYGZam{} 3.4828753076032726e\PYGZhy{}10 \PYGZam{} 2.065850662914468 \PYGZbs{}\PYGZbs{} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors\PYGZus{}2x\PYGZus{}vector = errors\PYGZus{}2x(n0, k, etd2rk\PYGZus{}trapezoidal\PYGZus{}rule, t0, tf, x0, A, g, sol, vectorize\PYGZus{}sol, error\PYGZus{}sup)
convergence\PYGZus{}table(errors\PYGZus{}2x\PYGZus{}vector, n0, k, t0, tf)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
128 \PYGZam{} 0.0078125 \PYGZam{} 0.0004242643044311458 \PYGZam{} \PYGZhy{} \PYGZbs{}\PYGZbs{} 
256 \PYGZam{} 0.00390625 \PYGZam{} 0.00010714498082271644 \PYGZam{} 1.9853990333325726 \PYGZbs{}\PYGZbs{} 
512 \PYGZam{} 0.001953125 \PYGZam{} 2.6871031228085582e\PYGZhy{}05 \PYGZam{} 1.9954406751889993 \PYGZbs{}\PYGZbs{} 
1024 \PYGZam{} 0.0009765625 \PYGZam{} 6.725136514989377e\PYGZhy{}06 \PYGZam{} 1.9984162299862431 \PYGZbs{}\PYGZbs{} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}
\begin{sphinxuseclass}{cell}\begin{sphinxVerbatimInput}

\begin{sphinxuseclass}{cell_input}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
n0 = 128
k = 4
t0 = 0
tf = 1
x0 = np.array([1])
A = np.array([[100]])

errors\PYGZus{}2x\PYGZus{}vector = errors\PYGZus{}2x(n0, k, etd2rk\PYGZus{}midpoint\PYGZus{}rule, t0, tf, x0, A, g, sol, vectorize\PYGZus{}sol, error\PYGZus{}sup)
convergence\PYGZus{}table(errors\PYGZus{}2x\PYGZus{}vector, n0, k, t0, tf)
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimInput}
\begin{sphinxVerbatimOutput}

\begin{sphinxuseclass}{cell_output}
\begin{sphinxVerbatim}[commandchars=\\\{\}]
128 \PYGZam{} 0.0078125 \PYGZam{} 0.00021050633676356068 \PYGZam{} \PYGZhy{} \PYGZbs{}\PYGZbs{} 
256 \PYGZam{} 0.00390625 \PYGZam{} 5.346923320679979e\PYGZhy{}05 \PYGZam{} 1.977082770096472 \PYGZbs{}\PYGZbs{} 
512 \PYGZam{} 0.001953125 \PYGZam{} 1.34290321535252e\PYGZhy{}05 \PYGZam{} 1.9933536556922617 \PYGZbs{}\PYGZbs{} 
1024 \PYGZam{} 0.0009765625 \PYGZam{} 3.362162453383888e\PYGZhy{}06 \PYGZam{} 1.9978939920584422 \PYGZbs{}\PYGZbs{} 
\end{sphinxVerbatim}

\end{sphinxuseclass}\end{sphinxVerbatimOutput}

\end{sphinxuseclass}






\renewcommand{\indexname}{Index}
\printindex
\end{document}